= Optimizers in Machine Learning: Gradient Descent
:page-mathjax: true


In the context of Machine Learning (ML), the space of optimization is the heart which helps its models stay alive. Basically, it is a way for models to `learn` better from the data. In this blog, we will start from the basics of optimization, their need and functioning and build a popular optimization algorithm - Gradient Descent, from scratch in Python.


== Optimization: Basics and Terminology
As the name suggests, optimization is a technique to `optimize` the value of a function having variables which might be constrained to take or not take certain values. Let's break down into pieces, by considering the case of single variables and the same can be extended to multivariate case. The function $f(x)$ is called the objective function, or the loss function in the context of ML, whose value we need to minimize (or equivalently maximize $-f(x)$) where $x$ can take all real values $\mathbb{R}$. The optimizer is tasked is to find the optimal point $ x^* $ in the search space (here, the whole real number line) for which the value of the objective function is minimum. Mathematically, it is often denoted by $x^* = \arg\min_x f(x)$ (read as, the argument $x$ of $f(.)$ which minimizes the function). 

[NOTE]
====
Without loss of generality, finding minimum of a function $f(x)$ is equivalent to finding maximum of $-f(x)$. For this reason, we often describe the optimization procedures for one case, usually minimum.
====

== How does it work?
A naive approach is to try out all possible values and reach the optimal point but it would be very inefficient due to large search space and it will take infinite amount of time and computation to explore all possible values of $x$. Can we optimize better? Yes! If the function is differentiable in the interval in the search space, we can leverage derivatives or gradients to guide us in the optimal direction. There are various techniques proposed in the literature and we look at one of the basic, yet powerful optimizer - Gradient Descent.


== Gradient Descent 
The core idea of this optimization procedure is to iteratively move towards the optimal point by using the gradients. The knowledge of gradients give us a whole lot of information, i.e., for a given point in space, they point in the direction of maximal change and their magnitude reflects the amount of change. The procedure to find minima works as follows: (a) start from a point in space at random, (b) compute the gradient at that point, iteratively move to next step using gradient and repeat until convergence. 

image::gradient-descent-overview.jpg[title="Overview of Gradient Descent Algorithm"]

The vanilla variant of this algorithm uses information from first order gradient and assumes that function is fully differentiable in the interval of search space. Further, it is guaranteed to find the optimal solution for convex functions no matter where we start, but it can get stuck at local minima for concave function depending upon the starting position and hence may not return us the global minima. 
 

.Gradient Descent Algorithm
====
* Input: function to minimize $f(x)$, and learning rate $\eta$  
* Output: optimal point $x^*$ for which input function gives minima 

1. $x_0 = 0$, $i = 0$;
2. do:
3.      $x_{i+1} = x_i - \eta \nabla f(x)$, where $\nabla = \frac{\partial f(x)}{\partial x}$
4. until convergence;
5. return $x_{i+1}$
====

[NOTE]
====
Since, gradients points in direction of increasing maximal change, the negative sign in above algorithm ensures that we move in the opposite direction for minimum and hence the name `gradient descent`.
====

There is a hyperparameter involved in this algorithm, which is the step-size aka. learning rate $\eta$. This is a measure of control of how far you want to shift from the last point to the next point. If learning rate is small, we are effectively taking baby steps to reach minima and it may take longer time and large steps before convergence. On the flip side, using a large learning rate can make the trajectory overshoot the minima and keep it bouncing around the minima. Generally, a good practice in ML is to start with low value, say $\eta = 0.001$ and adjust it in multiples of 3, if needed.

Another part of the algorithm is to establish a stopping criteria for the infinite loop. It can be either user-defined by providing number of iterations to perform (usually as an input parameter) or defining a tolerance parameter which ceases the loop if the difference in the function value drops below it.

(Advantages and Disadvantages)


== Python Implementation
A toy implementation of the gradient descent algorithm is described below:

[source,python]
----
import numdifftools as nd

def gd_optimizer(f_x, lr=0.01, iters=10):
    '''
    f_x:    lambda function, function to minimize
    lr:     float, learing rate
    iters:  integer, number of iterations
    '''
    x = 0
    for i in range(iters):
        print (f'Iteration {i}: f(x) = {f_x(x)}, x = {x}')
        grad = nd.Gradient(f_x)(x)
        x = x - lr * grad
    return x

gd_optimizer(lambda x: x**2 + 2*x + 3, lr=0.1, iters=20) 
----

Output:
----
Iteration 0: f(x) = 3, x = 0
Iteration 1: f(x) = 2.64, x = -0.19999999999999987
Iteration 2: f(x) = 2.4096, x = -0.36
Iteration 3: f(x) = 2.262144, x = -0.4879999999999999
Iteration 4: f(x) = 2.16777216, x = -0.5903999999999999
Iteration 5: f(x) = 2.107374182400006, x = -0.6723199999999909
Iteration 6: f(x) = 2.0687194767360038, x = -0.7378559999999929
Iteration 7: f(x) = 2.0439804651110425, x = -0.7902847999999941
Iteration 8: f(x) = 2.028147497671067, x = -0.8322278399999952
Iteration 9: f(x) = 2.018014398509483, x = -0.8657822719999962
Iteration 10: f(x) = 2.0115292150460693, x = -0.892625817599997
Iteration 11: f(x) = 2.007378697629484, x = -0.9141006540799986
Iteration 12: f(x) = 2.00472236648287, x = -0.9312805232639989
Iteration 13: f(x) = 2.0030223145490367, x = -0.945024418611199
Iteration 14: f(x) = 2.0019342813113834, x = -0.9560195348889592
Iteration 15: f(x) = 2.0012379400392852, x = -0.9648156279111674
Iteration 16: f(x) = 2.0007922816251424, x = -0.9718525023289338
Iteration 17: f(x) = 2.0005070602400914, x = -0.977482001863147
Iteration 18: f(x) = 2.0003245185536587, x = -0.9819856014905178
Iteration 19: f(x) = 2.0002076918743414, x = -0.9855884811924143
-0.9884707849539315 # returns approx -0.98 which is closer to -1 (actual minima)
----
== Interactive Tool for visualization
There exist many interactive tools for you to play around and experiment with gradient descent algo by tweaking certain things. Here are two awesome tools:
1. Online tool for GD: https://uclaacm.github.io/gradient-descent-visualiser/#playground
2. Github Repository for another tool: https://github.com/lilipads/gradient_descent_viz
    
== Usage in ML: a toy example of whole pipeline
Now, let's focus on implementing the GD algorithm from scratch in PyTorch. PyTorch is one of the widely used python library for machine learning and data science in general. We will also learn how to use PyTorch's in-built implementation for GD along with other advanced optimization techniques.

Let's begin with importing the basic libraries that we will be using for our implementation. Make sure they are installed on your machine or if on Anvil, you can switch to `fa2022-sp2023` kernel in a jupyter notebook after instantiating a new instance. We import PyTorch using the `torch` package, fetch a toy dataset along with other data pre-processing tools using the `sklearn` package and create beautiful visualization and plots using the popular `matplotlib` package.

[NOTE]
====
`torch.manual_seed(7)` helps in tracking the reproducibility of experiments involving stochasticity (randomization) in PyTorch. One instance of which is weights initialization while creating models.
====

[source,python]
----
import torch
torch.manual_seed(7)
from sklearn.datasets import load_diabetes, load_iris
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
%matplotlib inline
----

Next step is to load a toy dataset `Iris` which let's us build a classification model for learning three types (classes) of Iris plants. Each sample of this dataset represent some features of an Iris flower such as sepal length, sepal width, etc. and the target value is 0, 1 or 2 representing three classes. The aim to prepare a design matrices `X` and `y` of shape (no. of examples $\times$ feature dimension) and (no. of examples $\times$ 1) which represent the whole data. This helps us in building numpy arrays which as we will see later, are very handy for analysing our data.

[source,python]
----
dataset = load_iris()
X = dataset['data']
y = dataset['target']
feature_names = dataset['feature_names']
target_names = dataset['target_names']
----

Next step is to split the data into training and test set using function `train_test_split(...)`. This function asks the split ratio for each set along with a random seed which helps in reproducability of our experiment. Generally in ML, we train our model using train data and test it's performance on an unseen test data. Note that a random number generated using the same seed, produces same output on all machines. Next set of lines convert the numpy arrays to objects that PyTorch uses, *Tensors*. They are analogous to numpy arrays in terms of representation but are more powerful via auxilliary functions that PyTorch provides.

[source,python]
----
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)
----

After preparing the data, next task is to build the architecture of a neural network (NN) model that we will use for learning the given data. Our code uses a `Sequential(...)` API of PyTorch and builds a NN with two hidden layers of sizes 32 and 16 along with input and output layers of size 4 and 3 respectively. This is because the input dimension is 4 (as their are 4 features in the input data) and output dimentions (as there are 3 classes to make predictions about). `torch.Linear` layer builds a single NN layer which has weights and biases to learn from the data. `torch.ReLU` adds a ReLU activation function after every layer to add non-linearity to the network. It is important to add non-linearity because they are the core component for making neural nets so powerful that can learn any function. Without them, their is no point adding more layers since a 100 layer neural net without any non-linear activation is equivalent to just one layer of it. Finally, there is a `softmax` layer at the end which converts the predictions (also known as logits in the ML community) to probabilities via normalization.

[NOTE]
====
There are many other choices for activations functions apart from the common `ReLU` function, such as `sigmoid`, `Leaky ReLU`, `tanh`, etc. 
====

[source,python]
----
model = torch.nn.Sequential(
    torch.nn.Linear(4, 32),
    torch.nn.ReLU(),
    torch.nn.Linear(32, 16),
    torch.nn.ReLU(),
    torch.nn.Linear(16, 3),
    torch.nn.ReLU(),
    torch.nn.Softmax(dim=1),
)
----

For any optimization problem, there exist an objective function to minimize/maximize. Here, for the classificatio problem, we use a standard loss function of `Cross Entropy` for optimizing the neural network to learn from data. You may want to look out for other loss functions to suit the problem domain.

[source,python]
----
loss_fn = torch.nn.CrossEntropyLoss()
----

We also initantiate an optimizer from PyTorch's in-built function that we will use later as a replacement for our scratch implementation code for GD. Here, we use Stochastic Gradient Descent (SGD) which implements the vanilla version of GD but has support of other variations too (such as momentum, etc.). We can use other optimizers too, such as `Adam`, `RMSProp`, etc. by replacing with the appropriate module in PyTorch.

[source,python]
----
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
----

Next, we write the main training loop which is very standard in ML algorithms. This code snippet loops over the who dataset multiple times (known as epochs) and in each iteration, it does a forward pass to do a prediction, computes the error or loss and then backprogates the error to update the model parameters for training. Later, we save the results of training and testing error for plotting in next step.

[source,python]
----
scratch_implementation = True # allowed values: True, False
loss_history = {'Train Loss': [], 'Test Loss': []}
## training loop
for t in range(100):
    # forward pass
    y_pred = model(X_train)
    
    # compute loss
    loss = loss_fn(y_pred, y_train)
    
    # log 'train error' for plots
    loss_history['Train Loss'].append(loss.item())
    if t % 10 == 0:
        print(f'Iteration: {t}, Loss: {loss.item()}')
        
    # zero gradients
    model.zero_grad()
    
    # backpropagation
    loss.backward()
    
    # update params
    if scratch_implementation == True: 
        # gradient descent from scratch
        with torch.no_grad():
            for param in model.parameters():
                param -= learning_rate * param.grad
    else:
        # gradient descent using PyTorch's in-built module
        optimizer.step()

    # log 'test error' for plots
    with torch.no_grad():
        y_pred = model(X_test)
        loss = loss_fn(y_pred, y_test)
        loss_history['Test Loss'].append(loss.item())
----

[NOTE]
====
`with toch.no_grad()` directive informs the PyTorch that the subsequent forward pass doesn't need backpropagation to happen and thus saves time by not computing gradients and other related tasks.
====

Finally, after the model has been trained, we print the final accuracy on test data and plot the learning curves that shows the loss values as a function of time (here, epochs). They are very helpful in debugging the training process, the expected behavior is that with time, the loss should decrease but test loss may tend to increase suggesting overfitting of the model.

[source,python]
----
## final model performance
with torch.no_grad():
    y_pred_proba = model(X_test)
    y_pred = torch.argmax(y_pred_proba, dim=1)
test_acc = 100*torch.sum(y_pred == y_test)/len(y_test)
print('Test Accuracy = {}'.format(test_acc))

## plot learning curves during training
plt.plot(loss_history['Train Loss'], label='Train Loss')
plt.plot(loss_history['Test Loss'], label='Test Loss')
plt.legend()
----

Here is a complete code that you could use to experiment on your own.
[source,python]
----
import torch
torch.manual_seed(7)
from sklearn.datasets import load_diabetes, load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
%matplotlib inline

## load dataset
dataset = load_iris()
X = dataset['data']
y = dataset['target']
feature_names = dataset['feature_names']

## train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

## converting numpy arrays to tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

## neural network model
model = torch.nn.Sequential(
    torch.nn.Linear(4, 32),
    torch.nn.ReLU(),
    torch.nn.Linear(32, 16),
    torch.nn.ReLU(),
    torch.nn.Linear(16, 3),
    torch.nn.ReLU(),
    torch.nn.Softmax(dim=1),
)

## loss (objective) function
loss_fn = torch.nn.CrossEntropyLoss()

## optimizer
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

scratch_implementation = True # allowed values: True, False
loss_history = {'Train Loss': [], 'Test Loss': []}
## training loop
for t in range(100):
    # forward pass
    y_pred = model(X_train)
    
    # compute loss
    loss = loss_fn(y_pred, y_train)
    
    # log 'train error' for plots
    loss_history['Train Loss'].append(loss.item())
    if t % 10 == 0:
        print(f'Iteration: {t}, Loss: {loss.item()}')
        
    # zero gradients
    model.zero_grad()
    
    # backpropagation
    loss.backward()
    
    # update params
    if scratch_implementation == True: 
        # gradient descent from scratch
        with torch.no_grad():
            for param in model.parameters():
                param -= learning_rate * param.grad
    else:
        # gradient descent using PyTorch's in-built module
        optimizer.step()

    # log 'test error' for plots
    with torch.no_grad():
        y_pred = model(X_test)
        loss = loss_fn(y_pred, y_test)
        loss_history['Test Loss'].append(loss.item())
    
## final model performance
with torch.no_grad():
    y_pred_proba = model(X_test)
    y_pred = torch.argmax(y_pred_proba, dim=1)
test_acc = 100*torch.sum(y_pred == y_test)/len(y_test)
print('Test Accuracy = {}'.format(test_acc))

## plot learning curves during training
plt.plot(loss_history['Train Loss'], label='Train Loss')
plt.plot(loss_history['Test Loss'], label='Test Loss')
plt.legend()
plt.savefig('./learning-curve.png')
----

Example Output:
----
Iteration: 0, Loss: 1.0967992544174194
Iteration: 10, Loss: 1.077527642250061
Iteration: 20, Loss: 1.0559200048446655
Iteration: 30, Loss: 1.0225720405578613
Iteration: 40, Loss: 0.9848682284355164
Iteration: 50, Loss: 0.9532179236412048
Iteration: 60, Loss: 0.9333224892616272
Iteration: 70, Loss: 0.9213000535964966
Iteration: 80, Loss: 0.913439154624939
Iteration: 90, Loss: 0.9076613187789917
Test Accuracy = 73.33333587646484
----

image::learning-curve.png[title="Example output of `learning-curve.png`"]

References:
1. PyTorch Tutorials: https://pytorch.org/tutorials/beginner/basics/intro.html
2. ONLINE GRADIENT DESCENT: https://parameterfree.com/2019/09/11/online-gradient-descent/
3. Step by step guide for GD: https://www.jeremyjordan.me/gradient-descent/
4. Accerating GD using Momentum: https://distill.pub/2017/momentum/
