# Spring 2021 projects {#projects}

## Templates {#templates}

Our course project template can be found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), or on Scholar: 

`/class/datamine/apps/templates/project_template.Rmd`

**Important note:** We've updated the template to allow a code chunk option that prevents content from running off the page. Simply add `linewidth=80` to any code chunk that creates output that runs off the page.

This video demonstrates:

* opening a browser (emphasizing Firefox as the best choice),
* opening RStudio Server Pro (https://rstudio.scholar.rcac.purdue.edu),
* introducing (basics) about what RStudio looks like,
* checking to see that the students are using R 4.0,
* running the initial (one-time) setup script,
* opening the project template,
* knitting the template into a PDF file, and
* finally handling the popup blocker, which can potentially block the PDF.

**[Click here for video](https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_444kq84l&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5wx961lv)**

Students in STAT 19000, 29000, and 39000 are to use this as a template for all project submissions. The template includes a code chunk that "activates" our Python environment, and adjusts some default settings. In addition, it provides examples on how to include solutions for Python, R, Bash, and SQL. Every question should be clearly marked with a third-level header (using 3 `#`s) followed by `Question 1`, `Question 2`, etc. Sections for solutions should be added or removed, based on the number of questions in the given project. All code chunks are to be run and solutions displayed for the compiled PDF submission.

Any format or template related questions should be asked in Piazza.

## Submissions {#submissions}

Unless otherwise specified, all projects will need 2-4 submitted files: 

1. A compiled PDF file (built using the template), with all code and output.
2. The .Rmd file (based off of [the template](#templates)), used to Knit the final PDF.
3. If it is a project containing R code, a .R file containing all of the R code with comments explaining what the code does. _Note: This is **not** an .Rmd file._
4. If it is a project containing Python code, a .py file containing all of the Python code.

See [here](https://thedatamine.github.io/the-examples-book/faqs.html#faq-how-to-transfer-files) to learn how to transfer files to and from Scholar.

## STAT 19000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Intro to Python: part I|declaring variables, printing, running cells, exporting to different formats, etc.|
|Python|2|Intro to Python: part II|lists, tuples, if statements, opening files, pandas, matplotlib, etc.|
|Python|3|Intro to Python: part III|sets, dicts, pandas, matplotlib, lists, tuples, etc.|
|Python|4|Control flow in Python|if statements, for loops, dicts, lists, matplotlib, etc.|
|Python|5|Scientific computing/Data wrangling: part I|timing, I/O, indexing in pandas, pandas functions, matplotlib, etc.|
|Python|6|Functions: part I|writing functions, docstrings, pandas, etc.|
|Python|7|Functions: part II|writing functions, docstrings, pandas, etc.|
|Python|8|Scientific computing/Data wrangling: part II|building a recommendation system|
|Python|9|Scientific computing/Data wrangling: part III|building a recommendation system, continued...|
|Python|10|Packages|Learn more about Python packaging, importing, etc.|
|Python|11|Python Classes: part I|writing classes in Python to build a game, dunder methods, attributes, methods, etc.|
|Python|12|Python Classes: part II|writing classes in Python to build a game, dunder methods, attributes, methods, etc., continued...|
|Python|13|Data wrangling & matplotlib: part I|more pandas, more matplotlib, wrangling with increased difficulty, etc.|
|Python|14|Data wrangling & matplotlib: part II|more pandas, more matplotlib, wrangling with increased difficulty, etc.|

### Project 1 {#p01-190}

---

**Motivation:** In this course we require the majority of project submissions to include a compiled PDF, a .Rmd file based off of [our template](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), and a code file (a .R file if the project is in R, a .py file if the project is in Python). Although RStudio makes it easy to work with both Python and R, there are occasions where working out a Python problem in a Jupyter Notebook could be convenient. For that reason, we will introduce Jupyter Notebook in this project.

**Context:** This is the first in a series of projects that will introduce Python and its tooling to students.

**Scope:** jupyter notebooks, rstudio, python

**Learning objectives:** 

```{block, type="bbox"}
- Use Jupyter Notebook to run Python code and create Markdown text.
- Use RStudio to run Python code and compile your final PDF.
- Gain exposure to Python control flow and reading external data.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/open_food_facts/openfoodfacts.tsv`

#### Questions

##### 1. Navigate to https://notebook.scholar.rcac.purdue.edu/ and sign in with your Purdue credentials (_without_ BoilerKey). This is an instance of Jupyter Notebook. The main screen will show a series of files and folders that are in your `$HOME` directory. Create a new notebook by clicking on `New > f2020-s2021`. 

##### Change the name of your notebook to "LASTNAME_FIRSTNAME_project01" where "LASTNAME" is your family name, and "FIRSTNAME" is your given name. Try to export your notebook (using the `File` dropdown menu, choosing the option `Download as`), what format options (for example, `.pdf`) are available to you?

**Important note:** `f2020-s2021` is the name of our course notebook kernel. A notebook kernel is an engine that runs code in a notebook. ipython kernels run Python code. `f2020-s2021` is an ipython kernel that we've created for our course Python environment, which contains a variety of compatible, pre-installed packages for you to use. When you select `f2020-s2021` as your kernel, all of the packages in our course environment are automatically made available to you.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_4g2lwx5g)

If the kernel `f2020-s2021` does not appear in Jupyter Notebooks, you can make it appear as follows:

Login to [https://rstudio.scholar.rcac.purdue.edu](https://rstudio.scholar.rcac.purdue.edu)  
Click on `Tools > Shell...` (in the menu)  
In the shell (terminal looking thing that should say something like: `bash-4.2$`), type the following followed by Enter/Return: `/class/datamine/apps/runme`  
Then click on `Session > Restart R` (in the menu)  
You should now have access to the course kernel named `f2020-s2021` in [https://notebook.scholar.rcac.purdue.edu](https://notebook.scholar.rcac.purdue.edu)  

```{block, type="bbox"}
**Item(s) to submit:**

- A list of export format options.
```

##### Solution

```{block, type="solution"}
- .ipynb
- .py
- .html
- .md
- .rst
- .tex
- .pdf
```

##### 2. Each "box" in a Jupyter Notebook is called a _cell_. There are two primary types of cells: code, and markdown. By default, a cell will be a code cell. Place the following Python code inside the first cell, and run the cell. What is the output? 

```{python, eval=F}
from thedatamine import hello_datamine
hello_datamine()
```

**Hint:** You can run the code in the currently selected cell by using the GUI (the buttons), as well as by pressing `Ctrl+Return/Enter`. 

```{block, type="bbox"}
**Item(s) to submit:**

- Output from running the provided code.
```

##### Solution

```{block, type="solution"}
"Hello student! Welcome to The Data Mine!"
```

##### 3. Jupyter Notebooks allow you to easily pull up documentation, similar to `?function` in R. To do so, use the `help` function, like this: `help(my_function)`. What is the output from running the help function on `hello_datamine`? Can you modify the code from question (2) to print a customized message? Create a new _markdown_ cell and explain what you did to the code from question (2) to make the message customized.

**Important note:** Some Jupyter-only methods to do this are:

- Click on the function of interest and type `Shift+Tab` or `Shift+Tab+Tab`.
- Run `function?`, for example, `print?`.

**Important note:** You can also see the source code of a function in a Jupyter Notebook by typing `function??`, for example, `print??`.

```{block, type="bbox"}
**Item(s) to submit:**

- Output from running the `help` function on `hello_datamine`.
- Modified code from question (2) that prints a customized message.
```

##### Solution

```{python, eval=F, class.source="solution"}
help(hello_datamine)
```

```{block, type="solution"}
Help on function hello_datamine in module thedatamine.core:

hello_datamine(name: str = 'student') -> None
    Prints a hello message to a Data Mine student.
    
    Args:
        str (name, optional): The name of a student. Defaults to 'student'.
```

```{python, eval=F, class.source="solution"}
hello_datamine("Kevin")
```

##### 4. At this point in time, you've now got the basics of running Python code in Jupyter Notebooks. There is really not a whole lot more to it. For this class, however, we will continue to create RMarkdown documents in addition to the compiled PDFs. You are welcome to use Jupyter Notebooks for personal projects or for testing things out, however, we will still require an RMarkdown file (.Rmd), PDF (generated from the RMarkdown file), and .py file (containing your python code). For example, please move your solutions from Questions 1, 2, 3 from Jupyter Notebooks over to RMarkdown (we discuss RMarkdown below). Let's learn how to run Python code chunks in RMarkdown.

##### Sign in to https://rstudio.scholar.rcac.purdue.edu (_with_ BoilerKey). Projects in The Data Mine should all be submitted using our template found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd) or on Scholar (`/class/datamine/apps/templates/project_template.Rmd`).

##### Open the project template and save it into your home directory, in a new RMarkdown file named `project01.Rmd`. Prior to running any Python code, run `datamine_py()` in the R console, just like you did at the beginning of every project from the first semester.

##### Code chunks are parts of the RMarkdown file that contains code. You can identify what type of code a code chunk contains by looking at the _engine_ in the curly braces "{" and "}". As you can see, it is possible to mix and match different languages just by changing the engine. Move the solutions for questions 1-3 to your `project01.Rmd`. Make sure to place all Python code in `python` code chunks. Run the `python` code chunks to ensure you get the same results as you got when running the Python code in a Jupyter Notebook. 

**Important note:** Make sure to run `datamine_py()` in the R console prior to attempting to run any Python code.

**Hint:** The end result of the `project01.Rmd` should look _similar_ to [this](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/example02.Rmd).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

```{block, type="bbox"}
**Item(s) to submit:**

- `project01.Rmd` with the solutions from questions 1-3 (including any Python code in `python` code chunks). 
```

##### Solution

```{block, type="solution"}
Done.
```

##### 5. It is not a Data Mine project without data! [Here](#p-csv-pkg) are some examples of reading in data line by line using the `csv` package. How many columns are in the following dataset: `/class/datamine/data/open_food_facts/openfoodfacts.tsv`? Print the first row, the number of columns, and then exit the loop after the first iteration using the `break` keyword.

**Hint:** You can get the number of elements in a list by using the `len` method. For example: `len(my_list)`.

**Hint:** You can use the `break` keyword to exit a loop. As soon as `break` is executed, the loop is exited and the code immediately following the loop is run.

```{python, eval=F}
for my_row in my_csv_reader:
    print(my_row)
    break

print("Exited loop as soon as 'break' was run.")
```

**Hint:** `'\t'` represents a tab in Python.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ck74xlzq)

**Important note:** If you get a Dtype warning, feel free to just ignore it. 

**Relevant topics:** [for loops](#p-for-loops), [break](#p-break), [print](#p-print)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve this problem.
- The first row printed, and the number of columns printed.
```

##### Solution

```{python, eval=F, class.source="solution"}
import csv
with open('/class/datamine/data/open_food_facts/openfoodfacts.tsv') as my_file:
    my_reader = csv.reader(my_file, delimiter='\t')
    for row in my_reader:
        print(row)
        print(len(row))
        break # prematurely leave the loop
```

##### 6 (optional). Unlike in R, where many of the tools you need are built-in (`read.csv`, data.frames, etc.), in Python, you will need to rely on packages like `numpy` and `pandas` to do the bulk of your data science work. {#p1-06}

##### In R it would be really easy to find the mean of the 151st column, `caffeine_100g`:

```{r, eval=F}
myDF <- read.csv("/class/datamine/data/open_food_facts/openfoodfacts.tsv", sep="\t", quote="")
mean(myDF$caffeine_100g, na.rm=T) # 2.075503
```

##### If you were to try to modify our loop from question (5) to do the same thing, you will run into a myriad of issues, just to try and get the mean of a column. Luckily, it is easy to do using `pandas`:

```{python, eval=F}
import pandas as pd
myDF = pd.read_csv("/class/datamine/data/open_food_facts/openfoodfacts.tsv", sep="\t")
myDF["caffeine_100g"].mean() # 2.0755028571428573
```

##### Take a look at some of the methods you can perform using pandas [here](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats). Perform an interesting calculation in R, and replicate your work using `pandas`. Which did you prefer, Python or R?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ybx1iukd)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Python code used to solve the problem.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything.
```


---

### Project 2 {#p02-190}

---

**Motivation:** In Python it is very important to understand some of the data types in a little bit more depth than you would in R. Many of the data types in Python will seem very familiar. A `character` in R is similar to a `str` in Python. An `integer` in R is an `int` in Python. A `numeric` in R is similar to a `float` in Python. A `logical` in R is similar to a `bool` in Python. In addition to all of that, there are some very popular classes that packages like `numpy` and `pandas` introduces. On the other hand, there are some data types in Python like `tuple`s, `list`s, `set`s, and `dict`s that diverge from R a little bit more. It is integral to understand some basic concepts before jumping too far into everything. 

**Context:** This is the second project introducing some basic data types, and demonstrating some familiar control flow concepts, all while digging right into a dataset.

**Scope:** tuples, lists, if statements, opening files

**Learning objectives:** 

```{block, type="bbox"}
- List the differences between lists & tuples and when to use each.
- Gain familiarity with string methods, list methods, and tuple methods.
- Demonstrate the ability to read and write data of various formats using various packages.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. Read in the dataset `/class/datamine/data/craigslist/vehicles.csv` into a `pandas` DataFrame called `myDF`. `pandas` is an integral tool for various data science tasks in Python. You can read a quick intro [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html). We will be slowly introducing bits and pieces of this package throughout the semester. Similarly, we will try to introduce byte-sized (ha!) portions of plotting packages to slowly build up your skills.

##### How big is the dataset (in Mb or Gb)?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1bhwhkt2)

**Important note:** If you didn't do [optional question 6 in project 1](#p1-06), we would recommend taking a look.

**Hint:** Remember to check out a question's _relevant topics_. We try very hard to link you to content and examples that will get you up and running as _quickly_ as possible.

**Relevant topics:** [pandas read_csv](#p-pandas-read_csv), [get filesize in Python](#p-pathlib-path)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
```

##### Solution

```{python, eval=F, class.source="solution"}
import pandas as pd
from pathlib import Path

myDF = pd.read_csv("/class/datamine/data/craigslist/vehicles.csv")

p = Path("/class/datamine/data/craigslist/vehicles.csv")
size_in_mb = p.stat().st_size/1000
print(size_in_mb)
```

##### 2. In question (1) we read in our data into a `pandas` DataFrame. Use one of the `pandas` DataFrame [attributes](https://pandas.pydata.org/docs/reference/frame.html#attributes-and-underlying-data) to get the number of columns and rows of our dataset. How many columns and rows are there? Use f-strings to print a message, for example:

````
There are 123 columns in the DataFrame!
There are 321 rows in the DataFrame!
````

##### In project 1, we learned how to read a csv file in, line-by-line, and print values. Use the `csv` package to print _just_ the first row, which should contain the names of the columns, OR instead of using the `csv` package, use one of the `pandas` attributes from `myDF` (to print the column names).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_cifzobbk)

**Relevant topics:** [csv read csv](#p-csv-pkg), [pandas DataFrame](#p-pandas-dataframe), [f-strings](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python), [break](#p-break)

```{block, type="bbox"}
**Item(s) to submit:**

- The output from printing the f-strings.
- Python code used to solve the problem.
```

##### Solution

```{python, eval=F, class.source="solution"}
print(f'There are {myDF.shape[1]} columns in the DataFrame!')
print(f'There are {myDF.shape[0]} columns in the DataFrame!')
```

```{python, eval=F, class.source="solution"}
import csv

our_row = []
with open("/class/datamine/data/craigslist/vehicles.csv") as my_file:
    my_reader = csv.reader(my_file)
    for row in my_reader:
        our_row = row
        break

print(our_row)
print(myDF.columns.to_list())
```

##### 3. Use the `csv` or `pandas` package to get a [list](#p-lists-and-tuples) called `our_columns` that contains the column names. Add a string, "extra", to the end of `our_columns`. Print the second value in the list. Without using a loop, print the 1st, 3rd, 5th, etc. elements of the list. Print the last four elements of the list ( "state", "lat", "long", and "extra") by accessing their negative index.

##### "extra" doesn't belong in our list, you can easily remove this value from our list by doing the following:

```{python, eval=F}
our_columns.pop(25)

# or even this, as pop removes the last value by default
our_columns.pop()
```

##### BUT the problem with this solution is that you must know the index of the value you'd like to remove, and sometimes you do not know the index of the value. Instead, please show how to use a [list method](#p-list-methods) to remove "extra" by _value_ rather than by _index_.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1z6kxfn1)

**Relevant topics:** [csv read csv](#p-csv-pkg), [break](#p-break), [append](#p-list-methods), [indexing](#p-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
our_columns = []
with open("/class/datamine/data/craigslist/vehicles.csv") as my_file:
    my_reader = csv.reader(my_file)
    for row in my_reader:
        our_columns = row
        break

our_columns.append("extra")
print(our_columns[1])
print(our_columns[::2])
print(our_columns[-4:])

print(our_columns.remove("extra"))
```

##### 4. `matplotlib` is one of the primary plotting packages in Python. You are provided with the following code:

```{python, eval=F}
my_values = tuple(myDF.loc[:, 'odometer'].dropna().to_list())
```

##### The result is a _tuple_ containing the odometer readings from all of the vehicles in our dataset. Create a lineplot of the odometer readings.

##### Well, that plot doesn't seem too informative. Let's first sort the values in our tuple:

```{python, eval=F}
my_values.sort()
```

##### What happened? A tuple is immutable. What this means is that once the contents of a tuple are declared they cannot be modified. For example:

```{python, eval=F}
# This will fail because tuples are immutable
my_values[0] = 100
```

##### You can read a good article about this [here](http://www.compciv.org/guides/python/fundamentals/tuples-immutable/). In addition, [here](https://stackoverflow.com/questions/1708510/list-vs-tuple-when-to-use-each) is a great post that gives you an idea when using a tuple might be a good idea. Okay, so let's go back to our problem. We know that lists _are_ mutable (and therefore sortable), so convert `my_values` to a list and then sort, and re-plot.

##### It looks like there are some (potential) outliers that are making our plot look a little wonky. For the sake of seeing how the plot would look, use negative indexing to plot the sorted values _minus_ the last 50 values (the 50 highest values). New new plot may not look _that_ different, that is okay.

**Hint:** To prevent plotting values on the same plot, close your plot with the `close` method, for example:

```{python, eval=F}
import matplotlib.pyplot as plt
my_values = [1,2,3,4,5]
plt.plot(my_values)
plt.show()
plt.close()
```

**Relevant topics:** [list methods](#p-list-methods), [indexing](#p-indexing), [matplotlib lineplot](#p-matplotlib-lineplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
import matplotlib.pyplot as plt

my_values = tuple(myDF.loc[:, 'odometer'].dropna().to_list())
plt.plot(my_values)
plt.close()

my_values = list(my_values)
my_values.sort()
plt.plot(my_values)
plt.show()
plt.close()

plt.plot(my_values[:-50])
plt.show()
plt.close()
```

##### 5. We've covered a lot in this project! Use what you've learned so far to do one (or more) of the following tasks:

##### - Create a cool graphic using `matplotlib`, that summarizes some data from our dataset.

##### - Use `pandas` and your investigative skills to sift through the dataset and glean an interesting factoid.

##### - Create some commented coding examples that highlight the differences between lists and tuples. Include at least 3 examples.

**Relevant topics:** [pandas](#p-pandas), [indexing](#p-indexing), [matplotlib](#p-matplotlib)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
# Could be anything.
```

---

### Project 3 {#p03-190}

---

**Motivation:** A dictionary (referred to as a `dict`) is one of the most useful data structures in Python. You can think about them as a data structure containing _key_: _value_ pairs. Under the hood, a `dict` is essentially a data structure called a _hash table_. [Hash tables](https://en.wikipedia.org/wiki/Hash_table) are a data structure with a useful set of properties. The time needed for searching, inserting, or removing a piece of data has a constant average lookup time, meaning that no matter how big your hash table grows to be, inserting, searching, or deleting a piece of data will _usually_ take about the same amount of time.  (The worst case time increases linearly.) Dictionaries (`dict`) are used a lot, so it is worthwhile to understand them. Although not used quite as often, another important data type called a `set`, is also worthwhile learning about.

Dictionaries, often referred to as dicts, are really powerful. There are two primary ways to "get" information from a dict. One is to use the `get` method, the other is to use square brackets and strings. Test out the following to understand the differences between the two:

```{python, eval=F}
my_dict = {"fruits": ["apple", "orange", "pear"], "person": "John", "vegetables": ["carrots", "peas"]}

# If "person" is indeed a key, they will function the same way
my_dict["person"]
my_dict.get("person")

# If the key does not exist, like below, they will not 
# function the same way.
my_dict.get("height") # Returns None when key doesn't exist
print(my_dict.get("height")) # By printing, we can see None in this case
my_dict["height"] # Throws a KeyError exception because the key, "height" doesn't exist
```

**Context:** In our third project, we introduce some basic data types, and we demonstrate some familiar control flow concepts, all while digging right into a dataset. Throughout the course, we will slowly introduce concepts from `pandas`, and popular plotting packages.

**Scope:** dicts, sets, if/else statements, opening files, tuples, lists

**Learning objectives:** 

```{block, type="bbox"}
- Explain what is a `dict` is and why it is useful.
- Understand how a `set` works and when it could be useful.
- List the differences between lists & tuples and when to use each.
- Gain familiarity with string methods, list methods, and tuple methods.
- Gain familiarity with dict methods.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. In project 2 we learned how to read in data using `pandas`. Read in the (`/class/datamine/data/craigslist/vehicles.csv`) dataset into a DataFrame called `myDF` using `pandas`. In R we can get a sneak peek at the data by doing something like:

```{r, eval=F}
head(myDF) # where myDF is a data.frame
```

##### There is a very similar (and aptly named method) in `pandas` that allows us to do the exact same thing with a `pandas` DataFrame. Get the `head` of `myDF`, and take a moment to consider how much time it would take to get this information if we didn't have this nice `head` method.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1rqwiges)

**Relevant topics:** [pandas read_csv](#p-pandas-read_csv), [head](#p-pandas-dataframe)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The `head` of the dataset.
```

##### 2. Dictionaries, often referred to as dicts, are really powerful. There are two primary ways to "get" information from a dict. One is to use the `get` method, the other is to use square brackets and strings. Test out the following to understand the differences between the two:

```{python, eval=F}
my_dict = {"fruits": ["apple", "orange", "pear"], "person": "John", "vegetables": ["carrots", "peas"]}

# If "person" is indeed a key, they will function the same way
my_dict["person"]
my_dict.get("person")

# If the key does not exist, like below, they will not 
# function the same way.
my_dict.get("height") # Returns None when key doesn't exist
print(my_dict.get("height")) # By printing, we can see None in this case
my_dict["height"] # Throws a KeyError exception because the key, "height" doesn't exist
```

##### Look at the dataset. Create a dict called `my_dict` that contains key:value pairs where the keys are years, and the values are a single int representing the number of vehicles from that year on craigslist. Use the `year` column, a loop, and a dict to accomplish this. Print the dictionary. You can use the following code to extract the `year` column as a list. In the next project we will learn how to loop over `pandas` DataFrames.

**Hint:** If you get a `KeyError`, remember, you must declare each key value pair just like any other variable. Use the following code to initialize each `year` key to the value 0.

```{python, eval=F}
myyears = myDF['year'].dropna().to_list()

# get a list containing each unique year
unique_years = list(set(myyears))

# for each year (key), initialize the value (value) to 0
my_dict = {}
for year in unique_years:
    my_dict[year] = 0
```

**Hint:** Here are some of the results you should get:

```{python, eval=F}
print(my_dict[1912]) # 5
print(my_dict[1982]) # 185
print(my_dict[2014]) # 31703
```

**Note:** There is a special kind of `dict` called a `defaultdict`, that allows you to give default values to a `dict`, giving you the ability to "skip" initialization. We will show you this when we release the solutions to this project!  It is not required, but it is interesting to know about!

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_cl3b1xyy)

**Relevant topics:** [dicts](#p-dicts)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- `my_dict` printed.
```

##### 3. After completing question (2) you can easily access the number of vehicles from a given year. For example, to get the number of vehicles on craigslist from 1912, just run:

```{python, eval=F}
my_dict[1912]

# or

my_dict.get(1912)
```

##### A `dict` stores its data in key:value pairs. Identify a "key" from `my_dict`, as well as the associated "value". As you can imagine, having data in this format can be very beneficial. One benefit is the ability to easily create a graphic using `matplotlib`. Use `matplotlib` to create a bar graph with the year on the x-axis, and the number of vehicles from that year on the y-axis. 

**Important note:** If when you end up seeing something like `<BarContainer object of X artists>`, you should probably end the code chunk with `plt.show()` instead. What is happening is Python is trying to `print` the plot object. That text is the result. To instead display the plot you need to call `plt.show()`.

**Hint:** To use `matplotlib`, first import it:

```{python, eval=F}
import matplotlib.pyplot as plt

# now you can use it, for example
plt.plot([1,2,3,1])
plt.show()
plt.close()
```

**Hint:** The `keys` method and `values` method from `dict` could be useful here.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_mn3a3kss)

**Relevant topics:** [dicts](#p-dicts), [matplotlib](#p-matplotlib), [barplot]{#p-matplotlib-barplot}

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The resulting plot.
- A sentence giving an example of a "key" and associated "value" from `my_dict` (e.g., a sentence explaining the 1912 example above).
```

##### 4. In the hint in question (2), we used a `set` to quickly get a list of unique years in a list. Some other common uses of a `set` are when you want to get a list of values that are in one list but not another, or get a list of values that are present in both lists. Examine the following code. You'll notice that we are looping over many values. Replace the code for each of the three examples below with code that uses *no* loops whatsoever.

```{python, eval=F}
listA = [1, 2, 3, 4, 5, 6, 12, 12]
listB = [2, 1, 7, 7, 7, 2, 8, 9, 10, 11, 12, 13]

# 1. values in list A but not list B
# values in list A but not list B
onlyA = []
for valA in listA:
    if valA not in listB and valA not in onlyA:
        onlyA.append(valA)

print(onlyA) # [3, 4, 5, 6]

# 2. values in listB but not list A
onlyB = []
for valB in listB:
    if valB not in listA and valB not in onlyB:
        onlyB.append(valB)

print(onlyB) # [7, 8, 9, 10, 11, 13]

# 3. values in both lists
# values in both lists
in_both_lists = []
for valA in listA:
    if valA in listB and valA not in in_both_lists:
        in_both_lists.append(valA)

print(in_both_lists) # [1,2,12]
```

**Hint:** You should use a `set`.

**Note:** In addition to being easier to read, using a `set` is _much_ faster than loops!

**Note:** A set is a group of values that are unordered, unchangeable, and no duplicate values are allowed. While they aren't used a _lot_, they can be useful for a few common tasks like: removing duplicate values efficiently, efficiently finding values in one group of values that are not in another group of values, etc.

**Relevant topics:** [sets](#p-sets)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The output from running the code.
```

##### 5. The value of a dictionary does not have to be a single value (like we've shown so far). It can be _anything_. Observe that there is latitude and longitude data for each row in our DataFrame (`lat` and `long`, respectively). Wouldn't it be useful to be able to quickly "get" pairs of latitude and longitude data for a given state? 

##### First, run the following code to get a list of tuples where the first value is the `state`, the second value is the `lat`, and the third value is the `long`. 

```{python, eval=F}
states_list = list(myDF.loc[:, ["state", "lat", "long"]].dropna().to_records(index=False))
states_list[0:3] # [('az', 34.4554, -114.269), ('or', 46.1837, -123.824), ('sc', 34.9352, -81.9654)]

# to get the first tuple
states_list[0] # ('az', 34.4554, -114.269)

# to get the first value in the first tuple
states_list[0][0] # az

# to get the second tuple
states_list[1] # ('or', 46.1837, -123.824)

# to get the first value in the second tuple
states_list[1][0] # or
```

**Hint:** If you have an issue where you cannot append values to a specific key, make sure to first initialize the specific key to an empty list so the append method is available to use.

##### Now, organize the latitude and longitude data in a dictionary called `geoDict` such that each state from the `state` column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (`lat`) and the second value is the longitude (`long`). For example, the first 2 (lat,long) pairs in Indiana (`"in"`) are:

```{python, eval=F}
geoDict.get("in")[0:2] # [(39.0295, -86.8675), (38.8585, -86.4806)]
len(geoDict.get("in")) # 5687
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_zrd28fgo)

##### Now that you can easily access latitude and longitude pairs for a given state, run the following code to plot the points for Texas (the `state` value is `"tx"`). Include the the graphic produced below in your solution, but feel free to experiment with other states. 

##### NOTE:  You do NOT need to include this portion of Question 5 in your Markdown `.Rmd` file.  We cannot get this portion to build in Markdown, but please do include it in your Python `.py` file.

```{python, eval=F}
from shapely.geometry import Point
import geopandas as gpd
from geopandas import GeoDataFrame

usa = gpd.read_file('/class/datamine/data/craigslist/cb_2018_us_state_20m.shp')
usa.crs = {'init': 'epsg:4269'}

pts = [Point(y,x) for x, y in geoDict.get("tx")]
gdf = gpd.GeoDataFrame(geometry=pts, crs = 4269)
fig, gax = plt.subplots(1, figsize=(10,10))
base = usa[usa['NAME'].isin(['Hawaii', 'Alaska', 'Puerto Rico']) == False].plot(ax=gax, color='white', edgecolor='black')
gdf.plot(ax=base, color='darkred', marker="*", markersize=10)
plt.show()
plt.close()

# to save to jpg:
plt.savefig('q5.jpg')
```

**Relevant topics:** [dicts](#p-dicts), [lists and tuples](#p-lists-and-tuples)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Graphic file (`q5.jpg`) produced for the given state.
```

##### 6. Use your new skills to extract some sort of information from our dataset and create a graphic. This can be as simple or complicated as you are comfortable with!

**Relevant topics:** [dicts](#p-dicts), [lists and tuples](#p-lists-and-tuples)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- The graphic produced using the code.
```

---

### Project 4 {#p04-190}

---

**Motivation:** We've now been introduced to a variety of core Python data structures. Along the way we've touched on a bit of `pandas`, `matplotlib`, and have utilized some control flow features like for loops and if statements. We will continue to touch on `pandas` and `matplotlib`, but we will take a deeper dive in this project and learn more about control flow, all while digging into the data!

**Context:** We just finished a project where we were able to see the power of dictionaries and sets. In this project we will take a step back and make sure we are able to really grasp control flow (if/else statements, loops, etc.) in Python. 

**Scope:** python, dicts, lists, if/else statements, for loops

**Learning objectives:** 

```{block, type="bbox"}
- List the differences between lists & tuples and when to use each.
- Explain what is a dict and why it is useful.
- Demonstrate a working knowledge of control flow in python: if/else statements, while loops, for loops, etc.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/craigslist/vehicles.csv`

#### Questions

##### 1. Unlike in R, where traditional loops are rare and typically accomplished via one of the apply functions, in Python, loops are extremely common and important to understand. In Python, any iterator can be looped over. Some common iterators are: tuples, lists, dicts, sets, `pandas` Series, and `pandas` DataFrames. In the previous project we had some examples of looping over lists, let's learn how to loop over `pandas` Series and Dataframes!

##### Load up our dataset `/class/datamine/data/craigslist/vehicles.csv` into a DataFrame called `myDF`. In project (3), we organized the latitude and longitude data in a dictionary called `geoDict` such that each state from the `state` column is a key, and the respective value is a list of tuples, where the first value in each tuple is the latitude (`lat`) and the second value is the longitude (`long`). Repeat this question, but **do not** use lists, instead use `pandas` to accomplish this.

**Hint:** The data frame has 435,849 rows, and it takes forever to accomplish this with `pandas`.  We just want you to do this one time, to see how slow this is.  Try it first with only 10 rows, and then with 100 rows, and once you are sure it is working, try it with (say) 20,000 rows.  You do not need to do this with the entire data frame.  It takes too long!

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_hkg42wfr)

[Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow](https://mediaspace.itap.purdue.edu/id/1_tr8qvwn3)

**Relevant topics:** [DataFrame.iterrows()](#p-pandas-dataframe),

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```


##### 2. Wow! The solution to question (1) was _slow_. In general, you'll want to avoid looping over large DataFrames. [Here](https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/55557758#55557758) is a pretty good explanation of why, as well as a good system on what to try when computing something. In this case, we could have used indexing to get latitude and longitude values for each state, and would have no need to build this dict.

The method we learned in Project 3, Question 5 is faster and easier!  Just in case you did not solve Project 3, Question 5, here is a fast way to build `geoDict`:
```
import pandas as pd
myDF = pd.read_csv("/class/datamine/data/craigslist/vehicles.csv")
states_list = list(myDF.loc[:, ["state", "lat", "long"]].dropna().to_records(index=False))
geoDict = {}
for mytriple in states_list:
  geoDict[mytriple[0]] = []
for mytriple in states_list:
  geoDict[mytriple[0]].append( (mytriple[1],mytriple[2]) )
```

##### Now we will practice iterating over a dictionary, list, _and_ tuple, all at once! Loop through `geoDict` and use f-strings to print the state abbreviation. Print the first latitude and longitude pair, as well as every 5000th latitude and longitude pair for each state. Round values to the hundreths place. For example, if the state was "pu", and it had 12000 latitude and longitude pairs, we would print the following:

````
pu:
Lat: 41.41, Long: 41.41
Lat: 22.21, Long: 21.21
Lat: 11.11, Long: 10.22
````

##### In the above example, `Lat: 41.41, Long: 41.41` would be the 0th pair, `Lat: 22.21, Long: 21.21` would be the 5000th pair, and `Lat: 11.11, Long: 10.22` would be the 10000th pair. Make sure to use f-strings to round the latitude and longitude values to two decimal places.

There are several ways to solve this question.  You can use whatever method is easiest for you, but please be sure (as always) to add comments to explain your method of solution.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_7j9j7edi)

**Hint:** [Enumerate](#p-enumerate) is a useful function that adds an index to our loop.

**Hint:** Using an if statement and the [modulo operator](https://www.jquery-az.com/python-modulo/) could be useful.

**Note:** Whenever we have a loop _within_ another loop, the "inner" loop is called a "nested" loop, as it is "nested" inside of the other.

**Relevant topics:** [dicts](#p-dicts), modulus operator, [f-strings](#p-f-strings), [if/else](#p-if-else), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. We are curious about how the year of the car (`year`) effects the price (`price`). In R, we could get the median price by year easily, using `tapply`:

```{r, eval=F}
tapply(myDF$price, myDF$year, median, na.rm=T)
```

##### Using `pandas`, we would do this:

```{python, eval=F}
res = myDF.groupby(['year'], dropna=True).median()
```

##### These are very convenient functions that do a lot of work for you. If we were to take a look at the median price of cars by year, it would look like:

```{python, eval=F}
import matplotlib.pyplot as plt

res = myDF.groupby(['year'], dropna=True).median()["price"]
plt.bar(res.index, res.values)
```

##### Using the content of the variable `my_list` provided in the code below, calculate the median car price per year without using the `median` function and without using a `sort` function.  Use only dictionaries, for loops and if statements. Replicate the plot generated by running the code above (you can use the plot to make sure it looks right).

```{python, eval=F}
my_list = list(myDF.loc[:, ["year", "price",]].dropna().to_records(index=False))
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_li98uz29)

**Hint:** If you do not want to write your own median function to find the median, then it is OK to just use the `getMid` function [found here](#p-median) or to use a median function from elsewhere on the web.  Just be sure to cite your source, if you do use a median function that someone else provides or that you use from the internet.  There are many small variations on median functions, especially when it comes to (for instance) lists with even length.

**Hint:** It is also OK to use: `import statistics` and the function `statistics.median`

**Relevant topics:** [dicts](#p-dicts), [for loops](#p-for-loops), [if/else](#p-if-else)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The barplot.
```

##### 4. Now calculate the mean `price` by `year`(still not using pandas code), and create a barplot with the `price` on the y-axis and `year` on the x-axis. Whoa! Something is odd here. Explain what is happening. Modify your code to use an if statement to "weed out" the likely erroneous value. Re-plot your values.

[Click here for video (same as in Question 3)](https://mediaspace.itap.purdue.edu/id/1_li98uz29)

[Click here for another video](https://mediaspace.itap.purdue.edu/id/1_8kgi1uio)

[Click here for one more video](https://mediaspace.itap.purdue.edu/id/1_ladqyiqg)

**Hint:** It is also OK to use a built-in `mean` function, for instace: `import statistics` and the function `statistics.mean`

**Relevant topics:** sorted, [for loops](#p-for-loops), [if/else](#p-if-else), [list methods](#p-list-methods)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The barplot.
```

##### 5. List comprehensions are a neat feature of Python that allows for a more concise syntax for smaller loops. While at first they may seem difficult and more confusing, eventually they grow on you. For example, say you wanted to capitalize every `state` in a list full of states:

```{python, eval=F}
my_states = myDF['state'].to_list()
my_states = [state.upper() for state in my_states]
```

##### Or, maybe you wanted to find the average price of cars in "excellent" condition (without `pandas`):

```{python, eval=F}
my_list = list(myDF.loc[:, ["condition", "price",]].dropna().to_records(index=False))
my_list = [price for (condition, price) in my_list if condition == "excellent"]
sum(my_list)/len(my_list)
```

##### Do the following using list comprehensions, and the provided code:

```{python, eval=F}
my_list = list(myDF.loc[:, ["state", "price",]].dropna().to_records(index=False))
```

- Calculate the average price of vehicles from Indiana (`in`).
- Calculate the average price of vehicles from Indiana (`in`), Michigan (`mi`), and Illinois (`il`) combined.

```{python, eval=F}
my_list = list(myDF.loc[:, ["manufacturer", "year", "price",]].dropna().to_records(index=False))
```

- Calculate the average price of a "honda" (`manufacturer`) that is 2010 or newer (`year`).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_husv3712)

**Relevant topics:** sorted, [for loops](#p-for-loops), [if/else](#p-if-else), [list methods](#p-list-methods), sum, len, defaultdict, [matplotlib](#p-matplotlib)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 6. Let's use a package called `spacy` to try and parse phone numbers out of the `description` column. First, simply loop through and print the text and the label. What is the label of the majority of the phone numbers you can see?

```{python, eval=F}
import spacy

# get list of descriptions
my_list = list(myDF.loc[:, ["description",]].dropna().to_records(index=False))
my_list = [m[0] for m in my_list]

# load the pre-built spacy model
nlp = spacy.load("en_core_web_lg")

# apply the model to a description
doc = nlp(my_list[0])

# print the text and label of each "entity"
for entity in doc.ents:
    print(entity.text, entity.label_)
```

##### Use an if statement to filter out all entities that are not the label you see. Loop through again and see what our printed data looks like. There is still a lot of data there that we _don't_ want to capture, right? Phone numbers in the US are _usually_ 7 (5555555), 8 (555-5555), 10 (5555555555), 11 (15555555555), 12 (555-555-5555), or 14 (1-555-555-5555) digits. In addition to your first "filter", add another "filter" that keeps only text where the text is one of those lengths.

##### That is starting to look better, but there are still some erroneous values. Come up with another "filter", and loop through our data again. Explain what your filter does and make sure that it does a better job on the first 10 documents than when we don't use your filter.

**Note:** If you get an error when trying to knit that talks about "unicode" characters, this is caused by trying to print special characters (non-ascii). An easy fix is just to remove all non-ascii text. You can do this with the `encode` string method. For example:

Instead of: 

```{python, eval=F}
for entity in doc.ents:
    print(entity.text, entity.label_)
```

Do:

```{python, eval=F}
for entity in doc.ents:
    print(entity.text.encode('ascii', errors='ignore'), entity.label_)
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_cnjzidu8)

**Note:** It can be fun to utilize machine learning and natural language processing, but that doesn't mean it is always the best solution! We could get rid of all of our filters and use regular expressions with much better results! We will demonstrate this in our solution.

**Relevant topics:** [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- 1-2 sentences explaining what your filter does.
```


---

### Project 5 {#p05-190}

---

**Motivation:** Up until this point we've utilized bits and pieces of the `pandas` library to perform various tasks. In this project we will formally introduce `pandas` and `numpy`, and utilize their capabilities to solve data-driven problems.

**Context:** By now you'll have had some limited exposure to `pandas`. This is the first in a three project series that covers some of the main components of both the `numpy` and `pandas` libraries. We will take a two project intermission to learn about functions, and then continue.

**Scope:** python, pandas, numpy, DataFrames, Series, ndarrays, indexing

**Learning objectives:** 

```{block, type="bbox"}
- Distinguish the differences between numpy, pandas, DataFrames, Series, and ndarrays.
- Use numpy, scipy, and pandas to solve a variety of data-driven problems.
- Demonstrate the ability to read and write data of various formats using various packages.
- View and access data inside DataFrames, Series, and ndarrays.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/stackoverflow/unprocessed/2018.csv`

`/class/datamine/data/stackoverflow/unprocessed/2018.parquet`

`/class/datamine/data/stackoverflow/unprocessed/2018.feather`

#### Questions

##### 1. Take a look at the [`pandas` docs](https://pandas.pydata.org/docs/reference/io.html). There are a _lot_ of formats that `pandas` has the ability to read. The most popular formats in this course are: csv (with commas or some other separator), excel, json, or some database. CSV is very prevalent, but it was not designed to work well with large amounts of data. Newer formats like parquet and feather are designed from the ground up to be efficient, and take advantage of special processor instruction set called SIMD. The benefits of using these formats can be significant. Let's do some experiments!

##### How much space do each of the following files take up on Scholar: `2018.csv`, `2018.parquet`, and `2018.feather`? How much smaller (as a percentage) is the parquet file than the csv? How much smaller (as a percentage) is the feather file than the csv? Use f-strings to format the percentages.

##### Time reading in the following files: `2018.csv`, `2018.parquet`, and `2018.feather`. How much faster (as a percentage) is reading the parquet file than the csv? How much faster (as a percentage) is reading the feather file than the csv? Use f-strings to format the percentages.

##### To time a piece of code, you can use the `block-timer` package:

```{python, eval=F}
from block_timer.timer import Timer

with Timer(title="Using dict to declare a dict") as t1:
    my_dict = dict()

with Timer(title="Using {} to declare a dict") as t2:
    my_dict = {}

# or if you need more fine-tuned values
print(t1.elapsed)
print(t2.elapsed)
```

##### Read the `2018.csv` file into a `pandas` DataFrame called `my2018`. Time writing the contents of `my2018` to the following files: `2018.csv`, `2018.parquet`, and `2018.feather`. Write the files to your scratch directory: `/scratch/scholar/<username>`, where `<username>` is your username. How much faster (as a percentage) is writing the parquet file than the csv? How much faster (as a percentage) is writing the feather file than the csv? Use f-strings to format the percentages.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_8g9yeay5)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_bbwx9tdi)

**Relevant topics:** [pandas read_csv](#p-pandas-read_csv), [pandas to_csv](#p-pandas-to_csv)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. A _method_ is just a function associated with an object or class. For example, `mean` is just a method of the `pandas` DataFrame:

```{python, eval=F}
# myDF is an object of class DataFrame
# mean is a method of the DataFrame class
myDF.mean()
```

##### In `pandas` there are two main methods used for indexing: [`loc` and `iloc`](https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing). Use the column `Student` and indexing in `pandas` to calculate what percentage of respondents are students and not students. Consider the respondent to be a student if the `Student` column is anything but "No". Create a new DataFrame called `not_students` that is a subset of the original dataset _without_ students. 

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_49qt7vnq)

**Relevant topics:** [loc/iloc/indexing](#p-pandas-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. In `pandas`, if you were to isolate a single column using indexing, like this:

```{python, eval=F}
myDF.loc[:, "Student"]
```

##### The result would be a `pandas` Series. A Series is the 1-dimensional equivalent of a DataFrame. 

```{python, eval=F}
type(myDF.loc[:, "Student"]) # pandas.core.series.Series
```

##### `pandas` and `numpy` make it very easy to convert between a Series, ndarray, and list. [Here](https://miro.medium.com/max/1400/1*rv1JADavAhDKN4-3iM7phQ.png) is a very useful graphic to highlight how to do this. Look at the `DevType` column in `not_students`. As you can see, a single value may contain a list of semi-colon-separated professions. Create a list with a unique group of all the possible professions. Consider each semi-colon-separated value a profession. How many professions are there? 

##### It looks like somehow the profession "Student" got in there even though we filtered by the `Student` column. Use `not_students` to get a subset of our data for which the respondents replied "No" to `Student`, yet put "Student" as one of many possible `DevType`s. How many respondents are in that subset?

**Hint:** If you have a column containing strings in `pandas`, and would like to use string methods on every string in the column, you can use `.str`. For example:

```{python, eval=F}
# this would use the `strip` string method on each value in myColumn, and compare them to ''
# `contains` is another useful string method...
myDF.loc[myDF.loc[:, "myColumn"].str.strip() == '', :]
```

**Hint:** See [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing).

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_53z6v4m4)

**Relevant topics:** [list comprehensions](#p-list-comprehensions), [for loops](#p-for-loops), [loc/iloc/indexing](#p-pandas-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The number of professions there are.
- The number of respondents that replied "No" to `Student`, yet put "Student" as the `DevType`.
```

##### 4. As you can see, while perhaps a bit more strict, indexing in `pandas` is not that much more difficult than indexing in R. While not always necessary, remembering to put ":" to indicate "all columns" or "all rows" makes life easier. In addition, remembering to put parentheses around logical groupings is also a good thing. Practice makes perfect! Randomly select 100 females and 100 males. How many of each sample is in each `Age` category? (_Do not_ use the `sample` method yet, but instead use numeric indexing and `random`)

```{python}
import random

print(f"A random integer between 1 and 100 is {random.randint(1, 101)}")
```

##### It would be nice to visualize these results. `pandas` Series have some built in methods to create plots. Use [this] method to generate a bar plot for both females and males. How do they compare?

**Hint:** You may need to import `matplotlib` in order to display the graphic:

```{python, eval=F}
import matplotlib.pyplot as plt 

# female barplot code here
plt.show()

# male barplot code here
plt.show()
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_aib7hfxy)

**Hint:** Once you have your female and male DataFrames, the `value_counts` method found [here](https://pandas.pydata.org/docs/reference/series.html#computations-descriptive-stats) may be particularly useful.

**Relevant topics:** [list comprehensions](#p-list-comprehensions), [for loops](#p-for-loops), [loc/iloc/indexing](#p-pandas-indexing), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. `pandas` really helps out when it comes to working with data in Python. This is a really cool dataset, use your newfound skills to do a mini-analysis. Your mini-analysis should include 1 or more graphics, along with some interesting observation you made while exploring the data.

**Relevant topics:** 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- A graphic.
- 1-2 sentences explaining your interesting observation and graphic.
```

---

### Project 6 {#p06-190}

---

**Motivation:** Being able to analyze and create good visualizations is a skill that is invaluable in _many_ fields. It can be pretty fun too! In this project, we are going to take a small hiatus from the regular stream of projects to do some data visualizations.

**Context:** We've been working hard all semester and learning valuable skills. In this project we are going to ask you to examine some plots, write a little bit, and use your creative energies to create good visualizations about the flight data.

**Scope:** python, r, visualizing data

**Learning objectives:** 

```{block, type="bbox"}
- Demostrate the ability to create basic graphs with default settings.
- Demonstrate the ability to modify axes labels and titles.
- Demonstrate the ability to customize a plot (color, shape/linetype).
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/flights/*.csv` (all csv files)

#### Questions

##### 1. [Here](http://stat-computing.org/dataexpo/2009/posters/) are the results from the 2009 Data Expo poster competition. The object of the competition was to visualize interesting information from the flights dataset. Examine all 8 posters and write a single sentence for each poster with your first impression(s). An example of an impression that will not get full credit would be: "My first impression is that this poster is bad and doesn't look organized.". An example of an impression that will get full credit would be: "My first impression is that the author had a good visualization-to-text ratio and it seems easy to follow along.".

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_43j9xh4d)

```{block, type="bbox"}
**Item(s) to submit:**

- 8 bullets, each containing a sentence with the first impression of the 8 visualizations. Order should be "first place", to "honourable mention", followed by "other posters" in the given order. Or, label which graphic each sentence is about.
```

##### 2. [Creating More Effective Graphs](https://www.amazon.com/dp/0985911123/) by Dr. Naomi Robbins and [The Elements of Graphing Data](https://www.amazon.com/Elements-Graphing-Data-William-Cleveland/dp/0963488414/ref=sr_1_1?dchild=1&keywords=elements+of+graphing+data&qid=1614013761&sr=8-1) by Dr. William Cleveland at Purdue University, are two excellent books about data visualization. Read the following excerpts from the books (respectively), and list 2 things you learned, or found interesting from _each_ book. 

- [Excerpt 1](./files/CreatingMoreEffectiveGraphs.pdf)
- [Excerpt 2](./files/ElementsOfGraphingData.pdf)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_6c1ge176)

```{block, type="bbox"}
**Item(s) to submit:**

- Two bullets for each book with items you learned or found interesting.
```

##### 3. Of the 7 posters with at least 3 plots and/or maps, choose 1 poster that you think you could improve upon or "out plot". Create 4 plots/maps that either:

1. Improve upon a plot from the poster you chose, or
2. Show a completely different plot that does a good job of getting an idea or observation across, or
3. Ruin a plot. Purposefully break the best practices you've learned about in order to make the visualization misleading. (limited to 1 of the 4 plots)

##### For each plot/map where you choose to do (1), include 1-2 sentences explaining what exactly you improved upon and how. Point out some of the best practices from the 2 provided texts that you followed. For each plot/map where you choose to do (2), include 1-2 sentences explaining your graphic and outlining the best practices from the 2 texts that you followed. For each plot/map where you choose to do (3), include 1-2 sentences explaining what you changed, what principle it broke, and how it made the plot misleading or worse. 

##### While we are not asking you to create a poster, please use RMarkdown to keep your plots, code, and text nicely formatted and organized. The more like a story your project reads, the better. You are free to use either R or Python or both to complete this project. Please note that it would be unadvisable to use an interactive plotting package like `plotly`, as these packages will not render plots from within RMarkdown in RStudio.

##### Some useful R packages:

- [base R functions](#r-plotting): bar, plot, lines, etc.
- [usmap](#r-plot_usmap)
- [ggplot](https://uc-r.github.io/ggplot_intro)

##### Some useful Python packages:

- [matplotlib](https://thedatamine.github.io/the-examples-book/python.html#p-matplotlib)
- [plotnine](https://plotnine.readthedocs.io/en/stable/#)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_kmvuq0zc)

```{block, type="bbox"}
**Item(s) to submit:**

- All associated R/Python code you used to wrangling the data and create your graphics.
- 4 plots, with at least 4 associated RMarkdown code chunks.
- 1-2 sentences per plot explaining what exactly you improved upon, what best practices from the texts you used, and how. If it is a brand new visualization, describe and explain your graphic, outlining the best practices from the 2 texts that you followed. If it is the ruined plot you chose, explain what you changed, what principle it broke, and how it made the plot misleading or worse.
```

##### 4. Now that you've been exploring data visualization, copy, paste, and update your first impressions from question (1) with your updated impressions. Which impression changed the most, and why?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_3fkl4ln3)

```{block, type="bbox"}
**Item(s) to submit:**

- 8 bullets with updated impressions (still just a sentence or two) from question (1). 
- A sentence explaining which impression changed the most and why.
```

---

### Project 7 {#p07-190}

---

**Motivation:** There is one pretty major topic that we have yet to explore in Python -- functions! A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code. 

**Context:** We are taking a small hiatus from our `pandas` and `numpy` focused series to learn about and write our own functions in Python!

**Scope:** python, functions, pandas

**Learning objectives:** 

```{block, type="bbox"}
- Comprehend what a function is, and the components of a function in Python.
- Differentiate between positional and keyword arguments.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/yelp/data/parquet`

#### Questions

##### 1. You've been given a path to a folder for a dataset. Explore the files. Give a brief description of the files and what each file contains.

**Note:** Take a look at the size of each of the files. If you are interested in experimenting, try using `pandas` `read_json` function to read the `yelp_academic_dataset_user.json` file in the json folder `/class/datamine/data/yelp/data/json/yelp_academic_dataset_user.json`. Even with the large amount of memory available to you, this should fail. In order to make it work you would need to use the `chunksize` option to read the data in bit by bit. Now consider that the `reviews.parquet` file is .3gb _larger_ than the `yelp_academic_dataset_user.json` file, but can be read in with no problem. That is seriously impressive!

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_lbvcdjyw)

[Click here for another video](https://mediaspace.itap.purdue.edu/id/1_h13rpztv)

**Relevant topics:** [read_parquet](#p-pandas-read_parquet)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The name of each dataset and a brief summary of each dataset. No more than 1-2 sentences about each dataset.
```

##### 2. Read the `businesses.parquet` file into a `pandas` DataFrame called `businesses`. Take a look to the `hours` and `attributes` columns. If you look closely, you'll observe that both columns contain a lot more than a single feature. In fact, the `attributes` column contains 39 features and the `hours` column contains 7!

```{python, eval=F}
len(businesses.loc[:, "attributes"].iloc[0].keys()) # 39
len(businesses.loc[:, "hours"].iloc[0].keys()) # 7
```

##### Let's start by writing a simple function. Create a function called `has_attributes` that takes a `business_id` as an argument, and returns `True` if the business has any `attributes` and `False` otherwise. Test it with the following code:

```{python, eval=F}
print(has_attributes('f9NumwFMBDn751xgFiRbNA')) # True
print(has_attributes('XNoUzKckATkOD1hP6vghZg')) # False
print(has_attributes('Yzvjg0SayhoZgCljUJRF9Q')) # True
print(has_attributes('7uYJJpwORUbCirC1mz8n9Q')) # False
```

##### While this is useful to get whether or not a single business has any attributes, if you wanted to apply this function to the entire `attributes` column/Series, you would just use the `notna` method:

```{python, eval=F}
businesses.loc[:, "attributes"].notna()
```

**Important note:** Make sure your return value is of type `bool`. To check this:

```{python, eval=F}
type(True) # bool
type("True") # str
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_l2b5g5pb)

[Click here for another video](https://mediaspace.itap.purdue.edu/id/1_jcnolv45)

**Relevant topics:** [pandas indexing](#p-pandas-indexing), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running the provided "test" code.
```

##### 3. Take a look at the `attributes` of the first business:

```{python, eval=F}
businesses.loc[:, "attributes"].iloc[0]
```

##### What is the type of the value? Let's assume the company you work for gets data formatted like `businesses` each week, but your boss wants the 39 features in `attributes` and the 7 features in `hours` to become their own columns. Write a function called `fix_businesses_data` that accepts an argument called `data_path` (of type `str`) that is a full path to a parquet file that is in the exact same format as `businesses.parquet`. In addition to the `data_path` argument, `fix_businesses_data` should accept another argument called `output_dir` (of type `str`). `output_dir` should contain the path where you want your "fixed" parquet file to output. `fix_businesses_data` should return `None`.

##### The result of your function, `new_businesses.parquet`, should no longer contain either the `attributes` or `hours` columns. Instead, each row should contain 39+7=46 new columns. Test your function out:

```{python, eval=F}
from pathlib import Path

my_username = "kamstut" # replace "kamstut" with YOUR username
fix_businesses_data(data_path="/class/datamine/data/yelp/data/parquet/businesses.parquet", output_dir=f"/scratch/scholar/{my_username}")

# see if output exists
p = Path(f"/scratch/scholar/{my_username}").glob('**/*')
files = [x for x in p if x.is_file()]
print(files)
```

**Important note:** Make sure that either `/scratch/scholar/{my_username}` or `/scratch/scholar/{my_username}/` will work as arguments to `output_dir`. If you use the `pathlib` library, as shown in the provided function "skeleton" below, both will work automatically!

```{python, eval=F}
from pathlib import Path

def fix_businesses_data(data_path: str, output_dir: str) -> None:
    """
    fix_data accepts a parquet file that contains data in a specific format. 
    fix_data "explodes" the attributes and hours columns into 39+7=46 new 
    columns.
    Args:
        data_path (str): Full path to a file in the same format as businesses.parquet.
        output_dir (str): Path to a directory where new_businesses.parquet should be output.
    """
    # read in original parquet file
    businesses = pd.read_parquet(data_path)
    
    # unnest the attributes column
    
    # unnest the hours column
    
    # output new file
    businesses.to_parquet(str(Path(f"{output_dir}").joinpath("new_businesses.parquet")))
    
    return None
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_zf4bl8g4)

**Hint:** Check out the code below, notice how using `pathlib` handles whether or not we have the trailing `/`.

```{python, eval=F}
from pathlib import Path

print(Path("/class/datamine/data/").joinpath("my_file.txt"))
print(Path("/class/datamine/data").joinpath("my_file.txt"))
```

**Hint:** You can test out your function on `/class/datamine/data/yelp/data/parquet/businesses_small.parquet` to not waste as much time.

**Hint:** If we were using R and the `tidyverse` package, this sort of behavior is called "unnesting". You can read more about it [here](https://tidyr.tidyverse.org/reference/nest.html).

**Hint:** [This](https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns) stackoverflow post should be _very_ useful! Specifically, run this code and take a look at the output:

```{python, eval=F}
businesses
businesses.loc[0:4, "attributes"].apply(pd.Series)
```

Notice that some rows have json, and others have `None`:

```{python, eval=F}
businesses.loc[0, "attributes"] # has json
businesses.loc[2, "attributes"] # has None
```

This method allows us to handle both cases. If the row has json it converts the values, if it has `None` it just puts each column with a value of `None`.

**Hint:** [Here](https://stackoverflow.com/questions/44723377/pandas-combining-two-dataframes-horizontally) is an example that shows you how to concatenate (combine) dataframes.

**Relevant topics:** [pandas indexing](#p-pandas-indexing), [functions](#p-writing-functions), [concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html), [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. That's a pretty powerful function, and could definitely be useful. What if, instead of working on just our specifically formatted parquet file, we wrote a function that worked for _any_ `pandas` DataFrame? Write a function called `unnest` that accepts a `pandas` DataFrame as an argument (let's call this argument `myDF`), and a list of columns (let's call this argument `columns`), and returns a DataFrame where the provided columns are unnested. 

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_0yb372sp)

**Hint:** The following should work:

```{python, eval=F}
businesses = pd.read_parquet("/class/datamine/data/yelp/data/parquet/businesses.parquet")

new_businesses_df = unnest(businesses, ["attributes", ])
new_businesses_df.shape # (209393, 39)
new_businesses_df.head()

new_businesses_df = unnest(businesses, ["attributes", "hours"])
new_businesses_df.shape # (209393, 46)
new_businesses_df.head()
```

**Relevant topics:** [pandas indexing](#p-pandas-indexing), [functions](#p-writing-functions), [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html),
[drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running the provided code.
```

##### 5. Try out the code below. If a provided column isn't already nested, the column name is ruined and the data is changed. If the column doesn't already exist, a KeyError is thrown. Modify our function from question (4) to skip unnesting if the column doesn't exist. In addition, modify the function from question (4) to skip the column if the column isn't nested. Let's consider a column nested if the value of the column is a `dict`, and not nested otherwise.

```{python, eval=F}
businesses = pd.read_parquet("/class/datamine/data/yelp/data/parquet/businesses.parquet")

new_businesses_df = unnest(businesses, ["doesntexist",]) # KeyError
new_businesses_df = unnest(businesses, ["postal_code",]) # not nested
```

##### To test your code, run the following. The result should be a DataFrame where `attributes` has been unnested, and that is it.

```{python, eval=F}
businesses = pd.read_parquet("/class/datamine/data/yelp/data/parquet/businesses.parquet")
results = unnest(businesses, ["doesntexist", "postal_code", "attributes"])
results.shape # (209393, 39)
results.head()
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_0e4q4ley)

**Hint:** To see if a variable is a `dict` you could use `type`:

```{python, eval=T}
my_variable = {'key': 'value'}
type(my_variable)
```

**Relevant topics:** [pandas indexing](#p-pandas-indexing), [functions](#p-writing-functions), [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html),
[drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running the provided code.
```

---

## STAT 29000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Web scraping: part I|xml, lxml, pandas,  etc.|
|Python|2|Web scraping: part II|requests, functions, xml, loops, if statements, etc.|
|Python|3|Web scraping: part III|selenium, lxml, lists, pandas, etc.|
|Python|4|Web scraping: part IV|requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc.|
|Python|5|Web scraping: part V|web scraping + related topics|
|Python|6|Plotting in Python: part I|ways to plot in Python, more work with pandas, etc.|
|Python|7|Plotting in Python: part II|ways to plot in Python, more work with pandas, etc.|
|Python|8|Writing scripts: part I|how to write scripts in Python, more work with pandas, matplotlib, etc.|
|Python|9|Writing scripts: part II|how to write scripts in Python, argparse, more work with pandas, matplotlib, etc.|
|R|10|ggplot: part I|ggplot basics|
|R|11|ggplot: part II|more ggplot|
|R|12|tidyverse & data.table: part I|data wrangling and computation using tidyverse packages and data.table|
|R|13|tidyverse & data.table: part II|data wrangling and computation using tidyverse packages and data.table|
|R|14|tidyverse & data.table: part III|data wrangling and computation using tidyverse packages and data.table|

### Project 1 {#p01-290}

---

**Motivation:** Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like [parquet](https://arrow.apache.org/faq/) and [protobufs](https://developers.google.com/protocol-buffers) are becoming more common.

**Context:** In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the `lxml` package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python.

**Scope:** python, XML

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Match XML terms to sections of XML demonstrating working knowledge.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Resources

We realize that for many of you this is a big "jump" right into Python. Don't worry! Python is a very intuitive language with a clean syntax. It is easy to read and write. We will do our very best to keep things as straightforward as possible, especially in the early learning stages of the class.

We will be actively updating the examples book with videos and more examples throughout the semester. Ask a question in Piazza and perhaps we will add an example straight to the book to help out.

Some potentially useful resources for the semester include:

- The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week.
- [Here](https://towardsdatascience.com/cheat-sheet-for-python-dataframe-r-dataframe-syntax-conversions-450f656b44ca) is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python.
- [The Examples Book](https://thedatamine.github.io/the-examples-book/) -- updating daily with more examples and videos. Be sure to click on the "relevant topics" links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign.

#### Questions

**Important note:** It would be well worth your time to read through the [xml section](#xml) of the book, as well as take the time to work through [pandas 10 minute intro](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).

##### 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you _don't_ have the documentation. Start by finding the "root" node. What is the name of the root node of the provided dataset?

**Hint:** Make sure to import the `lxml` package first:

```{python, eval=F}
from lxml import etree
```

Here are two videos about running Python in RStudio:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

and here is a video about XML scraping in Python:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1ywsbxyp)

**Relevant topics:** [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from lxml import etree

tree = etree.parse("/class/datamine/data/apple/health/watch_dump.xml")

tree.xpath("/*")[0].tag
```

##### 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next "tier" of elements?

**Hint:** Now that we know the root node, you could use the root node name as a part of your xpath expression.

**Hint:** As you may have noticed in question (1) the `xpath` method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second "tier" elements, you could convert the resulting `list` to a `set` to quickly see the unique list as `set`'s only contain unique values.

**Relevant topics:** [for loops](#p-for-loops), [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
set([x.tag for x in tree.xpath("/HealthData/*")])
```

##### 3. Continue to explore each "tier" of data until there isn't any left. Name the "full paths" of all of the "last tier" tags.

**Hint:** Let's say a "last tier" tag is just a path where there are no more nested elements. For example, `/HealthData/Workout/WorkoutRoute/FileReference` is a "last tier" tag. If you try and get the nested elements for it, they don't exist:

```{python, eval=F}
tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")
```

**Hint:** Here are 3 of the 7 "full paths":

````
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Record/MetadataEntry
/HealthData/ActivitySummary
````

**Relevant topics:** [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
print(set([x.tag for x in tree.xpath("/HealthData/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/ActivitySummary/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/MetadataEntry/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEvent/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEntry/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/MetadataEntry/*")]))
```

```{block, type="solution"}
/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Workout/WorkoutRoute/MetadataEntry
/HealthData/Record/MetadataEntry
/HealthData/Workout/WorkoutEvent
/HealthData/Workout/WorkoutEntry
/HealthData/ActivitySummary
```

##### 4. At this point in time you may be asking yourself "but where is the data"? Depending on the structure of the XML file, the data could either be between tags like:

```{html, eval=F}
<some_tag>mydata</some_tag>
```

##### Or, it could be in an attribute:

```{html, eval=F}
<question answer="tac">What is cat spelled backwards?</question>
```

##### Collect the "ActivitySummary" data, and convert the list of dicts to a `pandas` DataFrame. The following is an example of converting a list of dicts to a `pandas` DataFrame called `myDF`:

```{python, eval=F}
import pandas as pd

list_of_dicts = []
list_of_dicts.append({'columnA': 1, 'columnB': 2})
list_of_dicts.append({'columnB': 4, 'columnA': 1}) 

myDF = pd.DataFrame(list_of_dicts)
```

**Hint:** It is important to note that an element's "attrib" attribute looks and feels like a `dict`, but it is actually a `lxml.etree._Attrib`. If you try to convert a list of `lxml.etree._Attrib` to a `pandas` DataFrame, it will not work out as you planned. Make sure to first convert each `lxml.etree._Attrib` to a `dict` before converting to a DataFrame. You can do so like:

```{python, eval=F}
# this will convert a single `lxml.etree._Attrib` to a dict
my_dict = dict(my_lxml_etree_attrib)
```

**Relevant topics:** [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
dat = tree.xpath("/HealthData/ActivitySummary")
list_of_dicts = []

for e in dat:
    list_of_dicts.append(dict(e.attrib))
    
myDF = pd.DataFrame(data=list_of_dicts)
myDF.sort_values(['activeEnergyBurned'], ascending=False).head()
```

##### 5. `pandas` is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the [pandas in 10 minutes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) page from the official documentation. Sort, find, and print the top 5 rows of data based on the "activeEnergyBurned" column. 

**Relevant topics:** [pandas](#p-pandas), [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything
```

---

### Project 2 {#p02-290}

---

**Motivation:** Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering.   

**Context:** In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions.  

**Scope:** python, web scraping, xml

**Learning objectives:** [html](#html)

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

You will be extracting your own data from online in this project. There is no base dataset.

#### Questions

##### 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. 

##### By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on "view page source", which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on "inspect element". This will pull up an inspector that allows you to see portions of the HTML.

##### Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime.

##### For example:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

![](./images/nytimes_image.jpg)

##### Copy and paste the **h1** element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_eew0i16y)

**Relevant topics:** [html](#html)

```{block, type="bbox"}
**Item(s) to submit:**

- 2 code chunks containing the HTML requested.
```

##### Solution

```{html, class.source="solution"}
<h1 id="link-4686dc8b" class="css-rsa88z e1h9rw200" data-test-id="headline">U.S. Says Chinas Repression of Uighurs Is Genocide</h1>
```

```{html, class.source="solution"}
<p id="article-summary" class="css-w6ymp8 e1wiw3jv0">The finding by the Trump administration is the strongest denunciation by any government of Chinas actions and follows a Biden campaign statement with the same declaration.</p>
```

##### 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for _new_ data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. 

##### For example, given a link to a new nytimes article, do you think you could isolate the article title by using the `id="link-4686dc8b"` attribute of the **h1** tag? Maybe, or maybe not, but it sure seems like "link-4686dc8b" might be unique to the article and not able to be used given a new article.

##### Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. 

**Important note:** You do _not_ need to test your xpath expression yet, we will be doing that shortly.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath)

```{block, type="bbox"}
**Item(s) to submit:**

- Two xpath expressions in an HTML code chunk. 
```

##### Solution

```{html, class.source="solution"}
//h1[@data-test-id="headline"]
```

```{html, class.source="solution"}
//p[@id="article-summary"]
```

##### 3. Use the `requests` package to scrape the webpage containing our article from questions (1) and (2). Use the `lxml.html` package and the `xpath` method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qw2xb058)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
import lxml.html
import requests

url =  "https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html"
response = requests.get(url, stream=True)
response.raw.decode_content = True
tree = lxml.html.parse(response.raw)

print(tree.xpath('//p[@id="article-summary"]')[0].text)
print(tree.xpath('//h1[@data-test-id="headline"]')[0].text)
```

##### 4. Here are a list of article links from https://nytimes.com:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html

https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html

##### Write a function called `get_article_and_summary` that accepts a string called `link` as an argument, and returns both the article title and summary. Test `get_article_and_summary` out on each of the provided links:

```{python, eval=F}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

**Hint:** The first line of your function should look like this:

`def get_article_and_summary(myURL: str) -> (str, str):`

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_jrtrt5fo)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_7yhabkeg)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from typing import Tuple
import lxml.html
import requests

def get_article_and_summary(link: str) -> Tuple[str, str]:
    """
    Given a link to a new york times article, return the article title and summary.

    Args:
        link (str): The link to the new york times article.

    Returns:
        Tuple[str, str]: A tuple first containing the article title, and then the article summary.
    """
    
    # scrape the web page
    response = requests.get(link, stream=True)
    response.raw.decode_content = True
    tree = lxml.html.parse(response.raw)
    
    title = tree.xpath('//p[@id="article-summary"]')[0].text
    summary = tree.xpath('//h1[@data-test-id="headline"]')[0].text
    
    return title, summary
```

```{python, eval=F, class.source="solution"}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

##### 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose **one** other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information.

**Important note:** If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from typing import Tuple
import lxml.html
import requests
import io
from PIL import Image
from IPython.display import display

def get_article_and_summary(link: str) -> Tuple[str, str]:
    """
    Given a link to a new york times article, return the article title and summary.

    Args:
        link (str): The link to the new york times article.

    Returns:
        Tuple[str, str]: A tuple first containing the article title, and then the article summary.
    """
    
    # scrape the web page
    response = requests.get(link, stream=True)
    response.raw.decode_content = True
    tree = lxml.html.parse(response.raw)
    
    # parse out the title
    title = tree.xpath('//p[@id="article-summary"]')[0].text
    
    # parse out the summary
    summary = tree.xpath('//h1[@data-test-id="headline"]')[0].text
    
    # parse out the url to the image
    photo_src = tree.xpath('//picture/img')[0].attrib.get("src")
    
    # scrape image
    photo_content = requests.get(photo_src).content
    
    # convert image format 
    photo_file = io.BytesIO(photo_content)
    photo = Image.open(photo_file).convert('RGB')    
    
    # parse out photo caption
    caption = tree.xpath('//figcaption/span')[0].text
    
    # parse out the photo credits
    credits = tree.xpath('//figcaption/span/span[contains(text(), "Credit")]/following-sibling::span/span')
    
    credits_list = []
    for c in credits:
        credits_list.append(c.text)
        
    # parse author photo url
    # only gets if "author" in photo src attribute
    photo_src_elements = tree.xpath('//img[contains(@src, "author")]')
    
    # if "author" in photo src attribute
    photo_srcs = []
    for p in photo_src_elements:
        photo_srcs.append(p.attrib.get("src"))
    
    # scrape image
    author_images = []
    for img in photo_srcs:
        photo_content = requests.get(img).content
    
        # convert image format 
        photo_file = io.BytesIO(photo_content)
        photo = Image.open(photo_file).convert('RGB')    
        
        author_images.append(photo)
        
    # parse out authors
    authors_elements = tree.xpath("//span[@class='byline-prefix']/following-sibling::a/span")
    authors = []
    for a in authors_elements:
        authors.append(a.text)
        
    # parse out article publish date/time
    dt = tree.xpath("//time")[0].attrib.get("datetime")
    
    return title, summary, photo, caption, credits, author_images, authors, dt


title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
```

---


### Project 3 {#p03-290}

---

**Motivation:** Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. 

**Context:** In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. 

**Scope:** python, web scraping, selenium

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using `selenium` write Python code that that first finds the `input` element, and then types "West Lafayette, IN" followed by an emulated "Enter/Return". Confirm you code works by printing the url after that process completes.

**Hint:** You will want to use `time.sleep` to pause a bit after the search so the updated url is returned.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ztju698f)

That video is already relevant for Question 2 too.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml)  

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Use your code from question (1) to test out the following queries:

- West Lafayette, IN (City, State)
- 47906 (Zip)
- 4505 Kahala Ave, Honolulu, HI 96816 (Full address)

##### If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful.

##### If you used xpath expressions to complete question (1), instead use a [different method](https://selenium-python.readthedocs.io/locating-elements.html#locating-elements) to find the `input` element. If you used a different method, use xpath expressions to complete question (1).

**Relevant topics:** [selenium](#p-selenium), [xml](#xml) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. Let's call the page after a city/state or zipcode search a "sales page". For example:

![](./images/trulia.png)

##### Use `requests` to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use `lxml.html` to parse the page and get all of the `img` elements that make up the house pictures on the left side of the website.

**Important note:** Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have:

```{python, eval=F}
import requests
response = requests.get(...)
print(response.text)
```

**Hint:** Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox:

```{python, eval=F}
import requests
my_headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(..., headers=my_headers)
```

##### Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the `img` elements and try to print the value of the `src` attribute, this will be clear:

```{python, eval=F}
import lxml.html
tree = lxml.html.fromstring(response.text)
elements = tree.xpath("//img")
for element in elements:
    print(element.attrib.get("src"))
```

##### This is because the webpage is not immediately, _completely_ loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images still needing to finish rendering all of the way, slowly. What we need to do to fix this, is use `selenium` (instead of `lxml.html`) to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements:

```{python, eval=F}
# driver setup and get the url

# Needed to get the window size set right and scroll in headless mode
myheight = driver.execute_script('return document.body.scrollHeight')
driver.set_window_size(1080,myheight+100)

def scroll(driver, scroll_point):  
    driver.execute_script(f'window.scrollTo(0, {scroll_point});')
    time.sleep(5) 
    
scroll(driver, myheight*1/4)
scroll(driver, myheight*2/4)
scroll(driver, myheight*3/4)
scroll(driver, myheight*4/4)

# find_elements_by_*
```

**Hint:** At the time of writing there should be about 86 links to images of homes.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_v9iesfnc)

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Write a function called `avg_house_cost` that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for "3+" beds and _then_ find the average. Test `avg_house_cost` out on the zip code `47906` and print the average costs.

**Important note:** Use `selenium` to "click" on the "3+ beds" filter.

**Hint:** If you get an error that tells you `button` is not clickable because it is covered by an `li` element, try clicking on the `li` element instead. 

**Hint:** You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything.

**Hint:** Your results may end up including prices for "Homes Near \<ZIPCODE\>". This is okay. Even better if you manage to remove those results. If you _do_ choose to remove those results, take a look at the `data-testid` attribute with value `search-result-list-container`. Perhaps only selecting the children of the first element will get the desired outcome.

**Hint:** You can use the following code to remove the non-numeric text from a string, and then convert to an integer:

```{python, eval=F}
import re

int(re.sub("[^0-9]", "", "removenon45454_numbers$"))
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ahm64jjz)

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops), [functions](#p-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. Get creative. Either add an interesting feature to your function from (4), or use `matplotlib` to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---

### Project 4 {#p04-290}

---

**Motivation:** In this project we will continue to hone your web scraping skills, introduce you to some "gotchas", and give you a little bit of exposure to a powerful tool called cron.

**Context:** We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. 

**Scope:** python, web scraping, selenium, cron

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Check out the following website: https://project4.tdm.wiki

##### Use `selenium` to scrape and print the 6 colors of pants offered.

[Click here for video for Question 1](https://mediaspace.itap.purdue.edu/id/1_j3ik06bo)

[Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow](https://mediaspace.itap.purdue.edu/id/1_tr8qvwn3)

**Hint:** You _may_ have to interact with the webpage for certain elements to render.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state `A` and state `B`). Upon refreshing the website, or scraping the website again, there is an $x\%$ chance that the website will be in state `A` and a $1-x\%$ chance the website will be in state `B`.

##### Describe the two states (the thing (element or set of elements) that changes as you refresh the page), and scrape the website enough to estimate $x$.

[Click here for video for Questions 2 and 3](https://mediaspace.itap.purdue.edu/id/1_2zaiodzi)

**Hint:** You _will_ need to interact with the website to "see" the change.

**Hint:** Since we are just asking about a state, and not any specific element, you could use the `page_source` attribute of the `selenium` driver to scrape the entire page instead of trying to use xpath expressions to find a specific element.

**Hint:** Your estimate of $x$ does not need to be perfect.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- What state `A` and `B` represent.
- An estimate for `x`.
```

##### 3. Dig into the changing "thing" from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? 

[Click here for video (same as above) for Questions 2 and 3](https://mediaspace.itap.purdue.edu/id/1_2zaiodzi)

**Hint:** Due to the changes that occur when a button is clicked, I'd highly advice you to use the `data-color` attribute in your xpath expression instead of `contains(text(), 'blahblah')`.

**Hint:** `parent::` and `following-sibling::` may be useful [xpath axes](https://www.w3schools.com/xml/xpath_axes.asp) to use.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example using `following-sibling::`](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works.

[Click here for video for Questions 4 and 5](https://mediaspace.itap.purdue.edu/id/1_dt6j3sov)

**Important note:** Do **NOT** include your password in your homework submission. Any time you need to type your password in you final submission just put something like "SUPERSECRETPASSWORD" or "MYPASSWORD".

**Hint:** To include an image (or screenshot) in RMarkdown, try `![](./my_image.png)` where `my_image.png` is inside the same folder as your `.Rmd` file.

**Hint:** The spacing and tabs near the `message` variable are very important. Make sure to copy the code exactly. Otherwise, your subject may not end up in the subject of your email, or the email could end up being blank when sent.

**Hint:** Questions 4 and 5 were inspired by examples and borrowed from the code found at the [Real Python](https://realpython.com/python-send-email/) website.

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message):
  import smtplib, ssl
  from email.mime.text import MIMEText
  from email.mime.multipart import MIMEMultipart
  
  message = MIMEMultipart("alternative")
  message["Subject"] = my_subject
  message["From"] = my_purdue_email
  message["To"] = to
  
  # Create the plain-text and HTML version of your message
  text = f'''\
Subject: {my_subject}
To: {to}
From: {my_purdue_email}
  
{my_message}'''
  html = f'''\
<html>
  <body>
    {my_message}
  </body>
</html>
'''
  # Turn these into plain/html MIMEText objects
  part1 = MIMEText(text, "plain")
  part2 = MIMEText(html, "html")
  
  # Add HTML/plain-text parts to MIMEMultipart message
  # The email client will try to render the last part first
  message.attach(part1)
  message.attach(part2)
  
  context = ssl.create_default_context()
  with smtplib.SMTP("smtp.purdue.edu", 587) as server:
    server.ehlo()  # Can be omitted
    server.starttls(context=context)
    server.ehlo()  # Can be omitted
    server.login(my_purdue_email, my_password)
    server.sendmail(my_purdue_email, to, message.as_string())
        
# this sends an email from kamstut@purdue.edu to mdw@purdue.edu
# replace supersecretpassword with your own password
# do NOT include your password in your homework submission.
send_purdue_email("kamstut@purdue.edu", "supersecretpassword", "mdw@purdue.edu", "put subject here", "put message body here")
```

**Relevant topics:** [functions](#p-writing-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- Screenshot showing your received the email.
```

##### 5. The following is the content of a new Python script called `is_in_stock.py`:

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message):
  import smtplib, ssl
  from email.mime.text import MIMEText
  from email.mime.multipart import MIMEMultipart
  
  message = MIMEMultipart("alternative")
  message["Subject"] = my_subject
  message["From"] = my_purdue_email
  message["To"] = to
  
  # Create the plain-text and HTML version of your message
  text = f'''\
Subject: {my_subject}
To: {to}
From: {my_purdue_email}
  
{my_message}'''
  html = f'''\
<html>
  <body>
    {my_message}
  </body>
</html>
'''
  # Turn these into plain/html MIMEText objects
  part1 = MIMEText(text, "plain")
  part2 = MIMEText(html, "html")
  
  # Add HTML/plain-text parts to MIMEMultipart message
  # The email client will try to render the last part first
  message.attach(part1)
  message.attach(part2)
  
  context = ssl.create_default_context()
  with smtplib.SMTP("smtp.purdue.edu", 587) as server:
    server.ehlo()  # Can be omitted
    server.starttls(context=context)
    server.ehlo()  # Can be omitted
    server.login(my_purdue_email, my_password)
    server.sendmail(my_purdue_email, to, message.as_string())
        
def main():
    # scrape element from question 3
    
    # does the text indicate it is in stock?
    
    # if yes, send email to yourself telling you it is in stock.
    
    # otherwise, gracefully end script using the "pass" Python keyword

if __name__ == "__main__":
    main()
```

##### First, make a copy of the script in your `$HOME` directory:

```{bash, eval=F}
cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py
```

##### If you now look in the "Files" tab in the lower right hand corner of RStudio, and click the refresh button, you should see the file `is_in_stock.py`. You can open and modify this file directly in RStudio. Before you do so, however, change the permissions of the `$HOME/is_in_stock.py` script so only YOU can read, write, and execute it:

```{bash, eval=F}
chmod 700 $HOME/is_in_stock.py
```

##### The script should now appear in RStudio, in your home directory, with the correct permissions. Open the script (in RStudio) and fill in the `main` function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not.

##### A cron job is a task that runs at a certain interval. Create a cron job that runs your script, `/class/datamine/apps/python/f2020-s2021/env/bin/python $HOME/is_in_stock.py` every 5 minutes. Wait 10-15 minutes and verify that it is working properly. The long path, `/class/datamine/apps/python/f2020-s2021/env/bin/python` simply makes sure that our script is run with access to all of the packages in our course environment. `$HOME/is_in_stock.py` is the path to your script (`$HOME` expands or transforms to `/home/<my_purdue_alias>`.

[Click here for video (same as above) for Questions 4 and 5](https://mediaspace.itap.purdue.edu/id/1_dt6j3sov)

[Click here for a longer video about setting up the cronjob in Question 5](https://mediaspace.itap.purdue.edu/id/1_xz9df91y)

**Hint:** If you struggle to use the text editor used with the `crontab -e` command, be sure to continue reading the [cron section](#cron) of the book. We highlight another method that may be easier.

**Hint:** Don't forget to copy your import statements from question (3) as well.

**Important note:** Once you are finished with the project, if you no longer wish to receive emails every so often, follow the instructions [here](#how-to-delete-cron-job) to remove the cron job.

**Relevant topics:** [cron](#cron), [crontab guru](https://crontab.guru)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The content of your cron job in a bash code chunk.
- The content of your `is_in_stock.py` script.
```

---

### Project 5 {#p05-290}

---

**Motivation:** One of the best things about learning to scrape data is the many applications of the skill that may pop into your mind. In this project, we want to give you some flexibility to explore your own ideas, but at the same time, add a couple of important tools to your tool set. We hope that you've learned a lot in this series, and can think of creative ways to utilize your new skills.

**Context:** This is the last project in a series focused on scraping data. We have created a couple of very common scenarios that can be problematic when first learning to scrape data, and we want to show you how to get around them.

**Scope:** python, web scraping, etc.

**Learning objectives:** 

```{block, type="bbox"}
- Use the requests package to scrape a web page.
- Use the lxml/selenium package to filter and parse data from a scraped web page.
- Learn how to step around header-based filtering.
- Learn how to handle rate limiting.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. It is not uncommon to be blocked from scraping a website. There are a variety of strategies that they use to do this, and in general they work well. In general, if a company wants you to extract information from their website, they will make an API (application programming interface) available for you to use. One method (that is commonly paired with other methods) is blocking your request based on _headers_. You can read about headers [here](https://developer.mozilla.org/en-US/docs/Glossary/Request_header). In general, you can think of headers as some extra data that gives the server or client context. [Here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers) is a list of headers, and some more explanation.

##### Each header has a purpose. One common header is called the [User-Agent header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent). A User-Agent looks something like:

````
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0
````

##### You can see headers if you open the console in Firefox or Chrome and load a website. It will look something like:

![](./images/headers01.png)

##### From the [mozilla link](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent), this header is a string that "lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent." Basically, if you are browsing the internet with a common browser, the server will know what you are using. In the provided example, we are using Firefox 86 from Mozilla, on a Mac running Mac OS 10.16 with an Intel processor. 

##### When we send a request from a package like `requests` in Python, here is what the headers look like:

```{python, eval=F}
import requests

response = requests.get("https://project5-headers.tdm.wiki")
print(response.request.headers)
```

````
{'User-Agent': 'python-requests/2.25.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
````

##### As you can see our User-Agent is `python-requests/2.25.1`. You will find that many websites block requests made from anything such user agents. One such website is: https://project5-headers.tdm.wiki. 

##### Scrape https://project5-headers.tdm.wiki from Scholar and explain what happens. What is the [response code](#p-requests-response-code), and what does that response code mean? Can you ascertain what you would be seeing (more or less) in a browser based on the [text of the response](#p-requests-example01) (the actual HTML)? Read [this section of the documentation for the `headers` package](https://requests.readthedocs.io/en/master/user/quickstart/#custom-headers), and attempt to "trick" https://project5-headers.tdm.wiki into presenting you with the desired information. The desired information should look something like:

````
Hostname: c1de5faf1daa
IP: 127.0.0.1
IP: 172.18.0.4
RemoteAddr: 172.18.0.2:34520
GET / HTTP/1.1
Host: project5-headers.tdm.wiki
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
Accept-Encoding: gzip
Accept-Language: en-US,en;q=0.5
Cdn-Loop: cloudflare
Cf-Connecting-Ip: 107.201.65.5
Cf-Ipcountry: US
Cf-Ray: 62289b90aa55f975-EWR
Cf-Request-Id: 084d3f8e740000f975e0038000000001
Cf-Visitor: {"scheme":"https"}
Cookie: __cfduid=d9df5daa57fae5a4e425173aaaaacbfc91613136177
Dnt: 1
Sec-Gpc: 1
Upgrade-Insecure-Requests: 1
X-Forwarded-For: 123.123.123.123
X-Forwarded-Host: project5-headers.tdm.wiki
X-Forwarded-Port: 443
X-Forwarded-Proto: https
X-Forwarded-Server: 6afe64faffaf
X-Real-Ip: 123.123.123.123
````

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qg61j5iu)

**Relevant topics:** [requests](#p-requests)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Response code received (a number), and an explanation of what that HTTP response code means.
- What you would (probably) be seeing in a browser if you were blocked.
- Python code used to "trick" the website into being scraped.
- The content of the successfully scraped site.
```

##### 2. Open a browser and navigate to: https://project5-rate-limit.tdm.wiki/. While at first glance, it will seem identical to https://project5-headers.tdm.wiki/, it is not. https://project5-rate-limit.tdm.wiki/ is rate limited based on IP address. Depending on when you are completing this project, this may or may not be obvious. If you refresh your browser fast enough, instead of receiving a bunch of information, you will receive text that says "Too Many Requests". 

##### The following function tries to scrape the `Cf-Request-Id` header which will have a unique value each request:

```{python, eval=F}
import requests
import lxml.html

def scrape_cf_request_id(url):
    resp = requests.get(url)
    tree = lxml.html.fromstring(resp.text)
    content = tree.xpath("//p")[0].text.split('\n')
    cfid = [l for l in content if 'Cf-Request-Id' in l][0].split()[1]
    return cfid
```

##### You can test it out:

```{python, eval=F}
scrape_cf_request_id("https://project5-rate-limit.tdm.wiki")
```

##### Write code to scrape 10 unique `Cf-Request-Id`s (in a loop), and save them to a list called `my_ids`. What happens when you run the code? This is caused by our expected text not being present. Instead text with "Too Many Requests" is. While normally this error would be something that makes more sense, like an HTTPError or a Timeout Exception, it _could_ be anything, depending on your code. 

##### One solution that might come to mind is to "wait" between each loop using `time.sleep()`. While yes, this may work, it is not a robust solution. Other users from your IP address may count towards your rate limit and cause your function to fail, the amount of sleep time may change dynamically, or even be manually adjusted to be longer, etc. The best way to handle this is to used something called exponential backoff.

##### In a nutshell, exponential backoff is a way to increase the wait time (exponentially) until an acceptable rate is found. [`backoff`](https://pypi.org/project/backoff/) is an excellent package to do just that. `backoff`, upon being triggered from a specified error or exception, will wait to "try again" until a certain amount of time has passed. Upon receving the same error or exception, the time to wait will increase exponentially. Use `backoff` to modify the provided `scrape_cf_request_id` function to use exponential backoff when the we alluded to occurs. Test out the modified function in a loop and print the resulting 10 `Cf-Request-Id`s.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_p0ioevlc)

**Note:** `backoff` utilizes decorators. For those interested in learning about decorators, [this](https://realpython.com/primer-on-python-decorators/) is an excellent article.

**Relevant topics:** [requests](#p-requests)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- What happens when you run the function 10 times in a row?
- Fixed code that will work regardless of the rate limiting.
- 10 unique `Cf-Request-Id`s printed.
```

##### 3. You now have a great set of tools to be able to scrape pretty much anything you want from the internet. Now all that is left to do is practice. Find a course appropriate website containing data you would like to scrape. Utilize the tools you've learned about to scrape at least 100 "units" of data. A "unit" is just a representation of what you are scraping. For example, a unit could be a tweet from Twitter, a basketball player's statistics from sportsreference, a product from Amazon, a blog post from your favorite blogger, etc.

##### The hard requirements are:

- Documented code with thorough comments explaining what the code does.
- At least 100 "units" scraped.
- The data must be from multiple web pages.
- Write at least 1 function (with a docstring) to help you scrape.
- A clear explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example a `head` of a pandas dataframe containing the data). 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code that scrapes 100 unites of data (with thorough comments explaining what the code does).
- The data must be from more than a single web page.
- 1 or more functions (with docstrings) used to help you scrape/parse data.
- Clear documentation and explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example using the `head` of a dataframe containing the data).
```

---

### Project 6 {#p06-290}

---

**Motivation:** Being able to analyze and create good visualizations is a skill that is invaluable in _many_ fields. It can be pretty fun too! In this project, we are going to dive into `matplotlib` with an open project.

**Context:** We've been working hard all semester and learning a lot about web scraping. In this project we are going to ask you to examine some plots, write a little bit, and use your creative energies to create good visualizations about the flight data using the go-to plotting library for many, `matplotlib`. In the next project, we will continue to learn about and become comfortable using `matplotlib`.

**Scope:** python, visualizing data

**Learning objectives:** 

```{block, type="bbox"}
- Demonstrate the ability to create basic graphs with default settings.
- Demonstrate the ability to modify axes labels and titles.
- Demonstrate the ability to customize a plot (color, shape/linetype).
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/flights/*.csv` (all csv files)

#### Questions

##### 1. [Here](http://stat-computing.org/dataexpo/2009/posters/) are the results from the 2009 Data Expo poster competition. The object of the competition was to visualize interesting information from the flights dataset. Examine all 8 posters and write a single sentence for each poster with your first impression(s). An example of an impression that will not get full credit would be: "My first impression is that this poster is bad and doesn't look organized.". An example of an impression that will get full credit would be: "My first impression is that the author had a good visualization-to-text ratio and it seems easy to follow along.".

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_43j9xh4d)

```{block, type="bbox"}
**Item(s) to submit:**

- 8 bullets, each containing a sentence with the first impression of the 8 visualizations. Order should be "first place", to "honourable mention", followed by "other posters" in the given order. Or, label which graphic each sentence is about.
```

##### 2. [Creating More Effective Graphs](https://www.amazon.com/dp/0985911123/) by Dr. Naomi Robbins and [The Elements of Graphing Data](https://www.amazon.com/Elements-Graphing-Data-William-Cleveland/dp/0963488414/ref=sr_1_1?dchild=1&keywords=elements+of+graphing+data&qid=1614013761&sr=8-1) by Dr. William Cleveland at Purdue University, are two excellent books about data visualization. Read the following excerpts from the books (respectively), and list 2 things you learned, or found interesting from _each_ book. 

- [Excerpt 1](./files/CreatingMoreEffectiveGraphs.pdf)
- [Excerpt 2](./files/ElementsOfGraphingData.pdf)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_6c1ge176)

```{block, type="bbox"}
**Item(s) to submit:**

- Two bullets for each book with items you learned or found interesting.
```

##### 3. Of the 7 posters with at least 3 plots and/or maps, choose 1 poster that you think you could improve upon or "out plot". Create 4 plots/maps that either:

1. Improve upon a plot from the poster you chose, or
2. Show a completely different plot that does a good job of getting an idea or observation across, or
3. Ruin a plot. Purposefully break the best practices you've learned about in order to make the visualization misleading. (limited to 1 of the 4 plots)

##### For each plot/map where you choose to do (1), include 1-2 sentences explaining what exactly you improved upon and how. Point out some of the best practices from the 2 provided texts that you followed. For each plot/map where you choose to do (2), include 1-2 sentences explaining your graphic and outlining the best practices from the 2 texts that you followed. For each plot/map where you choose to do (3), include 1-2 sentences explaining what you changed, what principle it broke, and how it made the plot misleading or worse. 

##### While we are not asking you to create a poster, please use RMarkdown to keep your plots, code, and text nicely formatted and organized. The more like a story your project reads, the better. In this project, we are restricting you to use `matplotlib` in Python. While there are many interesting plotting packages like `plotly` and `plotnine`, we really want you to take the time to dig into `matplotlib` and learn as much as you can.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_w3r8yhb2)

```{block, type="bbox"}
**Item(s) to submit:**

- All associated Python code you used to wrangling the data and create your graphics.
- 4 plots, with at least 4 associated RMarkdown code chunks.
- 1-2 sentences per plot explaining what exactly you improved upon, what best practices from the texts you used, and how. If it is a brand new visualization, describe and explain your graphic, outlining the best practices from the 2 texts that you followed. If it is the ruined plot you chose, explain what you changed, what principle it broke, and how it made the plot misleading or worse.
```

##### 4. Now that you've been exploring data visualization, copy, paste, and update your first impressions from question (1) with your updated impressions. Which impression changed the most, and why?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_3fkl4ln3)

```{block, type="bbox"}
**Item(s) to submit:**

- 8 bullets with updated impressions (still just a sentence or two) from question (1). 
- A sentence explaining which impression changed the most and why.
```

---

### Project 7 {#p07-290}

---

**Motivation:** Being able to analyze and create good visualizations is a skill that is invaluable in _many_ fields. It can be pretty fun too! As you probably noticed in the previous project, `matplotlib` can be finicky -- certain types of plots are really easy to create, while others are not. For example, you would think changing the color of a boxplot would be easy to do in `matplotlib`, perhaps we just need to add an option to the function call. As it turns out, this isn't so straightforward (as illustrated at the end of [this section](#p-matplotlib-boxplot)). Occasionally this will happen and that is when packages like `seaborn` or `plotnine` (both are packages built using `matplotlib`) can be good. In this project we will explore this a little bit, and learn about some useful `pandas` functions to help shape your data in a format that any given package requires.

**Context:** In the next project, we will continue to learn about and become comfortable using `matplotlib`, `seaborn`, and `plotnine`.

**Scope:** python, visualizing data

**Learning objectives:** 

```{block, type="bbox"}
- Demonstrate the ability to create basic graphs with default settings.
- Demonstrate the ability to modify axes labels and titles.
- Demonstrate the ability to customize a plot (color, shape/linetype).
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Questions

##### 1. In an earlier project we explored some XML data in the form of an Apple Watch data dump. Most health-related apps give you some sort of graph or set of graphs as an output. Use any package you want to parse the XML data. There are a lot of `Record`s in this dataset. Each `Record` has an attribute called `creationDate`. Create a barplot of the number of `Record`s per day. Make sure your plot is polished, containing proper labels and good colors.

**Hint:** You could start by parsing out the required data into a `pandas` dataframe or series.

**Hint:** The `groupby` method is one of the most useful `pandas` methods. It allows you to quickly perform operations on groups of data. 

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_69sdl8rk)

[Click here for another video](https://mediaspace.itap.purdue.edu/id/1_ho5ej4gy)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 2. The plot in question 1 should look bimodal. Let's focus only on the first apparent group of readings. Create a new dataframe containing only the readings for the time period from 9/1/2017 to 5/31/2019. How many `Record`s are there in that time period?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_wlkijwnv)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 3. It is hard to discern weekly patterns (if any) based on the graphics created so far. For the period of time in question 2, create a labeled bar plot for the count of `Record`s by day of the week. What (if any) discernable patterns are there? Make sure to include the labels provided below:

```{python, eval=F}
labels = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_e7d9ve0o)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 4. Create a `pandas` dataframe containing the following data from `watch_dump.xml`:

- A column called `bpm` with the `bpm` (beats per minute) of the `InstantaneousBeatsPerMinute`.
- A column called `time` with the `time` of each individual `bpm` reading in `InstantaneousBeatsPerMinute`.
- A column called `date` with the date.
- A column called `dayofweek` with the day of the week.

**Hint:** You may want to use `pd.to_numeric` to convert the `bpm` column to a numeric type.

**Hint:** This is one way to convert the numbers 0-6 to days of the week:

```{python, eval=F}
myDF['dayofweek'] = myDF['dayofweek'].map({0:"Mon", 1:"Tue", 2:"Wed", 3:"Thu", 4:"Fri", 5: "Sat", 6: "Sun"})
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_h8crbise)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 5. Create a heatmap using `seaborn`, where the y-axis shows the day of the week ("Mon" - "Sun"), the x-axis shows the hour, and the values on the interior of the plot are the average `bpm` by hour by day of the week.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_rl5iu9am)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [pivot](#p-pandas-pivot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

---

## STAT 39000

### Topics

The following table roughly highlights the topics and projects for the semester. This is slightly adjusted throughout the semester as student performance and feedback is taken into consideration.

|Language|Project #|Name|Topics|
|--------|---------|----|------|
|Python|1|Web scraping: part I|xml, lxml, pandas,  etc.|
|Python|2|Web scraping: part II|requests, functions, xml, loops, if statements, etc.|
|Python|3|Web scraping: part III|selenium, lxml, lists, pandas, etc.|
|Python|4|Web scraping: part IV|requests, beautifulsoup4, lxml, selenium, xml, cronjobs, loops, etc.|
|Python|5|Web scraping: part V|web scraping + related topics|
|Python|6|Plotting in Python: part I|ways to plot in Python, more work with pandas, etc.|
|Python|7|Plotting in Python: part II|ways to plot in Python, more work with pandas, etc.|
|Python|8|Writing scripts: part I|how to write scripts in Python, more work with pandas, matplotlib, etc.|
|Python|9|Writing scripts: part II|how to write scripts in Python, argparse, more work with pandas, matplotlib, etc.|
|R|10|ggplot: part I|ggplot basics|
|R|11|ggplot: part II|more ggplot|
|R|12|tidyverse & data.table: part I|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|
|R|13|tidyverse & data.table: part II|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|
|R|14|tidyverse & data.table: part III|data wrangling and computation using tidyverse packages and data.table, maybe some slurm?|

### Project 1 {#p01-390}

---

**Motivation:** Extensible Markup Language or XML is a very important file format for storing structured data. Even though formats like JSON, and csv tend to be more prevalent, many, many legacy systems still use XML, and it remains an appropriate format for storing complex data. In fact, JSON and csv are quickly becoming less relevant as new formats and serialization methods like [parquet](https://arrow.apache.org/faq/) and [protobufs](https://developers.google.com/protocol-buffers) are becoming more common.

**Context:** In previous semesters we've explored XML. In this project we will refresh our skills and, rather than exploring XML in R, we will use the `lxml` package in Python. This is the first project in a series of 5 projects focused on web scraping in R and Python.

**Scope:** python, XML

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Match XML terms to sections of XML demonstrating working knowledge.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Resources

We realize that it may be a while since you've used Python. That's okay! We are going to be taking things at a much more reasonable pace than Spring 2020.

Some potentially useful resources for the semester include:

- The STAT 19000 projects. We are easing 19000 students into Python and will post solutions each week. It would be well worth 10 minutes to look over the questions and solutions each week.
- [Here](https://towardsdatascience.com/cheat-sheet-for-python-dataframe-r-dataframe-syntax-conversions-450f656b44ca) is a decent cheat sheet that helps you quickly get an idea of how to do something you know how to do in R, in Python.
- [The Examples Book](https://thedatamine.github.io/the-examples-book/) -- updating daily with more examples and videos. Be sure to click on the "relevant topics" links as we try to point you to topics with examples that should be particularly useful to solve the problems we assign.

#### Questions

**Important note:** It would be well worth your time to read through the [xml section](#xml) of the book, as well as take the time to work through [pandas 10 minute intro](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).

##### 1. A good first step when working with XML is to get an idea how your document is structured. Normally, there should be good documentation that spells this out for you, but it is good to know what to do when you _don't_ have the documentation. Start by finding the "root" node. What is the name of the root node of the provided dataset?

**Hint:** Make sure to import the `lxml` package first:

```{python, eval=F}
from lxml import etree
```

Here are two videos about running Python in RStudio:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_nhkygxg9)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_tdz3wmim)

and here is a video about XML scraping in Python:

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_1ywsbxyp)

**Relevant topics:** [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from lxml import etree

tree = etree.parse("/class/datamine/data/apple/health/watch_dump.xml")

tree.xpath("/*")[0].tag
```

##### 2. Remember, XML can be nested. In question (1) we figured out what the root node was called. What are the names of the next "tier" of elements?

**Hint:** Now that we know the root node, you could use the root node name as a part of your xpath expression.

**Hint:** As you may have noticed in question (1) the `xpath` method returns a list. Sometimes this list can contain many repeated tag names. Since our goal is to see the names of the second "tier" elements, you could convert the resulting `list` to a `set` to quickly see the unique list as `set`'s only contain unique values.

**Relevant topics:** [for loops](#p-for-loops), [lxml](#p-lxml), [xml](#xml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
set([x.tag for x in tree.xpath("/HealthData/*")])
```

##### 3. Continue to explore each "tier" of data until there isn't any left. Name the "full paths" of all of the "last tier" tags.

**Hint:** Let's say a "last tier" tag is just a path where there are no more nested elements. For example, `/HealthData/Workout/WorkoutRoute/FileReference` is a "last tier" tag. If you try and get the nested elements for it, they don't exist:

```{python, eval=F}
tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")
```

**Hint:** Here are 3 of the 7 "full paths":

````
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Record/MetadataEntry
/HealthData/ActivitySummary
````

**Relevant topics:** [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
print(set([x.tag for x in tree.xpath("/HealthData/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/ActivitySummary/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Record/MetadataEntry/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEvent/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutEntry/*")]))

print(set([x.tag for x in tree.xpath("/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/FileReference/*")]))
print(set([x.tag for x in tree.xpath("/HealthData/Workout/WorkoutRoute/MetadataEntry/*")]))
```

```{block, type="solution"}
/HealthData/Record/HeartRateVariabilityMetadataList/InstantaneousBeatsPerMinute
/HealthData/Workout/WorkoutRoute/FileReference
/HealthData/Workout/WorkoutRoute/MetadataEntry
/HealthData/Record/MetadataEntry
/HealthData/Workout/WorkoutEvent
/HealthData/Workout/WorkoutEntry
/HealthData/ActivitySummary
```

##### 4. At this point in time you may be asking yourself "but where is the data"? Depending on the structure of the XML file, the data could either be between tags like:

```{html, eval=F}
<some_tag>mydata</some_tag>
```

##### Or, it could be in an attribute:

```{html, eval=F}
<question answer="tac">What is cat spelled backwards?</question>
```

##### Collect the "ActivitySummary" data, and convert the list of dicts to a `pandas` DataFrame. The following is an example of converting a list of dicts to a `pandas` DataFrame called `myDF`:

```{python, eval=F}
import pandas as pd

list_of_dicts = []
list_of_dicts.append({'columnA': 1, 'columnB': 2})
list_of_dicts.append({'columnB': 4, 'columnA': 1}) 

myDF = pd.DataFrame(list_of_dicts)
```

**Hint:** It is important to note that an element's "attrib" attribute looks and feels like a `dict`, but it is actually a `lxml.etree._Attrib`. If you try to convert a list of `lxml.etree._Attrib` to a `pandas` DataFrame, it will not work out as you planned. Make sure to first convert each `lxml.etree._Attrib` to a `dict` before converting to a DataFrame. You can do so like:

```{python, eval=F}
# this will convert a single `lxml.etree._Attrib` to a dict
my_dict = dict(my_lxml_etree_attrib)
```

**Relevant topics:** [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
dat = tree.xpath("/HealthData/ActivitySummary")
list_of_dicts = []

for e in dat:
    list_of_dicts.append(dict(e.attrib))
    
myDF = pd.DataFrame(data=list_of_dicts)
myDF.sort_values(['activeEnergyBurned'], ascending=False).head()
```

##### 5. `pandas` is a Python package that provides the DataFrame and Series classes. A DataFrame is very similar to a data.frame in R and can be used to manipulate the data within very easily. A Series is the class that handles a single column of a DataFrame. Go through the [pandas in 10 minutes](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) page from the official documentation. Sort, find, and print the top 5 rows of data based on the "activeEnergyBurned" column. 

**Relevant topics:** [pandas](#p-pandas), [dicts](#p-dicts), [lists](#p-lists), [lxml](#p-lxml), [xml](#xml), [for loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
# could be anything
```

---

### Project 2 {#p02-390}

---

**Motivation:** Web scraping is is the process of taking content off of the internet. Typically this goes hand-in-hand with parsing or processing the data. Depending on the task at hand, web scraping can be incredibly simple. With that being said, it can quickly become difficult. Typically, students find web scraping fun and empowering.   

**Context:** In the previous project we gently introduced XML and xpath expressions. In this project, we will learn about web scraping, scrape data from The New York Times, and parse through our newly scraped data using xpath expressions.  

**Scope:** python, web scraping, xml

**Learning objectives:** [html](#html)

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

You will be extracting your own data from online in this project. There is no base dataset.

#### Questions

##### 1. The New York Times is one of the most popular newspapers in the United States. Open a modern browser (preferably Firefox or Chrome), and navigate to https://nytimes.com. 

##### By the end of this project you will be able to scrape some data from this website! The first step is to explore the structure of the website. You can either right click and click on "view page source", which will pull up a page full of HTML used to render the page. Alternatively, if you want to focus on a single element, an article title, for example, right click on the article title and click on "inspect element". This will pull up an inspector that allows you to see portions of the HTML.

##### Click around the website and explore the HTML however you see fit. Open a few front page articles and notice how most articles start with a bunch of really important information, namely: an article title, summary, picture, picture caption, picture source, author portraits, authors, and article datetime.

##### For example:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

![](./images/nytimes_image.jpg)

##### Copy and paste the **h1** element (in its entirety) containing the article title (for the article provided) in an HTML code chunk. Do the same for the same article's summary.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_eew0i16y)

**Relevant topics:** [html](#html)

```{block, type="bbox"}
**Item(s) to submit:**

- 2 code chunks containing the HTML requested.
```

##### Solution

```{html, class.source="solution"}
<h1 id="link-4686dc8b" class="css-rsa88z e1h9rw200" data-test-id="headline">U.S. Says Chinas Repression of Uighurs Is Genocide</h1>
```

```{html, class.source="solution"}
<p id="article-summary" class="css-w6ymp8 e1wiw3jv0">The finding by the Trump administration is the strongest denunciation by any government of Chinas actions and follows a Biden campaign statement with the same declaration.</p>
```

##### 2. In question (1) we copied two elements of an article. When scraping data from a website, it is important to continually consider the patterns in the structure. Specifically, it is important to consider whether or not the defining characteristics you use to parse the scraped data will continue to be in the same format for _new_ data. What do I mean by defining characterstic? I mean some combination of tag, attribute, and content from which you can isolate the data of interest. 

##### For example, given a link to a new nytimes article, do you think you could isolate the article title by using the `id="link-4686dc8b"` attribute of the **h1** tag? Maybe, or maybe not, but it sure seems like "link-4686dc8b" might be unique to the article and not able to be used given a new article.

##### Write an xpath expression to isolate the article title, and another xpath expression to isolate the article summary. 

**Important note:** You do _not_ need to test your xpath expression yet, we will be doing that shortly.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath)

```{block, type="bbox"}
**Item(s) to submit:**

- Two xpath expressions in an HTML code chunk. 
```

##### Solution

```{html, class.source="solution"}
//h1[@data-test-id="headline"]
```

```{html, class.source="solution"}
//p[@id="article-summary"]
```

##### 3. Use the `requests` package to scrape the webpage containing our article from questions (1) and (2). Use the `lxml.html` package and the `xpath` method to test out your xpath expressions from question (2). Did they work? Print the content of the elements to confirm.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qw2xb058)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
import lxml.html
import requests

url =  "https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html"
response = requests.get(url, stream=True)
response.raw.decode_content = True
tree = lxml.html.parse(response.raw)

print(tree.xpath('//p[@id="article-summary"]')[0].text)
print(tree.xpath('//h1[@data-test-id="headline"]')[0].text)
```

##### 4. Here are a list of article links from https://nytimes.com:

https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html

https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html

https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html

##### Write a function called `get_article_and_summary` that accepts a string called `link` as an argument, and returns both the article title and summary. Test `get_article_and_summary` out on each of the provided links:

```{python, eval=F}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

**Hint:** The first line of your function should look like this:

`def get_article_and_summary(myURL: str) -> (str, str):`

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_jrtrt5fo)

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_7yhabkeg)

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from typing import Tuple
import lxml.html
import requests

def get_article_and_summary(link: str) -> Tuple[str, str]:
    """
    Given a link to a new york times article, return the article title and summary.

    Args:
        link (str): The link to the new york times article.

    Returns:
        Tuple[str, str]: A tuple first containing the article title, and then the article summary.
    """
    
    # scrape the web page
    response = requests.get(link, stream=True)
    response.raw.decode_content = True
    tree = lxml.html.parse(response.raw)
    
    title = tree.xpath('//p[@id="article-summary"]')[0].text
    summary = tree.xpath('//h1[@data-test-id="headline"]')[0].text
    
    return title, summary
```

```{python, eval=F, class.source="solution"}
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title: {title}, Summary: {summary}')
title, summary = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title: {title}, Summary: {summary}')
```

##### 5. In question (1) we mentioned a myriad of other important information given at the top of most New York Times articles. Choose **two** other listed pieces of information and copy, paste, and update your solution to question (4) to scrape and return those chosen pieces of information.

**Important note:** If you choose to scrape non-textual data, be sure to return data of an appropriate type. For example, if you choose to scrape one of the images, either print the image or return a PIL object.

**Relevant topics:** [html](#html), [xml](#xml), [xpath expressions](#xml-xpath), [lxml](#p-lxml), [functions](#p-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### Solution

```{python, eval=F, class.source="solution"}
from typing import Tuple
import lxml.html
import requests
import io
from PIL import Image
from IPython.display import display

def get_article_and_summary(link: str) -> Tuple[str, str]:
    """
    Given a link to a new york times article, return the article title and summary.

    Args:
        link (str): The link to the new york times article.

    Returns:
        Tuple[str, str]: A tuple first containing the article title, and then the article summary.
    """
    
    # scrape the web page
    response = requests.get(link, stream=True)
    response.raw.decode_content = True
    tree = lxml.html.parse(response.raw)
    
    # parse out the title
    title = tree.xpath('//p[@id="article-summary"]')[0].text
    
    # parse out the summary
    summary = tree.xpath('//h1[@data-test-id="headline"]')[0].text
    
    # parse out the url to the image
    photo_src = tree.xpath('//picture/img')[0].attrib.get("src")
    
    # scrape image
    photo_content = requests.get(photo_src).content
    
    # convert image format 
    photo_file = io.BytesIO(photo_content)
    photo = Image.open(photo_file).convert('RGB')    
    
    # parse out photo caption
    caption = tree.xpath('//figcaption/span')[0].text
    
    # parse out the photo credits
    credits = tree.xpath('//figcaption/span/span[contains(text(), "Credit")]/following-sibling::span/span')
    
    credits_list = []
    for c in credits:
        credits_list.append(c.text)
        
    # parse author photo url
    # only gets if "author" in photo src attribute
    photo_src_elements = tree.xpath('//img[contains(@src, "author")]')
    
    # if "author" in photo src attribute
    photo_srcs = []
    for p in photo_src_elements:
        photo_srcs.append(p.attrib.get("src"))
    
    # scrape image
    author_images = []
    for img in photo_srcs:
        photo_content = requests.get(img).content
    
        # convert image format 
        photo_file = io.BytesIO(photo_content)
        photo = Image.open(photo_file).convert('RGB')    
        
        author_images.append(photo)
        
    # parse out authors
    authors_elements = tree.xpath("//span[@class='byline-prefix']/following-sibling::a/span")
    authors = []
    for a in authors_elements:
        authors.append(a.text)
        
    # parse out article publish date/time
    dt = tree.xpath("//time")[0].attrib.get("datetime")
    
    return title, summary, photo, caption, credits, author_images, authors, dt



title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/19/us/politics/trump-china-xinjiang.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/06/technology/personaltech/tech-2021-augmented-reality-chatbots-wifi.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
title, summary, photo, caption, credits, author_images, authors, dt = get_article_and_summary('https://www.nytimes.com/2021/01/13/movies/letterboxd-growth.html')
print(f'Title:\n{title}, Summary:\n{summary}, Caption:\n{caption}')
```

---

### Project 3 {#p03-390}

---

**Motivation:** Web scraping takes practice, and it is important to work through a variety of common tasks in order to know how to handle those tasks when you next run into them. In this project, we will use a variety of scraping tools in order to scrape data from https://trulia.com. 

**Context:** In the previous project, we got our first taste at actually scraping data from a website, and using a parser to extract the information we were interested in. In this project, we will introduce some tasks that will require you to use a tool that let's you interact with a browser, selenium. 

**Scope:** python, web scraping, selenium

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Visit https://trulia.com. Many websites have a similar interface, i.e. a bold and centered search bar for a user to interact with. Using `selenium` write Python code that that first finds the `input` element, and then types "West Lafayette, IN" followed by an emulated "Enter/Return". Confirm you code works by printing the url after that process completes.

**Hint:** You will want to use `time.sleep` to pause a bit after the search so the updated url is returned.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ztju698f)

That video is already relevant for Question 2 too.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml)  

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Use your code from question (1) to test out the following queries:

- West Lafayette, IN (City, State)
- 47906 (Zip)
- 4505 Kahala Ave, Honolulu, HI 96816 (Full address)

##### If you look closely you will see that there are patterns in the url. For example, the following link would probably bring up homes in Crawfordsville, IN: https://trulia.com/IN/Crawfordsville. With that being said, if you only had a zip code, like 47933, it wouldn't be easy to guess https://www.trulia.com/IN/Crawfordsville/47933/, hence, one reason why the search bar is useful.

##### If you used xpath expressions to complete question (1), instead use a [different method](https://selenium-python.readthedocs.io/locating-elements.html#locating-elements) to find the `input` element. If you used a different method, use xpath expressions to complete question (1).

**Relevant topics:** [selenium](#p-selenium), [xml](#xml) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 3. Let's call the page after a city/state or zipcode search a "sales page". For example:

![](./images/trulia.png)

##### Use `requests` to scrape the entire page: https://www.trulia.com/IN/West_Lafayette/47906/. Use `lxml.html` to parse the page and get all of the `img` elements that make up the house pictures on the left side of the website.

**Important note:** Make sure you are actually scraping what you think you are scraping! Try printing your html to confirm it has the content you think it should have:

```{python, eval=F}
import requests
response = requests.get(...)
print(response.text)
```

**Hint:** Are you human? Depends. Sometimes if you add a header to your request, it won't ask you if you are human. Let's pretend we are Firefox:

```{python, eval=F}
import requests
my_headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get(..., headers=my_headers)
```

##### Okay, after all of that work you may have discovered that only a few images have actually been scraped. If you cycle through all of the `img` elements and try to print the value of the `src` attribute, this will be clear:

```{python, eval=F}
import lxml.html
tree = lxml.html.fromstring(response.text)
elements = tree.xpath("//img")
for element in elements:
    print(element.attrib.get("src"))
```

##### This is because the webpage is not immediately, _completely_ loaded. This is a common website behavior to make things appear faster. If you pay close to when you load https://www.trulia.com/IN/Crawfordsville/47933/, and you quickly scroll down, you will see images still needing to finish rendering all of the way, slowly. What we need to do to fix this, is use `selenium` (instead of `lxml.html`) to behave like a human and scroll prior to scraping the page! Try using the following code to slowly scroll down the page before finding the elements:

```{python, eval=F}
# driver setup and get the url

# Needed to get the window size set right and scroll in headless mode
myheight = driver.execute_script('return document.body.scrollHeight')
driver.set_window_size(1080,myheight+100)

def scroll(driver, scroll_point):  
    driver.execute_script(f'window.scrollTo(0, {scroll_point});')
    time.sleep(5) 
    
scroll(driver, myheight*1/4)
scroll(driver, myheight*2/4)
scroll(driver, myheight*3/4)
scroll(driver, myheight*4/4)

# find_elements_by_*
```

**Hint:** At the time of writing there should be about 86 links to images of homes.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_v9iesfnc)

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. Write a function called `avg_house_cost` that accepts a zip code as an argument, and returns the average cost of the first page of homes. Now, to make this a more meaningful statistic, filter for "3+" beds and _then_ find the average. Test `avg_house_cost` out on the zip code `47906` and print the average costs.

**Important note:** Use `selenium` to "click" on the "3+ beds" filter.

**Hint:** If you get an error that tells you `button` is not clickable because it is covered by an `li` element, try clicking on the `li` element instead. 

**Hint:** You will want to wait a solid 10-15 seconds for the sales page to load before trying to select or click on anything.

**Hint:** Your results may end up including prices for "Homes Near \<ZIPCODE\>". This is okay. Even better if you manage to remove those results. If you _do_ choose to remove those results, take a look at the `data-testid` attribute with value `search-result-list-container`. Perhaps only selecting the children of the first element will get the desired outcome.

**Hint:** You can use the following code to remove the non-numeric text from a string, and then convert to an integer:

```{python, eval=F}
import re

int(re.sub("[^0-9]", "", "removenon45454_numbers$"))
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_ahm64jjz)

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops), [functions](#p-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 5. Get creative. Either add an interesting feature to your function from (4), or use `matplotlib` to generate some sort of accompanying graphic with your output. Make sure to explain what your additions do.

**Relevant topics:** [selenium](#p-selenium), [xml](#xml), [loops](#p-for-loops) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

---

### Project 4 {#p04-390}

---

**Motivation:** In this project we will continue to hone your web scraping skills, introduce you to some "gotchas", and give you a little bit of exposure to a powerful tool called cron.

**Context:** We are in the second to last project focused on web scraping. This project will introduce some supplementary tools that work well with web scraping: cron, sending emails from Python, etc. 

**Scope:** python, web scraping, selenium, cron

**Learning objectives:** 

```{block, type="bbox"}
- Review and summarize the differences between XML and HTML/CSV.
- Use the requests package to scrape a web page.
- Use the lxml package to filter and parse data from a scraped web page.
- Use the beautifulsoup4 package to filter and parse data from a scraped web page.
- Use selenium to interact with a browser in order to get a web page to a desired state for scraping.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. Check out the following website: https://project4.tdm.wiki

##### Use `selenium` to scrape and print the 6 colors of pants offered.

[Click here for video for Question 1](https://mediaspace.itap.purdue.edu/id/1_j3ik06bo)

[Here is a video about the new feature to reset your RStudio session if you make a big mistake or if your session is very slow](https://mediaspace.itap.purdue.edu/id/1_tr8qvwn3)

**Hint:** You _may_ have to interact with the webpage for certain elements to render.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 2. Websites are updated frequently. You can imagine a scenario where a change in a website is a sign that there is more data available, or that something of note has happened. This is a fake website designed to help students emulate real changes to a website. Specifically, there is one part of the website that has two possible states (let's say, state `A` and state `B`). Upon refreshing the website, or scraping the website again, there is an $x\%$ chance that the website will be in state `A` and a $1-x\%$ chance the website will be in state `B`.

##### Describe the two states (the thing (element or set of elements) that changes as you refresh the page), and scrape the website enough to estimate $x$.

[Click here for video for Questions 2 and 3](https://mediaspace.itap.purdue.edu/id/1_2zaiodzi)

**Hint:** You _will_ need to interact with the website to "see" the change.

**Hint:** Since we are just asking about a state, and not any specific element, you could use the `page_source` attribute of the `selenium` driver to scrape the entire page instead of trying to use xpath expressions to find a specific element.

**Hint:** Your estimate of $x$ does not need to be perfect.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example clicking a button](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- What state `A` and `B` represent.
- An estimate for `x`.
```

##### 3. Dig into the changing "thing" from question (2). What specifically is changing? Use selenium and xpath expressions to scrape and print the content. What are the two possible values for the content? 

[Click here for video (same as above) for Questions 2 and 3](https://mediaspace.itap.purdue.edu/id/1_2zaiodzi)

**Hint:** Due to the changes that occur when a button is clicked, I'd highly advice you to use the `data-color` attribute in your xpath expression instead of `contains(text(), 'blahblah')`.

**Hint:** `parent::` and `following-sibling::` may be useful [xpath axes](https://www.w3schools.com/xml/xpath_axes.asp) to use.

**Relevant topics:** [scraping](#p-scraping), [selenium](#p-selenium), [example using `following-sibling::`](#p-selenium-example01)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```

##### 4. The following code allows you to send an email using Python from your Purdue email account. Replace the username and password with your own information and send a test email to yourself to ensure that it works.

[Click here for video for Questions 4 and 5](https://mediaspace.itap.purdue.edu/id/1_dt6j3sov)

**Important note:** Do **NOT** include your password in your homework submission. Any time you need to type your password in you final submission just put something like "SUPERSECRETPASSWORD" or "MYPASSWORD".

**Hint:** To include an image (or screenshot) in RMarkdown, try `![](./my_image.png)` where `my_image.png` is inside the same folder as your `.Rmd` file.

**Hint:** The spacing and tabs near the `message` variable are very important. Make sure to copy the code exactly. Otherwise, your subject may not end up in the subject of your email, or the email could end up being blank when sent.

**Hint:** Questions 4 and 5 were inspired by examples and borrowed from the code found at the [Real Python](https://realpython.com/python-send-email/) website.

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message):
  import smtplib, ssl
  from email.mime.text import MIMEText
  from email.mime.multipart import MIMEMultipart
  
  message = MIMEMultipart("alternative")
  message["Subject"] = my_subject
  message["From"] = my_purdue_email
  message["To"] = to
  
  # Create the plain-text and HTML version of your message
  text = f'''\
Subject: {my_subject}
To: {to}
From: {my_purdue_email}
  
{my_message}'''
  html = f'''\
<html>
  <body>
    {my_message}
  </body>
</html>
'''
  # Turn these into plain/html MIMEText objects
  part1 = MIMEText(text, "plain")
  part2 = MIMEText(html, "html")
  
  # Add HTML/plain-text parts to MIMEMultipart message
  # The email client will try to render the last part first
  message.attach(part1)
  message.attach(part2)
  
  context = ssl.create_default_context()
  with smtplib.SMTP("smtp.purdue.edu", 587) as server:
    server.ehlo()  # Can be omitted
    server.starttls(context=context)
    server.ehlo()  # Can be omitted
    server.login(my_purdue_email, my_password)
    server.sendmail(my_purdue_email, to, message.as_string())
        
# this sends an email from kamstut@purdue.edu to mdw@purdue.edu
# replace supersecretpassword with your own password
# do NOT include your password in your homework submission.
send_purdue_email("kamstut@purdue.edu", "supersecretpassword", "mdw@purdue.edu", "put subject here", "put message body here")
```

**Relevant topics:** [functions](#p-writing-functions) 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- Screenshot showing your received the email.
```

##### 5. The following is the content of a new Python script called `is_in_stock.py`:

```{python, eval=F}
def send_purdue_email(my_purdue_email, my_password, to, my_subject, my_message):
  import smtplib, ssl
  from email.mime.text import MIMEText
  from email.mime.multipart import MIMEMultipart
  
  message = MIMEMultipart("alternative")
  message["Subject"] = my_subject
  message["From"] = my_purdue_email
  message["To"] = to
  
  # Create the plain-text and HTML version of your message
  text = f'''\
Subject: {my_subject}
To: {to}
From: {my_purdue_email}
  
{my_message}'''
  html = f'''\
<html>
  <body>
    {my_message}
  </body>
</html>
'''
  # Turn these into plain/html MIMEText objects
  part1 = MIMEText(text, "plain")
  part2 = MIMEText(html, "html")
  
  # Add HTML/plain-text parts to MIMEMultipart message
  # The email client will try to render the last part first
  message.attach(part1)
  message.attach(part2)
  
  context = ssl.create_default_context()
  with smtplib.SMTP("smtp.purdue.edu", 587) as server:
    server.ehlo()  # Can be omitted
    server.starttls(context=context)
    server.ehlo()  # Can be omitted
    server.login(my_purdue_email, my_password)
    server.sendmail(my_purdue_email, to, message.as_string())
        
def main():
    # scrape element from question 3
    
    # does the text indicate it is in stock?
    
    # if yes, send email to yourself telling you it is in stock.
    
    # otherwise, gracefully end script using the "pass" Python keyword

if __name__ == "__main__":
    main()
```

##### First, make a copy of the script in your `$HOME` directory:

```{bash, eval=F}
cp /class/datamine/data/scraping/is_in_stock.py $HOME/is_in_stock.py
```

##### If you now look in the "Files" tab in the lower right hand corner of RStudio, and click the refresh button, you should see the file `is_in_stock.py`. You can open and modify this file directly in RStudio. Before you do so, however, change the permissions of the `$HOME/is_in_stock.py` script so only YOU can read, write, and execute it:

```{bash, eval=F}
chmod 700 $HOME/is_in_stock.py
```

##### The script should now appear in RStudio, in your home directory, with the correct permissions. Open the script (in RStudio) and fill in the `main` function as indicated by the comments. We want the script to scrape to see whether the pants from question 3 are in stock or not.

##### A cron job is a task that runs at a certain interval. Create a cron job that runs your script, `/class/datamine/apps/python/f2020-s2021/env/bin/python $HOME/is_in_stock.py` every 5 minutes. Wait 10-15 minutes and verify that it is working properly. The long path, `/class/datamine/apps/python/f2020-s2021/env/bin/python` simply makes sure that our script is run with access to all of the packages in our course environment. `$HOME/is_in_stock.py` is the path to your script (`$HOME` expands or transforms to `/home/<my_purdue_alias>`.

[Click here for video (same as above) for Questions 4 and 5](https://mediaspace.itap.purdue.edu/id/1_dt6j3sov)


[Click here for a longer video about setting up the cronjob in Question 5](https://mediaspace.itap.purdue.edu/id/1_xz9df91y)

**Hint:** If you struggle to use the text editor used with the `crontab -e` command, be sure to continue reading the [cron section](#cron) of the book. We highlight another method that may be easier.

**Hint:** Don't forget to copy your import statements from question (3) as well.

**Important note:** Once you are finished with the project, if you no longer wish to receive emails every so often, follow the instructions [here](#how-to-delete-cron-job) to remove the cron job.

**Relevant topics:** [cron](#cron), [crontab guru](https://crontab.guru)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- The content of your cron job in a bash code chunk.
- The content of your `is_in_stock.py` script.
```

##### 6. Take a look at the byline of each pair of pants (the sentences starting with "Perfect for..."). Inspect the HTML. Try and scrape the text using xpath expressions like you normally would. What happens? Are you able to scrape it? Google around and come up with your best explanation of what is happening.

**Relevant topics:** pseudo elements

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
- An explanation of what is happening.
```

---

### Project 5 {#p05-390}

---

**Motivation:** One of the best things about learning to scrape data is the many applications of the skill that may pop into your mind. In this project, we want to give you some flexibility to explore your own ideas, but at the same time, add a couple of important tools to your tool set. We hope that you've learned a lot in this series, and can think of creative ways to utilize your new skills.

**Context:** This is the last project in a series focused on scraping data. We have created a couple of very common scenarios that can be problematic when first learning to scrape data, and we want to show you how to get around them.

**Scope:** python, web scraping, etc.

**Learning objectives:** 

```{block, type="bbox"}
- Use the requests package to scrape a web page.
- Use the lxml/selenium package to filter and parse data from a scraped web page.
- Learn how to step around header-based filtering.
- Learn how to handle rate limiting.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Questions

##### 1. It is not uncommon to be blocked from scraping a website. There are a variety of strategies that they use to do this, and in general they work well. In general, if a company wants you to extract information from their website, they will make an API (application programming interface) available for you to use. One method (that is commonly paired with other methods) is blocking your request based on _headers_. You can read about headers [here](https://developer.mozilla.org/en-US/docs/Glossary/Request_header). In general, you can think of headers as some extra data that gives the server or client context. [Here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers) is a list of headers, and some more explanation.

##### Each header has a purpose. One common header is called the [User-Agent header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent). A User-Agent looks something like:

````
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0
````

##### You can see headers if you open the console in Firefox or Chrome and load a website. It will look something like:

![](./images/headers01.png)

##### From the [mozilla link](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent), this header is a string that "lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent." Basically, if you are browsing the internet with a common browser, the server will know what you are using. In the provided example, we are using Firefox 86 from Mozilla, on a Mac running Mac OS 10.16 with an Intel processor. 

##### When we send a request from a package like `requests` in Python, here is what the headers look like:

```{python, eval=F}
import requests

response = requests.get("https://project5-headers.tdm.wiki")
print(response.request.headers)
```

````
{'User-Agent': 'python-requests/2.25.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
````

##### As you can see our User-Agent is `python-requests/2.25.1`. You will find that many websites block requests made from anything such user agents. One such website is: https://project5-headers.tdm.wiki. 

##### Scrape https://project5-headers.tdm.wiki from Scholar and explain what happens. What is the [response code](#p-requests-response-code), and what does that response code mean? Can you ascertain what you would be seeing (more or less) in a browser based on the [text of the response](#p-requests-example01) (the actual HTML)? Read [this section of the documentation for the `headers` package](https://requests.readthedocs.io/en/master/user/quickstart/#custom-headers), and attempt to "trick" https://project5-headers.tdm.wiki into presenting you with the desired information. The desired information should look something like:

````
Hostname: c1de5faf1daa
IP: 127.0.0.1
IP: 172.18.0.4
RemoteAddr: 172.18.0.2:34520
GET / HTTP/1.1
Host: project5-headers.tdm.wiki
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
Accept-Encoding: gzip
Accept-Language: en-US,en;q=0.5
Cdn-Loop: cloudflare
Cf-Connecting-Ip: 107.201.65.5
Cf-Ipcountry: US
Cf-Ray: 62289b90aa55f975-EWR
Cf-Request-Id: 084d3f8e740000f975e0038000000001
Cf-Visitor: {"scheme":"https"}
Cookie: __cfduid=d9df5daa57fae5a4e425173aaaaacbfc91613136177
Dnt: 1
Sec-Gpc: 1
Upgrade-Insecure-Requests: 1
X-Forwarded-For: 123.123.123.123
X-Forwarded-Host: project5-headers.tdm.wiki
X-Forwarded-Port: 443
X-Forwarded-Proto: https
X-Forwarded-Server: 6afe64faffaf
X-Real-Ip: 123.123.123.123
````

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_qg61j5iu)

**Relevant topics:** [requests](#p-requests)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Response code received (a number), and an explanation of what that HTTP response code means.
- What you would (probably) be seeing in a browser if you were blocked.
- Python code used to "trick" the website into being scraped.
- The content of the successfully scraped site.
```

##### 2. Open a browser and navigate to: https://project5-rate-limit.tdm.wiki/. While at first glance, it will seem identical to https://project5-headers.tdm.wiki/, it is not. https://project5-rate-limit.tdm.wiki/ is rate limited based on IP address. Depending on when you are completing this project, this may or may not be obvious. If you refresh your browser fast enough, instead of receiving a bunch of information, you will receive text that says "Too Many Requests". 

##### The following function tries to scrape the `Cf-Request-Id` header which will have a unique value each request:

```{python, eval=F}
import requests
import lxml.html

def scrape_cf_request_id(url):
    resp = requests.get(url)
    tree = lxml.html.fromstring(resp.text)
    content = tree.xpath("//p")[0].text.split('\n')
    cfid = [l for l in content if 'Cf-Request-Id' in l][0].split()[1]
    return cfid
```

##### You can test it out:

```{python, eval=F}
scrape_cf_request_id("https://project5-rate-limit.tdm.wiki")
```

##### Write code to scrape 10 unique `Cf-Request-Id`s (in a loop), and save them to a list called `my_ids`. What happens when you run the code? This is caused by our expected text not being present. Instead text with "Too Many Requests" is. While normally this error would be something that makes more sense, like an HTTPError or a Timeout Exception, it _could_ be anything, depending on your code. 

##### One solution that might come to mind is to "wait" between each loop using `time.sleep()`. While yes, this may work, it is not a robust solution. Other users from your IP address may count towards your rate limit and cause your function to fail, the amount of sleep time may change dynamically, or even be manually adjusted to be longer, etc. The best way to handle this is to used something called exponential backoff.

##### In a nutshell, exponential backoff is a way to increase the wait time (exponentially) until an acceptable rate is found. [`backoff`](https://pypi.org/project/backoff/) is an excellent package to do just that. `backoff`, upon being triggered from a specified error or exception, will wait to "try again" until a certain amount of time has passed. Upon receving the same error or exception, the time to wait will increase exponentially. Use `backoff` to modify the provided `scrape_cf_request_id` function to use exponential backoff when the we alluded to occurs. Test out the modified function in a loop and print the resulting 10 `Cf-Request-Id`s.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_p0ioevlc)

**Note:** `backoff` utilizes decorators. For those interested in learning about decorators, [this](https://realpython.com/primer-on-python-decorators/) is an excellent article.

**Relevant topics:** [requests](#p-requests)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- What happens when you run the function 10 times in a row?
- Fixed code that will work regardless of the rate limiting.
- 10 unique `Cf-Request-Id`s printed.
```

##### 3. You now have a great set of tools to be able to scrape pretty much anything you want from the internet. Now all that is left to do is practice. Find a course appropriate website containing data you would like to scrape. Utilize the tools you've learned about to scrape at least 100 "units" of data. A "unit" is just a representation of what you are scraping. For example, a unit could be a tweet from Twitter, a basketball player's statistics from sportsreference, a product from Amazon, a blog post from your favorite blogger, etc.

##### The hard requirements are:

- Documented code with thorough comments explaining what the code does.
- At least 100 "units" scraped.
- The data must be from multiple web pages.
- Write at least 1 function (with a docstring) to help you scrape.
- A clear explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example a `head` of a pandas dataframe containing the data). 

```{block, type="bbox"}
**Item(s) to submit:**

- Python code that scrapes 100 unites of data (with thorough comments explaining what the code does).
- The data must be from more than a single web page.
- 1 or more functions (with docstrings) used to help you scrape/parse data.
- Clear documentation and explanation of what your scraper scrapes, challenges you encountered (if any) and how you overcame them, and a sample of your data printed out (for example using the `head` of a dataframe containing the data).
```

---

### Project 6 {#p06-390}

---

**Motivation:** Being able to analyze and create good visualizations is a skill that is invaluable in _many_ fields. It can be pretty fun too! In this project, you can pick and choose if a couple of different plotting projects.

**Context:** We've been working hard all semester, learning a lot about web scraping. In this project, you are given the choice between a project designed to go through some `matplotlib` basics, and a project that has you replicate plots from a book using `plotly` (an interactive plotting package) inside a Jupyter Notebook (which you would submit instead of an RMarkdown file).

**Scope:** python, visualizing data

**Learning objectives:** 

```{block, type="bbox"}
- Demostrate the ability to create basic graphs with default settings.
- Demonstrate the ability to modify axes labels and titles.
- Demonstrate the ability to customize a plot (color, shape/linetype).
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_2i0gm004)

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Option 1

[A maybe-too-familiar-to-you 29000 project](#p06-290)

#### Option 2

##### 1. [Here](https://github.com/oscarperpinan/bookvis/tree/master/figs) are a variety of interesting graphics from the popular book [Displaying time series, spatial and space-time data with R](https://oscarperpinan.github.io/spacetime-vis/) by Oscar Perpinan Lamigueiro. You can replicate the graphics using data found [here](https://github.com/oscarperpinan/bookvis/tree/master/data). 

##### Choose 3 graphics from the book to replicate using `plotly`. The replications do not need to be perfect -- a strong effort to get as close as possible is fine. Feel free to change colors as you please. If you have the desire to improve the graphic, please feel free to do so and explain how it is an improvement. 

##### Use https://notebook.scholar.rcac.purdue.edu and the f2020-s2021 kernel to complete this project. The _only_ thing you need to submit for this project is the downloaded .ipynb file. Make sure that the grader will be able to click "run all" (using the same kernel, f2020-s2021), and have everything run properly.

**Important note:** The object of this project is to challenge yourself (as much as you want), learn about and mess around with `plotly`, and be creative. If you have an idea for a cool plot, graphic, or modification, please include it!

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code.
```


---

### Project 7 {#p07-390}

---

**Motivation:** Being able to analyze and create good visualizations is a skill that is invaluable in _many_ fields. It can be pretty fun too! As you probably noticed in the previous project, `matplotlib` can be finicky -- certain types of plots are really easy to create, while others are not. For example, you would think changing the color of a boxplot would be easy to do in `matplotlib`, perhaps we just need to add an option to the function call. As it turns out, this isn't so straightforward (as illustrated at the end of [this section](#p-matplotlib-boxplot)). Occasionally this will happen and that is when packages like `seaborn` or `plotnine` (both are packages built using `matplotlib`) can be good. In this project we will explore this a little bit, and learn about some useful `pandas` functions to help shape your data in a format that any given package requires.

**Context:** In the next project, we will continue to learn about and become comfortable using `matplotlib`, `seaborn`, and `plotnine`.

**Scope:** python, visualizing data

**Learning objectives:** 

```{block, type="bbox"}
- Demonstrate the ability to create basic graphs with default settings.
- Demonstrate the ability to modify axes labels and titles.
- Demonstrate the ability to customize a plot (color, shape/linetype).
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/apple/health/watch_dump.xml`

#### Questions

##### 1. In an earlier project we explored some XML data in the form of an Apple Watch data dump. Most health-related apps give you some sort of graph or set of graphs as an output. Use any package you want to parse the XML data. There are a lot of `Record`s in this dataset. Each `Record` has an attribute called `creationDate`. Create a barplot of the number of `Record`s per day. Make sure your plot is polished, containing proper labels and good colors.

**Hint:** You could start by parsing out the required data into a `pandas` dataframe or series.

**Hint:** The `groupby` method is one of the most useful `pandas` methods. It allows you to quickly perform operations on groups of data. 

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_69sdl8rk)

[Click here for another video](https://mediaspace.itap.purdue.edu/id/1_ho5ej4gy)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 2. The plot in question 1 should look bimodal. Let's focus only on the first apparent group of readings. Create a new dataframe containing only the readings for the time period from 9/1/2017 to 5/31/2019. How many `Record`s are there in that time period?

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_wlkijwnv)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 3. It is hard to discern weekly patterns (if any) based on the graphics created so far. For the period of time in question 2, create a labeled bar plot for the count of `Record`s by day of the week. What (if any) discernable patterns are there? Make sure to include the labels provided below:

```{python, eval=F}
labels = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_e7d9ve0o)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 4. Create a `pandas` dataframe containing the following data from `watch_dump.xml`:

- A column called `bpm` with the `bpm` (beats per minute) of the `InstantaneousBeatsPerMinute`.
- A column called `time` with the `time` of each individual `bpm` reading in `InstantaneousBeatsPerMinute`.
- A column called `date` with the date.
- A column called `dayofweek` with the day of the week.

**Hint:** You may want to use `pd.to_numeric` to convert the `bpm` column to a numeric type.

**Hint:** This is one way to convert the numbers 0-6 to days of the week:

```{python, eval=F}
myDF['dayofweek'] = myDF['dayofweek'].map({0:"Mon", 1:"Tue", 2:"Wed", 3:"Thu", 4:"Fri", 5: "Sat", 6: "Sun"})
```

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_h8crbise)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [barplot](#p-matplotlib-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

##### 5. Create a heatmap using `seaborn`, where the y-axis shows the day of the week ("Mon" - "Sun"), the x-axis shows the hour, and the values on the interior of the plot are the average `bpm` by hour by day of the week.

[Click here for video](https://mediaspace.itap.purdue.edu/id/1_rl5iu9am)

**Relevant topics:** [lxml](#p-lxml), [groupby](#p-pandas-groupby), [pivot](#p-pandas-pivot)

```{block, type="bbox"}
**Item(s) to submit:**

- Python code used to solve the problem.
- Output from running your code (including the graphic).
```

---
