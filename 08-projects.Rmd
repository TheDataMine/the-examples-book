# Projects {#projects}

## Templates {#templates}

Our course project template can be found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), or on Scholar: 

`/class/datamine/apps/templates/project_template.Rmd`

This video demonstrates:

* opening a browser (emphasizing Firefox as the best choice),
* opening RStudio Server Pro (https://rstudio.scholar.rcac.purdue.edu),
* introducing (basics) about what RStudio looks like,
* checking to see that the students are using R 4.0,
* running the initial (one-time) setup script,
* opening the project template,
* knitting the template into a PDF file, and
* finally handling the popup blocker, which can potentially block the PDF.

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_444kq84l&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5wx961lv" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

Students in STAT 19000, 29000, and 39000 are to use this as a template for all project submissions. The template includes a code chunk that "activates" our Python environment, and adjusts some default settings. In addition, it provides examples on how to include solutions for Python, R, Bash, and SQL. Every question should be clearly marked with a third-level header (using 3 `#`s) followed by `Question 1`, `Question 2`, etc. Sections for solutions should be added or removed, based on the number of questions in the given project. All code chunks are to be run and solutions displayed for the compiled PDF submission.

Any format or template related questions should be asked in Piazza.

## Submissions {#submissions}

Unless otherwise specified, all projects will need 2-4 submitted files: 

1. A compiled PDF file (built using the template), with all code and output.
2. The .Rmd file (based off of [the template](#templates)), used to Knit the final PDF.
3. If it is a project containing R code, a .R file containing all of the R code with comments explaining what the code does. _Note: This is **not** an .Rmd file._
4. If it is a project containing Python code, a .py file containing all of the Python code.

## STAT 19000

### Project 1 {#p01-190}

---

**Motivation:** In this project we are going to jump head first into The Data Mine. We will load datasets into the R environment, and introduce some core programming concepts like variables, vectors, types, etc. As we will be "living" primarily in an IDE called RStudio, we will take some time to learn how to connect to it, configure it, and run code.

**Context:** This is our first project as a part of The Data Mine. We will get situated, configure the environment we will be using throughout our time with The Data Mine, and jump straight into working with data!

**Scope:** r, rstudio, Scholar

**Learning objectives:**

```{block, type="bbox"}
- Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc.
- Install R and setting up a working environment.
- Explain and demonstrate: positional, named, and logical indexing.
- Read and write basic (csv) data.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/disney/splash_mountain.csv`

#### Questions

##### 1. Read the webpage [here](https://www.rcac.purdue.edu/compute/scholar/). Scholar is the computing cluster you will be using throughout the semester, and your time with The Data Mine. Each _node_ is an individual machine with CPUs and memory (RAM). How many _cores_ and how much _memory_ is available, in total, for our 7 frontend nodes? How about for the sub-clusters? How much is available on your computer or laptop?

```{block, type="bbox"}
**Item(s) to submit:**

- A sentence explaining how much memory and how many cores the 7 frontends have combined.
- A sentence explaining how much memory and how many cores the 28 sub-clusters have combined.
- A sentence explaining how much memory and how many cores your personal computer has.
```

##### Solution

The memory on the 7 frontend nodes is: 4 * 512 + 3 * 768 = 4352 GB = 4.3 TB.

The memory on the 28 nodes in the sub-cluster is: 24 * 64 + 4 * 192 = 2304 GB = 2.3 TB.

The memory on Dr. Ward's laptop is 8 GB.

The number of cores on the 7 frontend nodes is: 4 * 20 + 3 * 20 = 140 cores.

The number of cores on the 28 nodes in the sub-cluster is: 24 * 20 + 4 * 16 = 544 cores.

The number of cores on Dr. Ward's laptop is 2.




##### 2. Navigate and login to https://rstudio.scholar.rcac.purdue.edu using your Purdue Career Account credentials (and Boilerkey). This is an instance of RStudio Server running on a Scholar frontend! Frontends are labeled. So, for example, `scholar-fe01.rcac.purdue.edu` is frontend `#1`.  Create a new R file (File > New File > R Script). Check which frontend you are logged in on by running the following in the new file: `system("hostname")`.

##### Press Control and Enter/Return keys at the same time, to run this line. Which frontend are you in?

**Relevant topics:** [running R code](#running-R-code)

```{block, type="bbox"}
**Item(s) to submit:**

- The `#` of the frontend your RStudio Server session is running on.
```

##### Solution

The node we are working on is:

```{r, eval=F}
system("hostname", intern=T)
```


##### 3. From within RStudio, we can run every type of code that you will need to run throughout your time with The Data Mine: Python, R, Bash, SQL, etc. We've created a one-time setup script for you to run, called
`/class/datamine/apps/runme.sh` (as seen in the video at the top of this page).

##### After you restart R (as in the video, after 4 minutes and 16 seconds), there should be a message that is printed in your "Console" tab. What does the message say?

```{block, type="bbox"}
**Item(s) to submit:**

- The sentence that is printed in the RStudio "Console".
```

##### Solution

The welcome message is:
`"You've successfully loaded The Data Mine R settings!"`



##### 4. Projects in The Data Mine should all be submitted using our template found [here](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd) or on Scholar (`/class/datamine/apps/templates/project_template.Rmd`). At the beginning of every project, the first step should be downloading and/or copying and pasting the template into a `.Rmd` file in RStudio.  This is also demonstrated in the video at the top of this page.

##### Open the project template and save it into your home directory, in a new RMarkdown file named `project01.Rmd`.

##### Code chunks are parts of the RMarkdown file that contains code. You can identify what type of code a code chunk contains by looking at the _engine_ in the curly braces "{" and "}". How many of each type of code chunk are in our default template?

**Hint:** You can read about the template [here](https://thedatamine.github.io/the-examples-book/projects.html#templates). 

```{block, type="bbox"}
**Item(s) to submit:**

- A list containing the type of code chunk (r, Python, sql, etc), and how many of each code chunks our default template contains.
```

##### Solution

There are 3 chunks of R code, 1 chunk of bash, 1 chunk of Python, 1 chunk of SQL



##### 5. Fill out the project template, replacing the default information with your own. If a category is not applicable to you, put N/A. This template provides examples of how to run each "type" of code we will run in this course. Look for the second R code chunk, and run it by clicking the tiny green play button in the upper right hand corner of the code chunk. What is the output?

```{block, type="bbox"}
**Item(s) to submit:**

- The output from running the R code chunk.
```

##### Solution

We store `1, 2, 3` into the variable my_variable, and then we display output: `1, 2, 3`
```{r}
my_variable <- c(1,2,3)
my_variable
```


##### 6. In question (1) we answered questions about CPUs and RAM for the Scholar cluster. To do so, we needed to perform some arithmetic. Instead of using a calculator (or paper), write these calculations using R. Replace the content of the second R code chunk in our template with your calculations.

**Relevant topics:** [templates](#templates)

```{block, type="bbox"}
**Item(s) to submit:**

- The R code chunk with your calculations, and output.
```

##### Solution

We go back to question 1 and compute directly
```{r}
4 * 512 + 3 * 768
24 * 64 + 4 * 192

4 * 20 + 3 * 20
24 * 20 + 4 * 16
```


##### 7. In (6) we got to see how you can type out arithmetic and R will calculate the result for you. One constant throughout the semester will be loading datasets into R. Load our dataset into R by running the following code:

```{r, eval=F}
dat <- read.csv("/class/datamine/data/disney/splash_mountain.csv")
```

Confirm the dataset has been read in by running the `head` function on it. `head` prints the first few rows of data:

```{r, eval=F}
head(dat)
```

`dat` is a variable which contains our data! We can name this variable anything we want, we do _not_ have to name it `dat`. Run our code to read in our dataset, this time, instead of naming our resulting dataset `dat`, name it `splash_mountain`. Place all of your code into a new R code chunk under a new level 3 header (i.e. `### Question 7`).

**Relevant topics:** [reading data in R](#r-reading-and-writing-data)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to answer this question in a code chunk in our template.
- Output of `head`.
```

##### Solution

We load in splash_mountain data and display the head

```{r, echo=F}
splash_mountain <- read.csv("http://llc.stat.purdue.edu/splash_mountain.csv")
```

```{r, eval=F}
splash_mountain <- read.csv("/class/datamine/data/disney/splash_mountain.csv")
```

```{r}
head(splash_mountain)
```

##### 8. Let's pretend we are now done with our project. We've written some R code, maybe added some text explaining what we did, and we are ready to turn things in. For this course, we will turn in a variety of work, depending on the type of project.

##### We will always require a PDF which contains text, code, and code output. Normally we would erase any code chunks from the template that are not used, however, for this project, it is OK to just keep the rest of the template intact.

##### A PDF is generated by "knitting" a PDF (using the "knit" button in RStudio).

##### In addition, if the project uses R code, you will need to also submit R code in an R script (file ending with `.R`).  (Later this year, when submitting Python code, you will submit a Python script instead.)

##### Let's practice. Take the code from your `project01.R` file and paste it (perhaps one or two lines at time) into your RMarkdown file (file ending with `.Rmd`).

##### Compile your RMarkdown project into a PDF. Follow the directions in Brightspace to upload and submit your RMarkdown file, compiled PDF, and R script. 

**Relevant topics:** [templates](#templates)

```{block, type="bbox"}
**Item(s) to submit:**

- Resulting knitted PDF.
- `project01.R` script (with all of your R code) and the analogous `project01.Rmd` file.
```

##### How to build the R script for Project 1 in STAT 19000.

In the videos below, for Question 1 and Question 6,
Dr. Ward forgot to calculate the number of cores.
(He only included the total amount of memory.)
Dr. Ward is a human being who sometimes makes mistakes.
Please remember to (also) calculate the number of cores,
when you submit Question 1 and Question 6!

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_v8c05lak&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_q2jq19e0" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

##### How to build the Rmd file and the PDF file for Project 1 in STAT 19000. 

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_ccpgymsn&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5kbz3ae1" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

---

### Project 2 {#p02-190}

---

##### Introduction to R using 84.51 examples

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_xqba3s8y&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=0_a5qq9eet" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

##### Introduction to R using NYC Yellow Taxi Cab examples

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_ryucs8fg&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=0_gpzkq4ub" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

**Motivation:** The R environment is a powerful tool to perform data analysis. R is a tool that is often compared to Python. Both have their advantages and disadvantages, and both are worth learning. In this project we will dive in head first and learn the basics while solving data-driven problems.

**Context:** Last project we set the stage for the rest of the semester. We got some familiarity with our project templates, and modified and ran some R code. In this project, we will continue to use R within RStudio to solve problems. Soon you will see how powerful R is and why it is often a more effective tool to use than spreadsheets.

**Scope:** r, vectors, indexing, recycling

**Learning objectives:**

```{block, type="bbox"}
- List the differences between lists, vectors, factors, and data.frames, and when to use each.
- Explain and demonstrate: positional, named, and logical indexing.
- Read and write basic (csv) data.
- Explain what "recycling" is in R and predict behavior of provided statements.
- Identify good and bad aspects of simple plots.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/disney/metadata.csv`

A public sample of the data can be found here: [/class/datamine/data/disney/metadata.csv](https://www.datadepot.rcac.purdue.edu/datamine/data/disney/metadata.csv)

#### Questions

##### 1. Use the `read.csv` function to load [/class/datamine/data/disney/metadata.csv](https://www.datadepot.rcac.purdue.edu/datamine/data/disney/metadata.csv) into a `data.frame` called `myDF`. Note that `read.csv` _by default_ loads data into a `data.frame`. (We will learn more about the idea of a `data.frame`, but for now, just think of it like a spreadsheet, in which each column has the same type of data.) Print the first few rows of `myDF` using the `head` function (as in Project 1, Question 7).

**Relevant topics:** [reading data in r](#reading-and-writing-data), [head](#head)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem in an R code chunk.
```

##### 2. We've provided you with R code below that will extract the column `WDWMAXTEMP` of `myDF` into a vector. What is the 1st value in the vector? What is the 50th value in the vector? What type of data is in the vector? (For this last question, use the `typeof` function to find the type of data.)

```{r, eval=F}
our_vec <- myDF$WDWMAXTEMP
```

**Relevant topics:** [indexing in r](#r-indexing), [type](#r-type), [creating variables](#r-variables)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem in an R code chunk.
- The values of the first, and 50th element in the vector.
- The type of data in the vector (using the `typeof` function).
```

##### 3. Use the head function to create a vector called `first50` that contains the first 50 values of the vector `our_vec`. Use the tail function to create a vector called `last50` that contains the last 50 values of the vector `our_vec`.

##### You can access many elements in a vector at the same time. To demonstrate this, create a vector called `mymix` that contain the sum of each element of `first50` being added to the analogous element of `last50`.

**Relevant topics:** [indexing in r](#r-indexing), [creating variables](#r-variables), [head and tail](#r-head)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem. 
- The contents of each of the three vectors.
```

##### 4. In (3) we were able to rapidly add values together from two different vectors. Both vectors were the same size, hence, it was obvious which elements in each vector were added together.

##### Create a new vector called `hot` which contains only the values of `myDF$WDWMAXTEMP` which are greater than or equal to 80 (our vector contains max temperatures for days at Disney World). How many elements are in `hot`?

##### Calculate the sum of `hot` and `first50`.  Do we get a warning? Read [this](https://excelkingdom.blogspot.com/2018/01/what-recycling-of-vector-elements-in-r.html) and then explain what is going on. 

**Relevant topics:** [logical indexing](#r-indexing), [length](#r-length), [recycling](#r-recycling)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem. 
- 1-2 sentences explaining what is happening when we are adding two vectors of different lengths.
```

##### 5. Plot the `WDWMAXTEMP` vector from `myDF`.

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem. 
- Plot of the `WDWMAXTEMP` vector from `myDF`.
```

**Relevant topics:** [plotting](#r-plotting)

##### 6. The following three pieces of code each create a graphic. The first two graphics are created using only core R functions.  The third graphic is created using a package called `ggplot`. We will learn more about all of these things later on. For now, pick your favorite graphic, and write 1-2 sentences explaining why it is your favorite, what could be improved, and include any interesting observations (if any).

```{r, eval=F}
dat <- table(myDF$SEASON)
dotchart(dat, main="Seasons", xlab="Number of Days in Each Season")
```

![](./images/stat19000project2figure1.png)

```{r, eval=F}
dat <- tapply(myDF$WDWMEANTEMP, myDF$DAYOFYEAR, mean, na.rm=T)
seasons <- tapply(myDF$SEASON, myDF$DAYOFYEAR, function(x) unique(x)[1])
pal <- c("#4E79A7", "#F28E2B", "#A0CBE8",  "#FFBE7D", "#59A14F", "#8CD17D", "#B6992D", "#F1CE63", "#499894", "#86BCB6", "#E15759", "#FF9D9A", "#79706E", "#BAB0AC", "#1170aa", "#B07AA1")
colors <- factor(seasons)
levels(colors) <- pal
par(oma=c(7,0,0,0), xpd=NA)
barplot(dat, main="Average Temperature", xlab="Jan 1 (Day 0) - Dec 31 (Day 365)", ylab="Degrees in Fahrenheit", col=as.factor(colors), border = NA, space=0)
legend(0, -30, legend=levels(factor(seasons)), lwd=5, col=pal, ncol=3, cex=0.8, box.col=NA)
```

![](./images/stat19000project2figure2.png)


```{r, eval=F}
library(ggplot2)
library(tidyverse)
summary_temperatures <- myDF %>%
    select(MONTHOFYEAR,WDWMAXTEMP:WDWMEANTEMP) %>%
    group_by(MONTHOFYEAR) %>%
    summarise_all(mean, na.rm=T)
ggplot(summary_temperatures, aes(x=MONTHOFYEAR)) +
    geom_ribbon(aes(ymin = WDWMINTEMP, ymax = WDWMAXTEMP), fill = "#ceb888", alpha=.5) +
    geom_line(aes(y = WDWMEANTEMP), col="#5D8AA8") +
    geom_point(aes(y = WDWMEANTEMP), pch=21,fill = "#5D8AA8", size=2) +
    theme_classic() +
    labs(x = 'Month', y = 'Temperature', title = 'Average temperature range' ) +
    scale_x_continuous(breaks=1:12, labels=month.abb)
```

![](./images/stat19000project2figure3.png)


---

### Project 3 {#p03-190}

---

**Motivation:** `data.frame`s are the primary data structure you will work with when using R. It is important to understand how to insert, retrieve, and update data in a `data.frame`. 

**Context:** In the previous project we got our feet wet, and ran our first R code, and learned about accessing data inside vectors. In this project we will continue to reinforce what we've already learned and introduce a new, flexible data structure called `data.frame`s.

**Scope:** r, data.frames, recycling, factors

**Learning objectives:**

```{block, type="bbox"}
- Explain what "recycling" is in R and predict behavior of provided statements.
- Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- List the differences between lists, vectors, factors, and data.frames, and when to use each.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/disney`

#### Questions

##### 1. Read the dataset `/class/datamine/data/disney/splash_mountain.csv` into a data.frame called `splash_mountain`. How many columns, or features are in each dataset? How many rows or observations?

**Relevant topics:** [str](#r-str), [dim](#r-dim)

```{block, type="bbox"}
**Item(s) to include:**

- R code used to solve the problem.
- How many columns or features in each dataset?
```

##### 2. Splash Mountain is a fan favorite ride at Disney World's Magic Kingdom theme park. `splash_mountain` contains a series of dates and datetimes. For each datetime, `splash_mountain` contains a posted minimum wait time, `SPOSTMIN`, and an actual minimum wait time, `SACTMIN`. What is the average posted minimum wait time for Splash Mountain? What is the standard deviation? Based on the fact that `SPOSTMIN` represents the posted minimum wait time for our ride, does our mean and standard deviation make sense? Explain.  (You might look ahead to Question 3 before writing the answer to Question 2.)

**Hint:** If you got `NA` or `NaN` as a result, see [here](https://thedatamine.github.io/the-examples-book/r.html#r-mean).

**Relevant topics:** [mean](#r-mean), [var](#r-var), [NA](#r-na), [NaN](#r-nan)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The results of running the R code. 
- 1-2 sentences explaining why or why not the results make sense.
```

##### 3. In (2) we got some peculiar values for the mean and standard deviation. If you read the "attractions" tab in the file `/class/datamine/data/disney/touringplans_data_dictionary.xlsx`, you will find that -999 is used as a value in `SPOSTMIN` and `SACTMIN` to indicate the ride as being closed. Recalculate the mean and standard deviation of `SPOSTMIN`, excluding values that are -999. Does this seem to have fixed our problem?

**Relevant topics:** [NA](#r-na), [mean](#r-mean), [var](#r-var), [indexing](#r-indexing), [which](#r-which)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The result of running the R code.
- A statement indicating whether or not the value look reasonable now.
```

##### 4. `SPOSTMIN` and `SACTMIN` aren't the greatest feature/column names. An outsider looking at the data.frame wouldn't be able to immediately get the gist of what they represent. Change `SPOSTMIN` to `posted_min_wait_time` and `SACTMIN` to `actual_wait_time`. 

**Hint:** You can always use hard-coded integers to change names manually, however, if you use `which`, you can get the index of the column name that you would like to change. For data.frames like `splash_mountain`, this is a lot more efficient than manually counting which column is the one with a certain name.

**Relevant topics:** [colnames](#r-colnames-and-rownames), [names](#r-names), [which](#r-which)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The output from executing `names(splash_mountain)` or `colnames(splash_mountain)`.
```

##### 5. Use the `cut` function to create a new vector called `quarter` that breaks the `date` column up by quarter. Use the `labels` argument in the `factor` function to label the quarters "q1", "q2", ..., "qX" where `X` is the last quarter. Add `quarter` as a column named `quarter` in `splash_mountain`. How many quarters are there?

**Hint:** If you have 2 years of data, this will result in 8 quarters: "q1", ..., "q8".

**Hint:** We can generate sequential data using `seq` and `paste0`:

```{r, eval=T}
paste0("item", seq(1, 5))
```

or

```{r, eval=T}
paste0("item", 1:5)
```

**Relevant topics:** [cut](#r-cut), [dates](#r-dates), [factor](#r-factors), [paste0](#r-paste), seq, [nlevels](#r-nlevels)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The `head` and `tail` of `splash_mountain`.
- The number of quarters in the new `quarter` column.
```

Question 5 is intended to be a little more challenging, so we worked through the _exact_ same steps, with two other data sets.  That way, if you work through these, all you will need to do, to solve Question 5, is to follow the example, and change two things, namely, the data set itself (in the `read.csv` file) and also the format of the date.

This basically steps you through _everything_ in Question 5.

We hope that these are helpful resources for you!  We appreciate you very much and we are here to support you!  You would not know how to solve this question on your own--because we are just getting started--but we like to sometimes put in a question like this, in which you get introduced to several new things, and we will dive deeper into these ideas as we push ahead.

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_mq1vhejd&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_1aapmvkx" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_2xg1g5rq&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ud2o1irw" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

##### 6. Please include a statement in Project 3 that says, "I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course." or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan.

---

### Project 4 {#p04-190}

---

**Motivation:** Control flow is (roughtly) the order in which instructions are executed. We can execute certain tasks or code _if_ certain requirements are met using if/else statements. In addition, we can perform operations many times in a loop using for loops. While these are important concepts to grasp, R differs from other programming languages in that operations are usually vectorized and there is little to no need to write loops.  

**Context:** We are gaining familiarity working in RStudio and writing R code. In this project we introduce and practice using control flow in R. 

**Scope:** r, data.frames, recycling, factors, if/else, for

**Learning objectives:**

```{block, type="bbox"}
- Explain what "recycling" is in R and predict behavior of provided statements.
- Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- List the differences between lists, vectors, factors, and data.frames, and when to use each.
- Demonstrate a working knowledge of control flow in r: if/else statements, while loops, etc.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/disney`

#### Questions

##### 1. Use `read.csv` to read in the `/class/datamine/data/disney/splash_mountain.csv` data into a data.frame called `splash_mountain`. In the previous project we calculated the mean and standard deviation of the `SPOSTMIN` (posted minimum wait time). These are vectorized operations (we will learn more about this next project). Instead of using the `mean` function, use a loop to calculate the mean(average), just like the previous project. Do not use `sum` either.

**Hint:** Remember, if a value is NA, we don't want to include it.

**Hint:** Remember, if a value is -999, it means the ride is closed, we don't want to include it.

**Note:** This exercise should make you appreciate the variety of useful functions R has to offer!

**Relevant topics:** [for loops](#r-for-loops), [if/else statements](#r-if-else), [is.na](#r-isna)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem w/comments explaining what the code does.
- The mean posted wait time.
```

##### 2. Choose one of the `.csv` files containing data for a ride. Use `read.csv` to load the file into a data.frame named `ride_name` where "ride_name" is the name of the ride you chose. Use a for loop to loop through the ride file and add a new column called `status`. `status` should contain a string whose value is either "open", or "closed". If `SPOSTMIN` or `SACTMIN` is -999, classify the row as "closed". Otherwise, classify the row as "open". After `status` is added to your data.frame, convert the column to a `factor`.

**Hint:** If you want to access two columns at once from a data.frame, you can do: `splash_mountain[i, c("SPOSTMIN", "SACTMIN")]`.

**Relevant topics:** [any](#r-any), [for loops](#r-for-loops), [if/else statements](#r-if-else), [nrow](#r-data-frames)

**Note:** For loops are often [much slower (here is a video to demonstrate)](#r-for-loops-versus-vectorized-functions) than vectorized functions, as we will see in (3) below.

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem w/comments explaining what the code does.
- The output from running `str` on `ride_name`.
```
In this video, we basically go all the way through Question 2 using a video:

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_867pc9uq&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_txgdir1q" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>


##### 3. Typically you want to avoid using for loops (or even apply functions (we will learn more about these later on, don't worry)) when they aren't needed. Instead you can use vectorized operations and indexing. Repeat (2) without using any for loops or apply functions (instead use indexing and the `which` function). Which method was faster?

**Hint:** To have multiple conditions within the `which` statement, use `|` for logical OR and `&` for logical AND.

**Hint:** You can start by assigning every value in `status` as "open", and then change the correct values to "closed".

**Note:** Here is a [complete example (very much like question 3) with another video](#r-example-safe-versus-contaminated) that shows how we can classify objects.

**Note:** Here is a [complete example with a video](#r-example-for-loops-compared-to-vectorized-functions) that makes a comparison between the concept of a for loop versus the concept for a vectorized function.

**Relevant topics:** [which](#r-which)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem w/comments explaining what the code does.
- The output from running `str` on `ride_name`.
```

##### 4. Create a pie chart for open vs. closed for `splash_mountain.csv`. First, use the `table` command to get a count of each `status`. Use the resulting table as input to the `pie` function. Make sure to give your pie chart a title that somehow indicates the ride to the audience.

**Relevant topics:** [pie](#r-pie), [table](#r-table)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem w/comments explaining what the code does.
- The resulting plot displayed as output in the RMarkdown.
```

##### 5. Loop through the vector of files we've provided below, and create a pie chart of open vs closed for each ride. Place all 6 resulting pie charts on the same image. Make sure to give each pie chart a title that somehow indicates the ride.

```{r, eval=F}
ride_names <- c("splash_mountain", "soarin", "pirates_of_caribbean", "expedition_everest", "flight_of_passage", "rock_n_rollercoaster")
ride_files <- paste0("/class/datamine/data/disney/", ride_names, ".csv")
```

**Hint:** To place all of the resulting pie charts in the same image, prior to running the for loop, run `par(mfrow=c(2,3))`.

**Relevant topics:** [for loop](#r-for-loops), [read.csv](#r-reading-and-writing-data), [pie](#r-pie), [table](#r-table), [par](#r-par)

This is not exactly the same, but it is a similar example, using the campaign election data:

```{r, eval=F}
mypiechart <- function(x) {
  myDF <- read.csv( paste0("/class/datamine/data/election/itcont", x, ".txt"), sep="|")
  mystate <- rep("other", times=nrow(myDF))
  mystate[myDF$STATE == "CA"] <- "California"
  mystate[myDF$STATE == "TX"] <- "Texas"
  mystate[myDF$STATE == "NY"] <- "New York"
  myDF$stateclassification <- factor(mystate)
  pie(table(myDF$stateclassification))
}

myyears <- c("1980","1984","1988","1992","1996","2000")
par(mfrow=c(2,3))
for (i in myyears) {
  mypiechart(i)
}
```

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_mi6rahcs&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_49hh9f0u" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem w/comments explaining what the code does.
- The resulting plot displayed as output in the RMarkdown.
```

---

### Project 5 {#p05-190}

---

**Motivation:** As briefly mentioned in project 4, R differs from other programming languages in that _typically_ you will want to avoid using for loops, and instead use vectorized functions and the apply suite. In this project we will demonstrate some basic vectorized operations, and how they are better to use than loops.

**Context:** While it was important to stop and learn about looping and if/else statements, in this project, we will explore the R way of doing things.

**Scope:** r, data.frames, recycling, factors, if/else, for

**Learning objectives:**

```{block, type="bbox"}
- Explain what "recycling" is in R and predict behavior of provided statements.
- Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- List the differences between lists, vectors, factors, and data.frames, and when to use each.
- Demonstrate a working knowledge of control flow in r: for loops .
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/fars`

To get more information on the dataset, see [here](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812602).

#### Questions

##### 1. The `fars` dataset contains a series of folders labeled by year. In each year folder there is (at least) the files `ACCIDENT.CSV`, `PERSON.CSV`, and `VEHICLE.CSV`. If you take a peek at any `ACCIDENT.CSV` file in any year, you'll notice that the column `YEAR` only contains the last two digits of the year. Add a new `YEAR` column which contains the _full_ year. Use a loop, and `rbind` function to create a data.frame called `accidents` which combines all years' `ACCIDENT.CSV` files into one big dataset. As you are looping through each of the years (from [1975, 1981]), make sure to fix the `YEAR`.

**Relevant topics:** [rbind](#r-bind), [for loops](#r-for-loops), [read.csv](#r-read)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The result of `unique(accidents$year)`.
```

##### 2. Using the new `accidents` data you created in (1), how many accidents are there where 1+ drunk drivers were involved in an accident with a school bus?

**Hint:** Look at the variables `DRUNK_DR` and `SCH_BUS`.

**Relevant topics:** [table](#r-table)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The result/answer itself.
```

##### 3. For accidents involving 1+ drunk drivers and a school bus, how many happened in each of the 7 years? Which year had the most qualifying accidents?

**Relevant topics:** [table](#r-table), [which](#r-which), [indexing](#r-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The results.
- Which year had the most qualifying accidents.
```

##### 4. Calculate the mean number of motorists involved in an accident (variable `PERSON`) with i drunk drivers where i takes the values from 0 through 6.

**Hint:** It is OK that there are no accidents involving just 5 drunk drivers.

**Relevant topics:** [for loops](#r-for-loops), [mean](#r-mean), [indexing](#r-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The output from running your code.
```

##### 5. We have a theory that there are more accidents in cold weather months for Indiana and states around Indiana. First, filter out all data where `STATE` is not one of: Indiana (18), Illinois (17), Ohio (39), or Michigan (26). Then create a barplot that shows the number of accidents by `STATE` and by month (`MONTH`) simultanously.  What months have the most accidents? Are you surprised by these results? Explain why or why not? 

**Relevant topics:** [in](#r-in), [barplot](#r-barplot)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The output (plot) from running your code.
- 1-2 sentences explaining which month(s) have the most accidents and whether or not this surprises you.
```

##### 6. *(optional)* Spruce up your plot from (5). Do any of the following: 

- add vibrant (and preferably colorblind friendly) colors to your plot
- add a title
- add a legend
- add month names or abbreviations instead of numbers

**Hint:** [Here](https://www.r-graph-gallery.com/209-the-options-of-barplot.html) is a resource to get you started.

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem/comments explaining what the code does.
- The output (plot) from running your code.
```

---

### Project 6 {#p06-190}

---

**Motivation:** `tapply` is a powerful function that allows us to group data, and perform calculations on that data in bulk. The "apply suite" of functions provide a fast way of performing operations that would normally require the use of loops. Typically, when writing R code, you will want to use an "apply suite" function rather than a for loop. 

**Context:** The past couple of projects have studied the use of loops and/or vectorized operations. In this project, we will introduce a function called `tapply` from the "apply suite" of functions in R.

**Scope:** r, for, tapply

**Learning objectives:**

```{block, type="bbox"}
- Explain what "recycling" is in R and predict behavior of provided statements.
- Explain and demonstrate how R handles missing data: NA, NaN, NULL, etc.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- List the differences between lists, vectors, factors, and data.frames, and when to use each.
- Demonstrate a working knowledge of control flow in r: if/else statements, while loops, etc.
- Demonstrate how apply functions are generally faster than using loops.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/fars/7581.csv`

Calculate the number of deaths where there was a drunk driver vs when no drunk driver.

Which state has the most drunk drivers?

#### Questions

##### 1. The dataset, `/class/datamine/data/fars/7581.csv` is the result of question 1 from the previous project. Load up the dataset into a data.frame named `dat`. In the previous project's question 4, we asked you to calculate the mean number of motorists involved in an accident (`PERSON`) with i drunk drivers for i in 0 through 6. Solve this question using `tapply` instead. Which method did you prefer and why?

**Relevant topics:** [tapply](#r-tapply), [mean](#r-mean)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The output/solution.
```

##### 2. Use `/class/datamine/data/states.csv` to map `STATE` codes to the names of states.

**Hints:**

- Make sure to first remove states from `states.csv` that are not in `STATE`, save the resulting vector as `substate`.
- Create an auxiliary variable containing the `STATE` vector converted to a factor.
- Reorder `substate` by `code` using the `order` function.
- Use the `levels` function to set the levels of our auxiliary variable to the reordered `substate`.

**Note:** In the next project, we will learn a much more effective way to accomplish this!

**Relevant topics:** as.factor, [order](#r-order), [levels](#r-levels), %in%

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- `head` of `dat`.
```

##### 3. In the previous project, we calculated how many accidents occured in 4 selected states, each month. If we wanted to extend this to every state, there would be more steps involved. `tapply` is a perfect fit for such a question. Use `tapply` to calculate the number of accidents (each row/observation is an accident) by month (`MONTH`) for each state (`STATE`). Which state has the most accidents?

**Relevant topics:** [tapply](#r-tapply)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The entire output.
- Which state has the most wrecks.
```

##### 4. Use `tapply` to calculate the percentage of accidents during snowy weather and rainy weather. Use the following image to help you:

![](./images/weather.png)

**Hint:** You can solve this using `tapply` twice, or, you can wrap the two conditions you'd like to group by in a `list` by using the `list` function. We will learn more about lists later, however, a `list` is essentially a `vector` containing various types rather than a single type.

**Relevant topics:** [tapply](#r-tapply), [list](#r-lists-and-vectors)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The percentage of accidents during snowy weather.
- The percentage of accidents during rainy weather.
```

##### 5. According to https://www.nhtsa.gov/risky-driving/drunk-driving, around 33% of all traffic crash fatalities in the US involve drunk drivers. Jimbob just learned to use `tapply`, and is bound and determined to use it all of the time. He wanted to see if he can confirm a similar number as https://www.nhtsa.gov using our `/class/datamine/data/fars/7581.csv` dataset. Examine his code, explain what he is doing wrong, and come up with a _much_ simpler solution. Can you confirm the statement from https://www.nhtsa.gov?

```{r, eval=F}
res <- tapply(dat$STATE, list(dat$STATE, dat$DRUNK_DR > 0), length)
mean(res[,2]/(res[,2]+res[,1]))
```

**Relevant topics:** [tapply](#r-tapply)

```{block, type="bbox"}
**Item(s) to submit:**

- 1-2 sentences explaining what Jimbob is doing wrong.
- The _much_ simpler solution to solve the problem.
- Does your solution and result match the findings from https://www.nhtsa.gov?
```

##### 6. Let's put (some of) Jimbob's work to good use, after all, his result, `res` _is_ interesting. Create a data.frame named `myDF` with a column named `state` or `states`, which contains the state names (which you can get from the `row.names` of `res`), and a column named `percent` with the percentage of drunk driving accidents in the associated state. Once complete, generate a map using the code below.

```{r, eval=F}
library(usmap)
library(ggplot2)
plot_usmap(data = myDF, values = "percent", color = "black") + 
    scale_fill_continuous(low = "white", high = "#C28E0E", 
                          name = "Drunk driving accidents (%)", 
                          label = scales::percent) + 
    theme(legend.position = "right")
```

**Relevant topics:** [data.frame](#r-data-frames), [tapply](#r-tapply)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The resulting plot.
```

---

### Project 7 {#p07-190}

---

**Motivation:** Three bread-and-butter functions that are a part of the base R are: `subset`, `merge`, and `split`. `subset` provides a more natural way to filter and select data from a data.frame. `split` is a useful function that splits a dataset based on one or more factors. `merge` brings the principals of combining data that SQL uses, to R.

**Context:** We've been getting comfortable working with data in within the R environment. Now we are going to expand our toolset with three useful functions, all the while gaining experience and practice wrangling data!

**Scope:** r, subset, merge, split, tapply

**Learning objectives:**

```{block, type="bbox"}
- Gain proficiency using split, merge, and subset.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Demonstrate how to use tapply to solve data-driven problems.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/goodreads/csv`

#### Questions

##### 1. Load up the following three datasets `goodreads_books.csv`, `goodreads_book_authors.csv`, and `goodreads_interactions.csv` into three data.frames `books`, `authors`, and `interactions` respectively. Read in only 1 million rows of the `goodreads_interactions.csv`. How many columns and rows are in each dataset?

**Relevant topics:** [read.csv](#r-read), [dim](#r-dim)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 2. We want to figure out how book size (`num_pages`) is associated with various metrics. First, let's create a vector called `book_size`, that categorizes books into 4 categories based on `num_pages`: `small` (up to 250 pages), `medium` (250-500 pages), `large` (500-1000 pages), `huge` (1000+ pages). 

**Relevant topics:** [cut](#cut)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of `table(book_size)`.
```

##### 3. Use `tapply` to calculate the mean `average_rating`, `text_reviews_count`, and `publication_year` by `book_size`. Did any of the result surprise you? Why or why not?

**Relevant topics:** [tapply](#r-tapply)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The output from running the R code.
```

##### 4. Notice in (3) every time we used `tapply` we were re-splitting the data each time. Use `split` to partition the data containing only the following 3 columns: `average_rating`, `text_reviews_count`, and `publication_year`, by `book_size`. Save the result as `books_by_size`. What class is the result? `lapply` is a function that allows you to loop over each item in a list and apply a function. Use `lapply` and `colMeans` to perform the same calculation as in (3).

**Relevant topics:** [lapply](#r-lapply), [split](#r-writing-functions), [colMeans](#r-means), [indexing](#r-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The copy and pasted output from running the code.
```

##### 5. We are working with a lot more data than we really want right now. You were provided with the following code to filter out non-English books and only keep columns of interest. Write out the equivalent code using `subset` instead of indexing, and save the result to `res`. Do the dimensions (using `dim`) of the `subset` version and the version below match? Why or why not?

```{r, eval=F}
en_books <- books[books$language_code %in% c("en-US", "en-CA", "en-GB", "eng", "en", "en-IN") & books$publication_year > 2000, c("author_id", "book_id", "average_rating", "description", "title", "ratings_count", "language_code", "publication_year")]
```

**Hint:** If the dimensions don't match, take a look at NA values for the variables used to subset our data.

**Relevant topics:** [indexing](#r-indexing), [subset](#r-subset), [NA](#r-na), %in%

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Do the dimensions match?
- 1-2 sentences explaining why or why not.
```

##### 6. We now have a nice and tidy subset of data, `res`. It would be really nice to get some information on the author (especially the name!). We can find that information in `authors`! In the previous project, we had a similar issue with the states names (in question 2). There is a *much* better way to solve these types of problems. Use the `merge` function to combine `res` and `authors` in a way which appends all information from `author` when there is a match in `res`. 

**Relevant topics:** [merge](#r-merge)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The `dim` of the newly merged data.frame.
```

##### 7. Look at the `names` of the resulting data.frame. Notice that there are two values for `ratings_count` and two values for `average_rating`. The names that have an appended `x` are those values from the first argument to `merge`, and the names that have an appended `y`, are those values from the second argument to `merge`. Rename these columns to indicate if they refer to a book, or an author. 

**Hint:** For example, `ratings_count.x` could be `ratings_count_book` or `ratings_count_author`.

**Relevant topics:** [names](#r-names)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The `names` of the new data.frame.
```

##### 8. For an author of your choice (that _is_ in the dataset), find the author's highest rated book. Do you agree?

**Relevant topics:** [indexing](#r-indexing), [subset](#r-subset), [which](#r-which), [max](#r-max)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The title of the highest rated book (from your author).
- 1-2 sentences explaining why or why not you agree with it being the highest rated book from that author.
```

---

### Project 8 {#p08-190}

---

**Motivation:** A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code!

**Context:** We've been learning about and using functions all year! Now we are going to learn more about some of the terminology and components of a function, as you will certainly need to be able to write your own functions soon.

**Scope:** r, functions

**Learning objectives:**

```{block, type="bbox"}
- Gain proficiency using split, merge, and subset.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Demonstrate how to use tapply to solve data-driven problems.
- Comprehend what a function is, and the components of a function in R.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/goodreads/csv`

#### Questions

##### 1. Read in the same data, in the same way as the previous project (with the same names). We've provided you with the function below. How many arguments does the function have? Name all of the arguments. What is the name of the function? Replace the `description` column in our `books` data.frame with the same information, but with stripped punctuation using the function provided.

```{r}
# A function that, given a string (description), returns the string
# without any punctuation.
strip_punctuation <- function(description) {
    # Use regular expressions to identify punctuation.
    # Replace identified punctuation with an empty string ''.
    desc_no_punc <- gsub('[[:punct:]]+', '', description)
    # Return the result
    return(desc_no_punc)
}
```

**Hint:** Since `gsub` accepts a vector of values, you can pass an entire vector to `strip_punctuation`.

##### 2. Now its time to write your own function. We want to write a function that counts the words in a string. There are already functions that do this, however, we want to write our own. We plan to use this on our non-punctuated descriptions. Begin by using the `strsplit` function to split a string by spaces. An examples string is: `test_string <- "This is  a test string  with no punctuation"`. Use `test_string` to test out your code. If you counted the words shown in your results, would it be an accurate count? Why or why not?

**Relevant topics:** [strsplit](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- 1-2 sentences explaining why or why not your count would be accurate.
```

##### 3. Fix the issue in (3), using `which`. You may need to `unlist` the `strsplit` result first. After you've accomplished this, you can count the remaining words!

**Relevant topics:** [which](#r-which)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem (including counting the words).
```

##### 4. We are finally to the point where we have code from questions (2) and (3) that we think we may want to use many times. Write a function called `count_words` which, given a string, `description`, returns the number of words in `description`. Test out `count_words` on the `description` from the second row of `books`. How many words are in the description?

**Relevant topics:** [functions](#r-writing-functions), [unlist](#r-writing-functions), [indexing](#r-indexing), [strsplit](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of using the function on the `description` from the second row of `books`.
```

##### 5. Practice makes perfect! Write a function of your own design that is intended on being used with one of our datasets. Test it out and share the results.

Note: You could even pass (as an argument) one of our datasets to your function and calculate a cool statistic or something like that! Maybe your function makes a plot? Who knows?

**Relevant topics:** [functions](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- An example (with output) of using your newly created function.
```

---

### Project 9 {#p09-190}

---

**Motivation:** A key component to writing efficient code is writing functions. Functions allow us to repeat and reuse coding steps that we used previously, over and over again. If you find you are repeating code over and over, a function may be a good way to reduce lots of lines of code!

**Context:** We've been learning about and using functions all year! Now we are going to learn more about some of the terminology and components of a function, as you will certainly need to be able to write your own functions soon.

**Scope:** r, functions

**Learning objectives:**

```{block, type="bbox"}
- Gain proficiency using split, merge, and subset.
- Demonstrate the ability to use the following functions to solve data-driven problem(s): mean, var, table, cut, paste, rep, seq, sort, order, length, unique, etc.
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Demonstrate how to use tapply to solve data-driven problems.
- Comprehend what a function is, and the components of a function in R.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/goodreads/csv`

#### Questions

##### 1. We've provided you with a function below. How many arguments does the function have, and what are their names? You can get a `book_id` from the URL of a goodreads book's webpage. For example, the `book_id` from https://www.goodreads.com/book/show/17332218-words-of-radiance#, is 17332218. Another example is https://www.goodreads.com/book/show/157993.The_Little_Prince?from_search=true&from_srp=true&qid=JJGqUK9Vp9&rank=1, with a `book_id` of 157993. Find 2 or 3 `book_id`s and test out the function until you get a success or two. Explain in words, what the function is doing, and what options you have.

```{r, eval=F}
books <- read.csv("/class/datamine/data/goodreads/csv/goodreads_books.csv")
authors <- read.csv("/class/datamine/data/goodreads/csv/goodreads_book_authors.csv")
fun_plot <- function(book_id, display_cover=T) {
    library(imager)
    get_author_name <- function(author_id){
        return(authors[authors$author_id==author_id,'name'])
    }
    
    book_info <- books[books$book_id==book_id,]
    all_books_by_author <- books[books$author_id==book_info$author_id,]
    author_name <- get_author_name(book_info$author_id)
    
    img_url <- book_info$image_url
    img <- load.image(img_url)
    
    if(display_cover){
        par(mfrow=c(1,2))
        plot(img, axes=FALSE)
    }
    plot(all_books_by_author$num_pages, all_books_by_author$average_rating, ylim=c(0,5.1),
         pch=21, bg='grey80',
         xlab='Number of pages', ylab='Average rating', main=paste('Books by', author_name))
    points(book_info$num_pages, book_info$average_rating,pch=21, bg='orange', cex=1.5)
}
```

**Relevant topics:** [functions](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- How many arguments does the function have, and what are their names? 
- The result of using the function on 2-3 `book_id`s.
- 1-2 sentences explaining what the function does (generally), and what (if any) options the function provides you with.
```

##### 2. You may have encountered a situation where the `book_id` was not in our dataset, and hence, didn't get plotted. When writing functions, it is usually best to try and foresee issues like this and have the function fail gracefully, instead of showing some ugly (and sometimes unclear) warning. Add a check at the beginning of our function that checks for the `book_id`, and if it does not exist, prints "Book ID not found.", and exits the function. Test it out on `book_id=123`.

**Hint:** Run `?stop` to see if that is a function that may be useful.

**Relevant topics:** [functions](#r-writing-functions), [if/else](#r-if-else), stop

```{block, type="bbox"}
**Item(s) to submit:**

- R code with your new and improved function.
- The results from `fun_plot(123)`.
- The results from `fun_plot(19063)`.
```

##### 3. You may have noticed a function _inside_ our `fun_plot` function. It looks like it accepts an `author_id` and returns the name of the author. Try running `get_author_name(6252)`, does it work? Read [this](https://www.datamentor.io/r-programming/environment-scope/) and explain in 1-2 sentences what you think is happening.

**Relevant topics:** scoping

```{block, type="bbox"}
**Item(s) to submit:**

- The results from `get_author_name(6252)`.
- 1-2 sentences explaining what is happening.
```

##### 4. Our `fun_plot` requires that the datasets `books` and `authors` have been loaded exactly right (and with the correct names) in the environment. By including objects outside of our function's scope, within our function (in this case `books` and `authors`) it leaves our `fun_plot` function prone to errors, as any changes to those objects may break our function. Fix this by making the datasets (`books` and `authors`) arguments in the function, and modifying our function accordingly to run based on those arguments.

**Revelant topics:** [functions](#r-writing-functions), [read.csv](#r-read), scoping

```{block, type="bbox"}
**Item(s) to submit:**

- R code with your new and improved function.
- An example using the updated function.
```

##### 5. Write your own custom function. Make sure your function includes at least 2 arguments. If you access one of our datasets from within your function (which you _definitely_ should do), use what you learned in (4), to avoid future errors dealing with scoping. Your function could output a cool plot, interesting tidbits of information, or anything else you can think of. Get creative and make a function that is fun to use!

**Relevant topics:** scoping, [functions](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Examples using your function with included output.
```

---

### Project 10 {#p10-190}

---

**Motivation:** Functions are powerful. They are building blocks to more complex programs and behavior. In fact, there is an entire programming paradigm based on functions called [functional programming](https://en.wikipedia.org/wiki/Functional_programming). In this project, we will learn to _apply_ functions to entire vectors of data using `sapply`.

**Context:** We've just taken some time to learn about and create functions. One of the more common "next steps" after creating a function is to use it on a series of data, like a vector. `sapply` is one of the best ways to do this in R.

**Scope:** r, sapply, functions

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/okcupid/filtered`

#### Questions

##### 1. Load up the the following datasets into data.frames named `users` and `questions`, respectively: `/class/datamine/data/okcupid/filtered/users.csv`, `/class/datamine/okcupid/filtered/questions.csv`. This is data from users on OkCupid, on online dating app. In your own words, explain what each file contains and how they are related -- its _always_ a good idea to poke around the data to get a better understanding of how things are structured!

**Hint:** Be careful, just because a file ends in `.csv`, does _not_ mean it is comma-separated.

**Relevant topics:** [read.csv](#r-read)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- 1-2 sentences describing what each file contains and how they are related.
```

##### 2. `grep` is an incredibly powerful tool available to us in R. We will learn more about `grep` in the future, but for now, know that a simple application of `grep` is to find a word in a string. In R, `grep` is vectorized and can be applied to an entire vector of strings. Use `grep` to find a question that references "google". What is the question?

**Hint:** If at first you don't succeed, run `?grep` and check out the `ignore.case` argument.

**Relevant topics:** [grep](#r-grep)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The `text` of the question that references Google.
```

##### 3. In (2) we found a pretty interesting question. What is the percentage of users that Google someone before the first date? Does the proportion change by gender (as defined by `gender2`)? How about by `gender_orientation`?

**Hint:** If you look at the question column for this question, you should notice that this column is a `factor` with two possible answers: "No. Why spoil the mystery?" and "Yes. Knowledge is power!". If you start by creating a function that calculates the percentage of people who answer each question, you could use `tapply` in combination with this function to break the answer down by gender.

**Relevant topics:** [functions](#r-writing-functions), [tapply](#r-tapply), [table](#r-table), [prop.table](#r-table)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The results of running the code.
- Written answers to the questions.
```

##### 4. In project (8) we created a function called `count_words`. Use this function and `sapply` to create a vector with the length (in words) of the questions. Call the new column of data `question_length`, and add the column to our data.frame.

```{r, eval=F}
count_words <- function(description) {
    split_desc <- unlist(strsplit(description, " "))
    return(length(split_desc[which(split_desc != "")]))
}
```

**Hint:** `questions$text` is a `factor`. Use `as.character` to convert the factor to a character before passing it to `count_words`.

**Relevant topics:** [sapply](#r-sapply)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The result of `str(questions)`.
```

##### 5. Write a function called `number_of_options` that accepts the dataset, and a question key (for example `q484`) and counts the number of answer options that the question has. Although each question has 4 option columns, not every column is filled. Consider an option empty if it is NA or blank. What percentage of questions have 1, 2, 3, and 4 options? Add this data to a new column in our questions dataset called `number_options`.

**Hint:** Use `sapply` to apply your function to every id in the vector (`questions$X`).

**Hint:** The way `sapply` works is the the first argument is by default the first argument to your function, the second argument is the function you want applied, and after that you can specify arguments by name.

**Relevant topics:** [table](#r-table), [prop.table](#r-table), [sapply](#r-sapply), [functions](#r-writing-functions), [if/else](#r-if-else), [indexing](#r-indexing), [is.na](#r-isna)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The results of the running the code.
```

##### 6. Does it appear that there is an association between the length of the question and whether or not users answered the question? Assume NA means "unanswered". First create a function called `percent_answered` that, given a vector, returns the percentage of values that are not NA. Use `percent_answered` and `sapply` to calculate the percentage of users who answer each question. Plot this result, against the length of the questions. 

**Hint:** `length_of_questions <- questions$question_length[grep("^q", questions$X)]`

**Hint:** Use the same trick we used in the previous hint, to subset our `users` data.frame before using `sapply` to apply `percent_answered`. `grep("^q", questions$X)` returns the column index of every column that starts with "q".

**Relevant topics:** [sapply](#r-sapply), [is.na](#r-isna), [length](#r-length), [grep](#r-grep), [plot](#r-plot)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The plot.
- Whether or not you think there may or may not be an association between question length and whether or not the question is answered. 
```

##### 7. _Lots_ of questions are asked in this dataset. Explore the dataset, and either calculate an interesting statistic/result using `sapply`, or generate a graphic (with good x-axis and/or y-axis labels, main labels, legends, etc.), or both! Write 1-2 sentences about your analysis and/or graphic, and explain what you thought you'd find, and what you actually discovered.

**Relevant topics:** [plotting](#r-plotting), [functions](#r-writing-functions), [sapply](#r-sapply)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve this problem.
- The results from running your code.
- 1-2 sentences about your analysis and/or graphic, and explain what you thought you'd find, and what you actually discovered.
```

---

### Project 11 {#p11-190}

---

**Motivation:** The ability to understand a problem, know what tools are available to you, and select the right tools to get the job done, takes practice. In this project we will use what you've learned so far this semester to solve data-driven problems. In previous projects, we've directed you towards certain tools. In this project, there will be less direction, and you will have the freedom to choose the tools you'd like.

**Context:** You've learned lots this semester about the R environment. You now have experience using a very balanced "portfolio" of R tools. We will practice using these tools on a set of economic data from Zillow.

**Scope:** r

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
- Comprehend what a function is, and the components of a function in R.
- Demonstrate the ability to use nested apply functions to solve a data-driven problem.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/zillow`

#### Questions

##### 1. Read `/class/datamine/data/zillow/Zip_time_series.csv` into a data.frame called `zipc`. Look at the `RegionName` column. It is supposed to be a 5-digit zip code. Either fix the column by writing a function and applying it to the column, or take the time to read the `read.csv` documentation by running `?read.csv` and use an argument to make sure that column is not read in as an integer (which is _why_ zip codes starting with `0` lose the leading `0` when being read in).

**Relevant topics:** [read.csv](#read), [sapply](#r-sapply), [functions](#r-writing-functions), strrep, nchar

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- `head` of the `RegionName` column.
```

##### 2. One might assume that the owner of a house tends to value that house more than the buyer. If that was the case, perhaps the median listing price (the price which the seller puts the house on the market, or ask price) would be higher than the ZHVI (Zillow Home Value Index -- essentially an estimate of the home value). For those rows where both `MedianListingPrice_AllHomes` and `ZHVI_AllHomes` have non-NA values, on average how much higher or lower is the median listing price? Can you think of any other reasons why this may be?

**Relevant topics:** [mean](#r-mean)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result itself and 1-2 sentences talking about whether or not you can think of any other reasons that may explain the result.
```

##### 3. Convert the `Date` column to a date using `as.Date`. How many years of data do we have in this dataset? Create a line plot with lines for the average `MedianListingPrice_AllHomes` and average `ZHVI_AllHomes` by year.

**Hint:** For a nice addition, add a dotted vertical line on year 2008 near the housing crisis:

```{r, eval=F}
abline(v="2008", lty="dotted")
```

**Relevant topics:** [cut](#cut), [as.Date](#r-dates), [tapply](#r-tapply), [plot](#r-plotting), [lines](#r-lines), [legend](#r-legend)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The results of running the code.
```

##### 4. Read `/class/datamine/data/zillow/State_time_series.csv` into a data.frame called `states`. Calculate the average median listing price by state, and create a map using `plot_usmap` from the `usmaps` package that shows the average median price by state.

**Hint:** Look at the solution to question 6 in project 6 for an example using `plot_usmap`. You can change `scales::percent` to `scales::dollar` when dealing with dollar data.

**Hint:** In order for `plot_usmap` to work, you must name the column containing states' names to "state".

**Hint:** To split words like "OhSoCool" into "Oh So Cool", try this: `trimws(gsub('([[:upper:]])', ' \\1', "OhSoCool"))`. This will be useful as you'll need to correct the `RegionName` column at some point in time. Notice that this will not completely fix "DistrictofColumbia". You will need to fix that one manually.

##### 5. Read `/class/datamine/data/zillow/County_time_series.csv` into a data.frame named `counties`. Choose a state (or states) that you would like to "dig down" into county-level data for, and create a plot (or plots) like in (4) that show some interesting statistic by county. You can choose average median listing price if you so desire, however, you don't need to! There are other cool data! 

**Hint:** Make sure that you remember to aggregate your data by date so the plot renders correctly.

**Hint:** `plot_usmap` looks for a column named `fips`. Make sure to rename the `RegionName` column to `fips` prior to passing the data.frame to `plot_usmap`.

---

### Project 12 {#p12-190}

---

**Motivation:** In the previous project you were forced to do a little bit of date manipulation. Dates _can_ be very difficult to work with, regardless of the language you are using. `lubridate` is a package within the famous [tidyverse](https://www.tidyverse.org/), that greatly simplifies some of the most common tasks one needs to perform with date data.

**Context:** We've been reviewing topics learned this semester. In this project we will continue solving data-driven problems, wrangling data, and creating graphics. We will introduce a [tidyverse](https://www.tidyverse.org/) package that adds great stand-alone value when working with dates.

**Scope:** r

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
- Demostrate the ability to create basic graphs with default settings.
- Demonstratre the ability to modify axes labels and titles.
- Incorporate legends using legend().
- Demonstrate the ability to customize a plot (color, shape/linetype).
- Convert strings to dates, and format dates using the lubridate package.
```

#### Questions

##### 1. Let's continue our exploration of the Zillow time series data. A useful package for dealing with dates is called `lubridate`. This is part of the famous [tidyverse](https://www.tidyverse.org/) suite of packages. Run the code below to load it. Read the `/class/datamine/data/zillow/State_time_series.csv` dataset into a data.frame named `states`. What class and type is the column `Date`?

```{r}
library(lubridate)
```

**Relevant topics:** [class](#r-lists-and-vectors), [typeof](#r-type)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- `class` and `typeof` column `Date`.
```

##### 2. Convert column `Date` to a corresponding date format using `lubridate`. Check that you correctly transformed it by checking its class like we did in (1). Compare and contrast this method of conversion with the solution you came up with for question (3) in the previous project. Which method do you prefer?

**Hint:** Take a look at the following functions from `lubridate`: `ymd`, `mdy`, `dym`.

**Relevant topics:** [dates](#r-dates), [lubridate](#r-lubridate)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- `class` of modified column `Date`.
- 1-2 sentences stating which method you prefer (if any) and why.
```

##### 3. Create 3 new columns in `state` called `year`, `month`, `day_of_week` (Sun-Sat) using `lubridate`. Get the frequency table for your newly created columns. Do we have the same amount of data for all years, for all months, and for all days of the week? We did something similar in question (3) in the previous project -- specifically, we broke each date down by year. Which method do you prefer and why?

**Hint:** Take a look at functions `month`, `year`, `day`, `wday`.

**Hint:** You may find the argument of `label` in `wday` useful.

**Relevant topics:** [dates](#r-dates), [lubridate](#r-lubridate)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- Frequency table for newly created columns.
- 1-2 sentences answering whether or not we have the same amount of data for all years, months, and days of the week.
- 1-2 sentences stating which method you prefer (if any) and why.
```

##### 4. Is there a better month or set of months to put your house on the market? Use `tapply` to compare the average `DaysOnZillow_AllHomes` for all months. Make a barplot showing our results. Make sure your barplot includes "all of the fixings" (title, labeled axes, legend if necessary, etc. Make it look good.).

**Relevant topics:** [tapply](#r-tapply), [barplot](#r-barplot)

**Hint:** If you want to have the month's abbreviation in your plot, you may find both the `month.abb` object and the argument `names.arg` in `barplot` useful.

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- The barplot of `DaysOnZillow_AllHomes` for all months.
- 1-2 sentences answering the question "Is there a better time to put your house on the market?" based on your results.
```

##### 5. Filter the `states` data to contain only years from 2010+ and called it `states2010plus`. Make a lineplot showing the average `DaysOnZillow_AllHomes` by `Date` using `states2008plus` data. Can you spot any trends? Write 1-2 sentences explaining what (if any) trends you see.

**Relevant topics:** [subset](#r-subset), [tapply](#r-tapply), [plot](#r-plot)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- The time series lineplot for `DaysOnZillow_AllHomes` per date.
- 1-2 sentences commenting on the patterns found in the plot, and your impressions of it.
```

##### 6. Do homes sell faster in certain states? For the the following states: 'California', 'Indiana', 'NewYork' and 'Florida', make a lineplot for `DaysOnZillow_AllHomes` by `Date` with one line per state. Make sure to use `states2010plus` dataset. Make sure to have each state line colored differently, and to add a legend to your plot. Examine the plot and write 1-2 sentences about any observations you have.

**Hint:** You may want to use the `lines` function to add the lines for different state.

**Hint:** Make sure to fix the y-axis limits using the `ylim` argument in `plot` to properly show all four lines.

**Hint:** You may find the argument `col` useful to change the color of your line.

**Hint:** To make your legend fit, consider using the states abbreviation, and the arguments `ncol` and `cex` of the `legend` function.

**Relevant topics:** [subset](#r-subset), [indexing](#r-indexing), [plot](#r-plot), [lines](#r-lines)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- The time series lineplot for `DaysOnZillow_AllHomes` per date for the 4 states.
- 1-2 sentences commenting on the patterns found in the plot, and your answer to the question "Do homes sell faster than in certain states rather than others?".
```

---

### Project 13 {#p13-190}

---

**Motivation:** Its important to be able to lookup and understand the documentation of a new function. You may have looked up the documentation of functions like `paste0` or `sapply`, and noticed that in the "usage" section, one of the arguments is an ellipsis (`...`). Well, unless you understand what this does, its hard to really _get_ it. In this project, we will experiment with ellipsis, and write our own function that utilizes one.

**Context:** We've learned about, used, and written functions in many projects this semester. In this project, we will utilize some of the less-known features of functions.

**Scope:** r, functions

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
- Demostrate the ability to create basic graphs with default settings.
- Demonstratre the ability to modify axes labels and titles.
- Incorporate legends using legend().
- Demonstrate the ability to customize a plot (color, shape/linetype).
- Convert strings to dates, and format dates using the lubridate package.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/beer/`

#### Questions

##### 1. Read `/class/datamine/data/beer/beers.csv` into a data.frame named `beers`. Read `/class/datamine/data/beer/breweries.csv` into a data.frame named `breweries`. Read `/class/datamine/data/beer/reviews.csv` into a data.frame named `reviews`. 

**Hint:** Wow! `reviews.csv` is a _large_ file. Luckily, we will now introduce a function that is part of the famous `data.table` package called `fread`. `fread` is _much_ faster than `read.csv`. It reads the data into a class called `data.table`. We will learn more about this later on. For now, convert the `data.table` into a `data.frame` by wrapping the result of `fread` in the `data.frame` function.

```{r, eval=F}
microbenchmark(read.csv("/class/datamine/data/beer/reviews.csv", nrows=100000), data.frame(fread("/class/datamine/data/beer/reviews.csv", nrows=100000)), times=5)
```

```{txt}
Unit: milliseconds
expr
read.csv("/class/datamine/data/beer/reviews.csv", nrows = 1e+05)
data.frame(fread("/class/datamine/data/beer/reviews.csv", nrows = 1e+05))
       min        lq      mean    median        uq       max neval
 5948.6289 6482.3395 6746.8976 7040.5881 7086.6728 7176.2589     5
  120.7705  122.3812  127.9842  128.7794  133.7695  134.2205     5
```

**Relevant topics:** fread, [data.frame](#r-data-frames)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
```

##### 2. Take some time to explore the datasets. Like many datasets, our data is broken into 3 "tables". What columns connect each table? How many breweries in `breweries` don't have an associated beer in `beers`? How many beers in `beers` don't have an associated brewery in `breweries`?

**Relevant topics:** [names](#r-names), %in%, [logical operators](#r-logical-operators), [unique](#r-unique)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- A description of columns which connect each of the files.
- How many breweries don't have an associated beer in `beers`.
- How many beers don't have an associated brewery in `breweries`.
```

##### 3. Run `?sapply` and look at the usage section for `sapply`. If you look at the description for the `...` argument, you'll see it is "optional arguments to `FUN`". What this means is you can specify additional input for the function you are passing to `sapply`. One example would be passing `T` to `na.rm` in the mean function: `sapply(dat, mean, na.rm=T)`. Use `sapply` and the `strsplit` function to separate the types of breweries (`types`) by commas. Use another `sapply` to loop through your results and count the number of types for each brewery. Be sure to name your final results `n_types`. What is the average amount of services (`n_types`) breweries in IN and MI offer? Does that surprise you?

**Note:** When you have one `sapply` inside of another, or one loop inside of another, or an if/else statement inside of another, this is commonly referred to as nesting. So when Googling, you can type "nested sapply" or "nested if statements", etc.

**Relevant topics:** [...](#dots), [sapply](#r-sapply), [strplit](#r-writing-functions), %in%, [mean](#r-mean)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- 1-2 sentences answering the average amount of services breweries in Indiana and Michigan offer, and commenting on this answer.
```

##### 4. Write a function called `compare_beers` that accepts a function `FUN`, and any number of vectors of beer ids. `compare_beers` should cycle through each group of `beer_id`s, compute `FUN` on the subset of reviews, and print "Group X: some_score" where X is the number 1+, and some_score is the result of applying `FUN` on the subset of data.

Example:
```{r, eval=F}
compare_beers(reviews, median, c(271781), c(125646, 82352))
```

Fake output:
```{txt}
Group 1: 16
Group 2: 2.3
```

**Relevant topics:** [...](#r-dots), %in%, [indexing](#r-indexing), [paste0](#r-paste), [for loops](#r-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result from running the provided example.
```

##### 5. Beer wars! IN and MI against AZ and CO. Use the function in (4) to compare beer_id from each group of states. Make a cool plot of some sort. Be sure to comment on your plot.

**Hint:** Create a vector of `beer_ids` per group before passing it to your function from (3).

**Relevant topics:** [...](#dots), %in%, [indexing](#r-indexing), [paste0](#r-paste), [for loops](#r-for-loops)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result from running the your function.
- The resulting plot.
- 1-2 sentecens commenting on your plot.
```

---

### Project 14 {#p14-190}

---

**Motivation:** Functions are the building blocks of more complex programming. It's vital that you understand how to read and write functions. In this project we will incrementally build and improve upon a function designed to recommend a beer. Note that you will not be winning any awards for this recommendation system, it is just for fun!

**Context:** One of the main focuses throughout the semester has been on functions, and for good reason. In this project we will continue to exercise our R skills and build up our recommender function.

**Scope:** r, functions

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/beer/`

#### Questions

##### 1. Read `/class/datamine/data/beer/beers.csv` into a data.frame named `beers`. Read `/class/datamine/data/beer/breweries.csv` into a data.frame named `breweries`. Read `/class/datamine/data/beer/reviews.csv` into a data.frame named `reviews`. As in the previous project, make sure you used the `fread` function from `data.table` package, and convert the `data.table` to a `data.frame`. We want to create a very basic beer recommender. We will start simple. Create a function called `recommend_a_beer` that takes as input `my_beer_id` (a single value) and returns a vector of `beer_ids` from the same `style`. Test your function on `2093`.

**Hint:** Make sure you do not include the given `my_beer_id` from your recommended beer vector.

**Hint:** You may find the function `setdiff` useful. Run the example below to get an idea of what it does.

**Note:** You will not win any awards for this recommendation system!

```{r}
x <- c('a','b','b','c')
y <- c('c','b','d','e','f')
setdiff(x,y)
setdiff(y,x)
```

**Relevant topics:** fread, [data.frame](#r-data-frames), [function](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Length of result from `recommend_a_beer(2093)`.
- The result of `2093 %in% recommend_a_beer(2093)`.
```

##### 2. That is a lot of beer recommendations! Let's try to narrow it down. Include an argument in your function called `min_score` with default value of 4.5. Our recommender will only recommend `beer_ids` with at least one score of at least `min_score`. Test your improved beer recommender with the same `beer_id` from (1).

**Hint:** Note that now we need to look at both `beers` and `reviews` datasets.

**Relevant topics:** %in%, [unique](#r-unique), [subset](#r-subset)/[indexing](#r-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Length of result from `recommend_a_beer(2093)`.
```

##### 3. There is still room for improvement (obviously) for our beer recommender. Include a new argument in your function called `same_brewery_only` with default value `FALSE`. This argument will determine whether or not our beer recommender will return only beers from the same brewery. Test our newly improved beer recommender with the same `beer_id` from (1) with `same_brewery_only` set as `TRUE`.

**Hint:** You may find the function `intersect` useful. Run the example below to get an idea of what it does.

```{r}
x <- c('a','b','b','c')
y <- c('c','b','d','e','f')
intersect(x,y)
intersect(y,x)
```

**Relevant topics:** [if/else](#r-if-else), [subset](#r-subset), intersect, [indexing](#r-indexing)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Length of result from `recommend_a_beer(2093, same_brewery_only=TRUE)`.
```

##### 4. Oops! Bad idea! Maybe including only beers from the same brewery is not the best option. Add an argument to our beer recommender named `type`. If `type=style` our recommender will recommend beers based on the `style` as we did in (3). If `type=reviewers`, our recommender will recommend beers based on reviewers with "similar taste". Select reviewers that have a `min_score` for the given beer id (`my_beer_id`). For those reviewers, find the `beer_ids` for other beers that these reviewers have given a score of at least `min_score`. These `beer_ids` are the ones our recommender will return. Be sure to test our improved recommender on the same `beer_id` as in (1)-(3).

**Relevant topics:** [if](#r-if-else), [subset](#r-subset), %in%, setdiff, [unique](#r-unique)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Length of result from `recommend_a_beer(2093, type="reviewers")`.
```

##### 5. Let's try to narrow down the recommendations. Include an argument called `abv_range` that indicates the abv range we would like the recommended beers to be at. Set `abv_range` default value to `NULL` so that if a user does not specify the `abv_range` our recommender does not consider it. Test our recommender for `beer_id` 2093, with `abv_range = c(8.9,9.1)` and `min_score=4.9`.

**Hint:** You may find the function `is.null` useful.

**Relevant topics:** [if](#r-if-else), [>=](#r-logical-operators), [<=](#r-logical-operators), intersect

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Length of result from `recommend_a_beer(2093, abv_range=c(8.9, 9.1), type="reviewers", min_score=4.9)`.
```

##### 6. Play with our `recommend_a_beer` function. Include another feature to it. Some ideas are: putting a limit on the number of `beer_id`s we will return, error catching (what if we don't have reviews for a given `beer_id`?), including a plot to the output, returning beer names instead of ids or new arguments to decide what `beer_id`s to recommend. Be creative and have fun!

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result from running the improved `recommend_a_beer` function showcasing your improvements to it.
- 1-2 sentecens commenting on what you decided to include and why.
```

---

### Project 15 {#p15-190}

---

**Motivation:** Some people say it takes 20 hours to learn a skill, some say 10,000 hours. What is certain is it definitely takes time. In this project we will explore an interesting dataset and exercise some of the skills learned the semester.

**Context:** This is the final project of the semester. We sincerely hope that you've learned something, and, if you haven't, we hope we've provided you with first hand experience digging through data.

**Scope:** r

**Learning objectives:**

```{block, type="bbox"}
- Read and write basic (csv) data.
- Explain and demonstrate: positional, named, and logical indexing.
- Utilize apply functions in order to solve a data-driven problem.
- Gain proficiency using split, merge, and subset.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/donerschoose/`

#### Questions

##### 1. Read the data `/class/datamine/data/donerschoose/Projects.csv` into a data.frame called `projects`. Make sure you use the function you learned in Project 13 -- `fread` function from `data.table` package -- to read the data. Don't forget to then convert the `data.table` into a `data.frame`. Let's do an initial exploration of this data. What types of projects (`Project.Type`) are there? How many resource categories (`Project.Resource.Category`) are there?

**Hint:** If a column name has a space in it, surround the name in backticks \` to access it. See the example below. Note that you should convert your `data.table` to a `data.frame`, and as a result, the column names should not have spaces.

```{r, eval=F}
projects$`Project Type`
```

**Relevant topics:** fread, [unique](#r-unique), [length](#r-length)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- 1-2 sentences containing the project's types and how many resource categories are in the dataset.
```

##### 2. Create two new variables in `projects`, the number of days a project's lasted and the number of days until the project was fully funded. Name those variables `project_duration` and `time_until_funded`, respectively. To calculate them use the project's posted date (`Project.Posted.Date`), expiration date (`Project.Expiration.Date`), and fully funded date (`Project.Fully.Funded.Date`). What are the shortest and longest times until a project is fully funded? For consistency check, see if we have any negative project's duration. If so, how many?

**Hint:** You _may_ find the argument `units` in `difftime` useful.

**Hint:** Be sure to pay attention to the order of operations of `difftime`.

**Hint:** Note that if you used `fread` function from `data.table` you will not need to convert the columns as date.

**Hint:** It is _not_ required that you use `difftime`.

**Relevant topics:** difftime, [lubridate](#r-lubridate)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- Shortest and longest times until a project is fully funded.
- 1-2 sentences answering whether we have if we have negative project's duration, and if so how many.
```

##### 3. As you noted in (2) there may be some project's with negative duration time. As we may have some concerns for the data regarding these projects, filter the `projects` data to exclude the projects with negative duration, and call this filtered data `selected_projects`. With that filtered data, make a `dotchart` for mean time until the project is fully funded (`time_until_funded`) for the various resource categories (`Project.Resource.Category`). Make sure to comment on your results. Are they surprising? Could there be another variable influencing this result? If so, name at least one.

**Hint:** You will first need to average the time until project your for the different categories before making your plot.

**Hint:** To make your `dotchart` look nicer, you may want to first order the average time until fully funded before passing it to `dotchart.` In addition, consider reducing the y-axis font size using the argument `cex`.

**Relevant topics:** [indexing](#r-indexing), [subset](#r-subset), [tapply](#r-tapply), [dotchart](#r-dotchart)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- Resulting barplot.
- 1-2 sentences commenting on your plot. Make sure to mention whether you are surprised or not by the results. Don't forget to add if you think there could be more factors influencing your answer, and if so, be sure to give examples.
```

##### 4. Read `/class/datamine/data/donerschoose/Schools.csv` into a data.frame called `schools`. Combine `selected_projects` and `schools` by `School.ID` keeping only `School.ID`s present in both datasets. Name the combined data.frame `selected_projects`. Use the newly combined data to determine the percentage of already fully funded projects (`Project.Current.Status`) for schools in West Lafayette, IN. In addition, determine the state (`School.State`) with the highest number of projects. Be sure to specify the number of projects this state has.

**Hint:** West Lafayette, IN zip codes are 47906 and 47907.

**Relevant topics:** fread, [read.csv](#r-read), [subset](#r-subset), [indexing](#r-indexing), [merge](#r-merge), [table](#r-table), [prop.table](#r-table), [which.max](#r-which)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- 1-2 sentences answering the percentage of already fully funded projects for schools in West Lafayette, IN, the state with the highest number of projects, and the number of projects this state has.
```

##### 5. Using the combined `selected_projects` data, get the school(s) (`School.Name`), city/cities (`School.City`) and state(s) (`School.State`) for the teacher with the highest percentage of fully funded projects (`Project.Current.Status`).

**Hint:** There are many ways to solve this problem. For example, one option to get the teacher's ID is to create a variable indicating whether or not the project is fully funded and use `tapply`. Another option is to create `prop.table` and select the corresponding column/row.

**Hint:** Note that each row in the data corresponds to a unique project ID.

**Hint:** Once you have the teacher's ID, consider filtering `projects` to contain only rows for which the corresponding teacher's ID is in, and only the columns we are interested in: `School.Name`, `School.City`, and `School.State`. Then, you can get the unique values in this shortened data.

**Hint:** To get only certain columns when subetting, you may find the argument `select` from `subset` useful.

**Relevant topics:** [indexing](#r-indexing), [which](#r-which), [max](#r-max), [subset](#r-subset), [unique](#r-unique), [row.names](#r-data-frames) (if using `table`), [names](#r-names) (if using `tapply`)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the question.
- 1-2 sentences answering the percentage of already fully funded projects for schools in West Lafayette, IN, the state with the highest amount of projects, and the number of projects this state has.
```

---

## STAT 29000

### Project 1 {#p01-290}

---

**Motivation:** In this project we will jump right into an R review. In this project we are going to break one larger data-wrangling problem into discrete parts. There is a slight emphasis on writing functions and dealing with strings. At the end of this project we will have greatly simplified a dataset, making it easy to dig into.

**Context:** We just started the semester and are digging into a large dataset, and in doing so, reviewing R concepts we've previously learned.

**Scope:** data wrangling in R, functions

**Learning objectives:**

```{block, type="bbox"}
- Comprehend what a function is, and the components of a function in R.
- Read and write basic (csv) data.
- Utilize apply functions in order to solve a data-driven problem.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

**Important note:** It is highly recommended that you use https://rstudio.scholar.rcac.purdue.edu/. Simply click on the link and login using your Purdue account credentials.

We decided to move away from ThinLinc and away from the version of RStudio used last year (https://desktop.scholar.rcac.purdue.edu).  That version of RStudio is known to have some strange issues when running code chunks.

Remember the very useful documentation shortcut `?`. To use, simply type `?` in the console, followed by the name of the function you are interested in. 

You can also look for package documentation by using `help(package=PACKAGENAME)`, so for example, to see the documentation for the package `ggplot2`, we could run:

```{r, eval=F}
help(package=ggplot2)
```

Sometimes it can be helpful to see the source code of a defined function. A [function](https://www.tutorialspoint.com/r/r_functions.htm) is any chunk of organized code that is used to perform an operation. Source code is the underlying `R` or `c` or `c++` code that is used to create the function. To see the source code of a defined function, type the function's name without the `()`. For example, if we were curious about what the function `Reduce` does, we could run:

```{r, eval=F}
Reduce
```

Occasionally this will be less useful as the resulting code will be code that calls `c` code we can't see. Other times it will allow you to understand the function better.

#### Dataset: 

`/class/datamine/data/airbnb`

Often times (maybe even the majority of the time) data doesnt come in one nice file or database. Explore the datasets in `/class/datamine/data/airbnb`.

##### 1. You may have noted that, for each country, city, and date we can find 3 files: `calendar.csv.gz`, `listings.csv.gz`, and `reviews.csv.gz` (for now, we will ignore all files in the "visualisations" folders).

##### Let's take a look at the data in each of the three types of files. Pick a country, city and date, and read the first 50 rows of each of the 3 datasets (`calendar.csv.gz`, `listings.csv.gz`, and `reviews.csv.gz`). Provide 1-2 sentences explaining the type of information found in each, and what variable(s) could be used to join them. 

**Hint:** `read.csv` has an argument to select the number of rows we want to read.

**Hint:** Depending on the country that you pick, the listings and/or the reviews might not display properly in RMarkdown.  So you do not need to display the first 50 rows of the listings and/or reviews, in your RMarkdown document.  It is OK to just display the first 50 rows of the calendar entries.

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code used to read the first 50 rows of each dataset.
- 1-2 sentences briefly describing the information contained in each dataset.
- Name(s) of variable(s) that could be used to join them.
```

To read a compressed csv, simply use the `read.csv` function:

```{r, eval=F}
dat <- read.csv("/class/datamine/data/airbnb/brazil/rj/rio-de-janeiro/2019-06-19/data/calendar.csv.gz")
head(dat)
```

Let's work towards getting this data into an easier format to analyze. From now on, we will focus on the `listings.csv.gz` datasets.


##### 2. Write a function called `get_paths_for_country`, that, given a string with the country name, returns a vector with the full paths for all `listings.csv.gz` files, starting with `/class/datamine/data/airbnb/...`.

##### For example, the output from `get_paths_for_country("united-states")` should have 28 entries.  Here are the first 5 entries in the output:

```{txt}
 [1] "/class/datamine/data/airbnb/united-states/ca/los-angeles/2019-07-08/data/listings.csv.gz"       
 [2] "/class/datamine/data/airbnb/united-states/ca/oakland/2019-07-13/data/listings.csv.gz"           
 [3] "/class/datamine/data/airbnb/united-states/ca/pacific-grove/2019-07-01/data/listings.csv.gz"     
 [4] "/class/datamine/data/airbnb/united-states/ca/san-diego/2019-07-14/data/listings.csv.gz"         
 [5] "/class/datamine/data/airbnb/united-states/ca/san-francisco/2019-07-08/data/listings.csv.gz"     
```

**Hint:** `list.files` is useful with the `recursive=T` option.

**Hint:** Use `grep` to search for the pattern `listings.csv.gz` (within the results from the first hint), and use the option `value=T` to display the values found by the `grep` function.

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `get_paths_for_country` function.
```


##### 3. Write a function called `get_data_for_country` that, given a string with the country name, returns a data.frame containing the all listings data for that country. Use your previously written function to help you. 

**Hint:** Use `stringsAsFactors=F` in the `read.csv` function.

**Hint:** Use `do.call(rbind, <listofdataframes>)` to combine a list of dataframes into a single dataframe.

**Relevant topics:** [rbind](#r-bind), [lapply](#r-lapply), [function](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `get_data_for_country` function.
```


##### 4. Use your `get_data_for_country` to get the data for a country of your choice, and make sure to name the data.frame `listings`. Take a look at the following columns: `host_is_superhost`, `host_has_profile_pic`, `host_identity_verified`, and  `is_location_exact`. What is the data type for each column?  (You can use `class` or `typeof` or `str` to see the data type.)

##### These columns would make more sense as logical values (TRUE/FALSE/NA).

##### Write a function called `transform_column` that, given a column containing lowercase "t"s and "f"s, your function will transform it to logical (TRUE/FALSE/NA) values. Note that NA values for these columns appear as blank (`""`), and we need to be careful when transforming the data. Test your function on column `host_is_superhost`.

**Relevant topics:** class, typeof, str, toupper, as.logical

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `transform_column` function.
- Type of `transform_column(listings$host_is_superhost)`.
```


##### 5. Apply your function `transform_column` to the columns `instant_bookable` and `is_location_exact` in your `listings` data.

##### Based on your `listings` data, if you are looking at an instant bookable listing (where `instant_bookable` is `TRUE`), would you expect the location to be exact (where `is_location_exact` is `TRUE`)? Why or why not?

**Hint:** Make a frequency table, and see how many instant bookable listings have exact location.

**Relevant topics:** [apply](#r-apply), [table](#r-table)

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code to get a frequency table.
- 1-2 sentences explaining whether or not we would expect the location to be exact if we were looking at a instant bookable listing.
```



---

### Project 2 {#p02-290}

---

**Motivation:** The ability to quickly reproduce an analysis is important. It is often necessary that other individuals will need to be able to understand and reproduce an analysis. This concept is so important there are classes solely on reproducible research! In fact, there are papers that investigate and highlight the lack of reproducibility in various fields. If you are interested in reading about this topic, a good place to start is the paper titled ["Why Most Published Research Findings Are False"](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124), by John Ioannidis (2005). 

**Context:** Making your work reproducible is extremely important. We will focus on the computational part of reproducibility. We will learn RMarkdown to document your analyses so others can easily understand and reproduce the computations that led to your conclusions. Pay close attention as future project templates will be RMarkdown templates.

**Scope:** Understand Markdown, RMarkdown, and how to use it to make your data analysis reproducible.

**Learning objectives:**

```{block, type="bbox"}
- Use Markdown syntax within an Rmarkdown document to achieve various text transformations.
- Use RMarkdown code chunks to display and/or run snippets of code.
```

#### Questions

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_8rsq5yrn&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_bjrv34ss" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

##### 1. Make the following text (including the asterisks) bold: `This needs to be **very** bold`. Make the following text (including the underscores) italicized: `This needs to be _very_ italicized.`

**Important note:** Surround your answer in 4 backticks. This will allow you to display the markdown _without_ having the markdown "take effect". For example:

`````markdown
````
Some *marked* **up** text.
````
`````

**Hint:** *Be sure to check out the [Rmarkdown Cheatsheet](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf) and our section on [Rmarkdown in the book](https://thedatamine.github.io/the-examples-book/r.html#r-rmarkdown).*

**Note:** *Rmarkdown is essentially Markdown + the ability to run and display code chunks. In this question, we are actually using Markdown within Rmarkdown!*

**Relevant topics:** [rmarkdown](#r-rmarkdown), [escaping characters](#escape-characters)

```{block, type="bbox"}
**Item(s) to submit:**
- 2 lines of markdown text, surrounded by 4 backticks. Note that when compiled, this text will be unmodified, regular text.
```

##### 2. Create an unordered list of your top 3 favorite academic interests (some examples could include: machine learning, operating systems, forensic accounting, etc.). Create another *ordered* list that ranks your academic interests in order of most interested to least interested.

**Hint:** *You can learn what ordered and unordered lists are [here](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).*

**Note:** *Similar to (1), in this question we are dealing with Markdown. If we were to copy and paste the solution to this problem in a Markdown editor, it would be the same result as when we Knit it here.*

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**
- Create the lists, this time don't surround your code in backticks. Note that when compiled, this text will appear as nice, formatted lists.
```

##### 3. Browse https://www.linkedin.com/ and read some profiles. Pay special attention to accounts with an "About" section. Write your own personal "About" section using Markdown. Include the following:

- A header for this section (your choice of size) that says "About".
- The text of your personal "About" section that you would feel comfortable uploading to linkedin, including at least 1 link.

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**
- Create the described profile, don't surround your code in backticks.
```

##### 4. Your co-worker wrote a report, and has asked you to beautify it. Knowing Rmarkdown, you agreed. Make improvements to this section. At a minimum:

- Make the title pronounced.
- Make all links appear as a word or words, rather than the long-form URL.
- Organize all code into code chunks where code and output are displayed. If the output is really long, just display the code.
- Make the calls to the `library` function be evaluated but not displayed. 
- Make sure all warnings and errors that may eventually occur, do not appear in the final document.

Feel free to make any other changes that make the report more visually pleasing.

````markdown
`r ''````{r my-load-packages}
library(ggplot2)
```

`r ''````{r declare-variable-290, eval=FALSE}
my_variable <- c(1,2,3)
```

All About the Iris Dataset

This paper goes into detail about the `iris` dataset that is built into r. You can find a list of built-in datasets by visiting https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html or by running the following code:

data()

The iris dataset has 5 columns. You can get the names of the columns by running the following code:

names(iris)

Alternatively, you could just run the following code:

iris

The second option provides more detail about the dataset.

According to https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html there is another dataset built-in to r called `iris3`. This dataset is 3 dimensional instead of 2 dimensional.

An iris is a really pretty flower. You can see a picture of one here:

https://www.gardenia.net/storage/app/public/guides/detail/83847060_mOptimized.jpg

In summary. I really like irises, and there is a dataset in r called `iris`.
````

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**

- Make improvements to this section, and place it all under the Question 4 header in your template.
```


##### 5. Create a plot using a built-in dataset like `iris`, `mtcars`, or `Titanic`, and display the plot using a code chunk. Make sure the code used to generate the plot is hidden. Include a descriptive caption for the image. Make sure to use an RMarkdown chunk option to create the caption.

**Relevant topics:** *[rmarkdown](#r-rmarkdown), [plotting in r](#r-plotting)*

```{block, type="bbox"}
**Item(s) to submit:**

- Code chunk under that creates and displays a plot using a built-in dataset like `iris`, `mtcars`, or `Titanic`.
```

##### 6. Insert the following code chunk under the Question 6 header in your template. Try knitting the document. Two things will go wrong. What is the first problem? What is the second problem?
       
````markdown
```{r my-load-packages}`r ''`
plot(my_variable)
```
````

**Hint:** *Take a close look at the name we give our code chunk.*

**Hint:** *Take a look at the code chunk where `my_variable` is declared.*

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**

- The modified version of the inserted code that fixes both problems.
- A sentence explaining what the first problem was.
- A sentence explaining what the second problem was.
```




##### For Project 2, please submit your .Rmd file and the resulting .pdf file.  (For this project, you do not need to submit a .R file.)




---

### Project 3 {#p03-290}

---

**Motivation:** The ability to navigate a shell, like `bash`, and use some of its powerful tools, is very useful. The number of disciplines utilizing data in new ways is ever-growing, and as such, it is very likely that many of you will eventually encounter a scenario where knowing your way around a terminal will be useful. We want to expose you to some of the most useful `bash` tools, help you navigate a filesystem, and even run `bash` tools from within an RMarkdown file in RStudio.

**Context:** At this point in time, you will each have varying levels of familiarity with Scholar. In this project we will learn how to use the terminal to navigate a UNIX-like system, experiment with various useful commands, and learn how to execute bash commands from within RStudio in an RMarkdown file.

**Scope:** bash, RStudio

**Learning objectives:**

```{block, type="bbox"}
- Distinguish differences in /home, /scratch, and /class.
- Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc.
- Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc.
- Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc.
- Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc.
- Use `man` to read and learn about UNIX utilities.
- Run `bash` commands from within and RMarkdown file in RStudio.
```

There are a variety of ways to connect to Scholar. In this class, we will _primarily_ connect to RStudio Server by opening a browser and navigating to https://rstudio.scholar.rcac.purdue.edu/, entering credentials, and using the excellent RStudio interface. 

Here is a video to remind you about some of the basic tools you can use in UNIX/Linux:

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_9mz5s0wd&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_0y4x1feo" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

This is the easiest book for learning this stuff; it is short and gets right to the point:

https://go.oreilly.com/purdue-university/library/view/-/0596002610

you just log in and you can see it all; we suggest Chapters 1, 3, 4, 5, 7 (you can basically skip chapters 2 and 6 the first time through).

It is a very short read (maybe, say, 2 or 3 hours altogether?), just a thin book that gets right to the details.

##### 1. Navigate to https://rstudio.scholar.rcac.purdue.edu/ and login. Take some time to click around and explore this tool. We will be writing and running Python, R, SQL, and `bash` all from within this interface. Navigate to `Tools > Global Options ...`. Explore this interface and make at least 2 modifications. List what you changed.

Here are some changes Kevin likes:

- Uncheck "Restore .Rdata into workspace at startup".
- Change tab width 4.
- Check "Soft-wrap R source files".
- Check "Highlight selected line".
- Check "Strip trailing horizontal whitespace when saving".
- Uncheck "Show margin".

(Dr Ward does not like to customize his own environment, but he does use the emacs key bindings: Tools > Global Options > Code > Keybindings, but this is only recommended if you already know emacs.)

```{block, type="bbox"}
**Item(s) to submit:**

- List of modifications you made to your Global Options.
```

##### 2. There are four primary panes, each with various tabs. In one of the panes there will be a tab labeled "Terminal". Click on that tab. This terminal by default will run a `bash` shell right within Scholar, the same as if you connected to Scholar using ThinLinc, and opened a terminal. Very convenient! 

##### What is the default directory of your bash shell? 

**Hint:** Start by reading the section on `man`. `man` stands for manual, and you can find the "official" documentation for the command by typing `man <command_of_interest>`. For example:

```{bash, eval=F}
# read the manual for the `man` command
# use "k" or the up arrow to scroll up, "j" or the down arrow to scroll down
man man 
```

**Relevant topics:** [man](#man), [pwd](#pwd), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- The full filepath of default directory (home directory). Ex: Kevin's is: `/home/kamstut`
- The `bash` code used to show your home directory or current directory (also known as the working directory) when the `bash` shell is first launched.
```

##### 3. Learning to navigate away from our home directory to other folders, and back again, is vital. Perform the following actions, in order:

- Write a single command to navigate to the folder containing our full datasets: `/class/datamine/data`.
- Write a command to confirm you are in the correct folder. 
- Write a command to list the files and directories within the data directory. (You do not need to recursively list subdirectories and files contained therein.) What are the names of the files and directories?
- Write another command to return back to your home directory. 
- Write a command to confirm you are in the correct folder.

Note: `/` is commonly referred to as the root directory in a linux/unix filesystem. Think of it as a folder that contains _every_ other folder in the computer. `/home` is a folder within the root directory. `/home/kamstut` is the full filepath of Kevin's home directory. There is a folder `home` inside the root directory. Inside `home` is another folder named `kamstut` which is Kevin's home directory. 

**Relevant topics:** [man](#man), [cd](#cd), [pwd](#pwd), [ls](#ls), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to the data directory.
- Command used to confirm you are in the data directory.
- Command used to list files and folders.
- List of files and folders in the data directory.
- Command used to navigate back to the home directory.
- Command used to confirm you are in the home directory.
```

##### 4. Let's learn about two more important concepts. `.` refers to the current working directory, or the directory displayed when you run `pwd`. Unlike `pwd` you can use this when navigating the filesystem! So, for example, if you wanted to see the contents of a file called `my_file.txt` that lives in `/home/kamstut` (so, a full path of `/home/kamstut/my_file.txt`), and you are currently in `/home/kamstut`, you could run: `cat ./my_file.txt`. 

##### `..` represents the parent folder or the folder in which your current folder is contained. So let's say I was in `/home/kamstut/projects/` and I wanted to get the contents of the file `/home/kamstut/my_file.txt`. You could do: `cat ../my_file.txt`. 

##### When you navigate a directory tree using `.` and  `..` you create paths that are called _relative_ paths because they are _relative_ to your current directory. Alternatively, a _full_ path or (_absolute_ path) is the path starting from the root directory. So `/home/kamstut/my_file.txt` is the _absolute_ path for `my_file.txt` and `../my_file.txt` is a _relative_ path. Perform the following actions, in order:

- Write a single command to navigate to the data directory.
- Write a single command to navigate back to your home directory using a _relative_ path. Do not use `~` or the `cd` command without a path argument.

**Relevant topics:** [man](#man), [cd](#cd), [pwd](#pwd), [ls](#ls), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to the data directory.
- Command used to navigate back to your home directory that uses a _relative_ path.
```

##### 5. In Scholar, when you want to deal with _really_ large amounts of data, you want to access scratch (you can read more [here](https://www.rcac.purdue.edu/policies/scholar/)). Your scratch directory on Scholar is located here: `/scratch/scholar/$USER`. `$USER` is an environment variable containing your username. Test it out: `echo /scratch/scholar/$USER`. Perform the following actions:

- Navigate to your scratch directory. 
- Confirm you are in the correct location.
- Execute `myquota`.
- Find the location of the `myquota` bash script.
- Output the first 5 and last 5 lines of the bash script. 
- Count the number of lines in the bash script.
- How many kilobytes is the script?

**Hint:** You could use each of the commands in the relevant topics once.

**Hint:** When you type `myquota` on Scholar there are sometimes two warnings about `xauth` but sometimes there are no warnings.  If you get a warning that says `Warning: untrusted X11 forwarding setup failed: xauth key data not generated` it is safe to ignore this error.

**Hint:** Commands often have _options_. _Options_ are features of the program that you can trigger specifically. You can see the _options_ of a command in the `DESCRIPTION` section of the `man` pages. For example: `man wc`. You can see `-m`, `-l`, and `-w` are all options for `wc`. To test this out:

```{bash, eval=F}
# using the default wc command. "/class/datamine/data/flights/1987.csv" is the first "argument" given to the command.
wc /class/datamine/data/flights/1987.csv
# to count the lines, use the -l option
wc -l /class/datamine/data/flights/1987.csv
# to count the words, use the -w option
wc -w /class/datamine/data/flights/1987.csv
# you can combine options as well
wc -w -l /class/datamine/data/flights/1987.csv
# some people like to use a single tack `-`
wc -wl /class/datamine/data/flights/1987.csv
# order doesn't matter
wc -lw /class/datamine/data/flights/1987.csv
```

**Hint:** The `-h` option for the `du` command is useful.

**Relevant topics:** [cd](#cd), [pwd](#pwd), [type](#type), [head](#head), [tail](#tail), [wc](#wc), [du](#du)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to your scratch directory.
- Command used to confirm your location.
- Output of `myquota`.
- Command used to find the location of the `myquota` script.
- Absolute path of the `myquota` script.
- Command used to output the first 5 lines of the `myquota` script.
- Command used to output the last 5 lines of the `myquota` script.
- Command used to find the number of lines in the `myquota` script.
- Number of lines in the script.
- Command used to find out how many kilobytes the script is.
- Number of kilobytes that the script takes up.
```

##### 6. Perform the following operations:

- Navigate to your scratch directory.
- Copy and paste the file: `/class/datamine/data/flights/1987.csv` to your current directory (scratch).
- Create a new directory called `my_test_dir` in your scratch folder.
- Move the file you copied to your scratch directory, into your new folder.
- Use `touch` to create an empty file named `im_empty.txt` in your scratch folder.
- Remove the directory `my_test_dir` _and_ the contents of the directory.
- Remove the `im_empty.txt` file.

**Hint:** `rmdir` may not be able to do what you think, instead, check out the options for `rm` using `man rm`.

**Relevant topics:** [cd](#cd), [cp](#cp), [mv](#mv), [mkdir](#mkdir), [touch](#touch), [rmdir](#rmdir), [rm](#rm)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to your scratch directory.
- Command used to copy the file, `/class/datamine/data/flights/1987.csv` to your current directory (scratch).
- Command used to create a new directory called `my_test_dir` in your scratch folder.
- Command used to move the file you copied earlier `1987.csv` into your new `my_test_dir` folder.
- Command used to create an empty file named `im_empty.txt` in your scratch folder.
- Command used to remove the directory _and_ the contents of the directory `my_test_dir`.
- Command used to remove the `im_empty.txt` file.
```

##### 7. Please include a statement in Project 3 that says, "I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course." or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan.

---

### Project 4 {#p04-290}

---

**Motivation:** The need to search files and datasets based on the text held within is common during various parts of the data wrangling process. `grep` is an extremely powerful UNIX tool that allows you to do so using regular expressions. Regular expressions are a structured method for searching for specified patterns. Regular expressions can be very complicated, [even professionals can make critical mistakes](https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/). With that being said, learning some of the basics is an incredible tool that will come in handy regardless of the language you are working in.

**Context:** We've just begun to learn the basics of navigating a file system in UNIX using various terminal commands. Now we will go into more depth with one of the most useful command line tools, `grep`, and experiment with regular expressions using `grep`, R, and later on, Python.

**Scope:** grep, regular expression basics, utilizing regular expression tools in R and Python

**Learning objectives:**

```{block, type="bbox"}
- Use `grep` to search for patterns within a dataset.
- Use `cut` to section off and slice up data from the command line.
- Use `wc` to count the number of lines of input.
```

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

**Important note:** I would highly recommend using single quotes `'` to surround your regular expressions. Double quotes can have unexpected behavior due to some shell's expansion rules. In addition, pay close attention to [escaping](#faq-escape-characters) certain [characters](https://unix.stackexchange.com/questions/20804/in-a-regular-expression-which-characters-need-escaping) in your regular expressions. 

#### Dataset

The following questions will use the dataset `the_office_dialogue.csv` found in Scholar under the data directory `/class/datamine/data/`. A public sample of the data can be found here: [the_office_dialogue.csv](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/the_office_dialogue.csv)

Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset.

`grep` stands for (g)lobally search for a (r)egular (e)xpression and (p)rint matching lines. As such, to best demonstrate `grep`, we will be using it with textual data. You can read about and see examples of `grep` [here](https://thedatamine.github.io/the-examples-book/unix.html#grep).

##### 1. Login to Scholar and use `grep` to find the dataset we will use this project. The dataset we will use is the only dataset to have the text "Bears. Beets. Battlestar Galactica.". Where is it located exactly?

**Relevant topics:** *[grep](https://thedatamine.github.io/the-examples-book/unix.html#grep)*

```{block, type="bbox"}
**Item(s) to submit:**

- The `grep` command used to find the dataset.
- The name and location in Scholar of the dataset.
```

##### 2. `grep` prints the line that the text you are searching for appears in. In project 3 we learned a UNIX command to quickly print the first _n_ lines from a file. Use this command to get the headers for the dataset. As you can see, each line in the tv show is a row in the dataset. You can count to see which column the various bits of data live in.

##### Write a line of UNIX commands that searches for "bears. beets. battlestar galactica." and, rather than printing the entire line, prints only the character who speaks the line, as well as the line itself.

**Hint:** *The result if you were to search for "bears. beets. battlestar galactica." should be:*

```{txt}
"Jim","Fact. Bears eat beets. Bears. Beets. Battlestar Galactica."
```

**Hint:** *One method to solve this problem would be to [pipe](https://thedatamine.github.io/the-examples-book/unix.html#piping-and-redirection) the output from `grep` to [`cut`](https://thedatamine.github.io/the-examples-book/unix.html#cut).*

**Relevant topics:** *[cut](#cut), [grep](#grep)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to find the character and original dialogue line that contains "bears. beets. battlestar galactica.". 
```

##### 3. This particular dataset happens to be very small. You could imagine a scenario where the file is many gigabytes and not easy to load completely into R or Python. We are interested in learning what makes Jim and Pam tick as a couple. Use a line of UNIX commands to create a new dataset called `jim_and_pam.csv` (remember, a good place to store data temporarily is `/scratch/scholar/$USER`). Include only lines that are spoken by either Jim or Pam, or reference Jim or Pam in any way. How many rows of data are in the new file? How many megabytes is the new file (to the nearest 1/10th of a megabyte)?

**Hint:** *[Redirection](https://thedatamine.github.io/the-examples-book/unix.html#piping-and-redirection).*

**Hint:** *It is OK if you get an erroneous line where the word "jim" or "pam" appears as a part of another word.*

**Relevant topics:** *[grep](#grep), [ls](#ls), [wc](#wc), [redirection](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to create the new file. 
- The number of rows of data in the new file, and the accompanying UNIX command used to find this out.
- The number of megabytes (to the nearest 1/10th of a megabyte) that the new file has, and the accompanying UNIX command used to find this out.
```

##### 4. Find all lines where either Jim/Pam/Michael/Dwight's name is followed by an exclamation mark. Use only 1 "!" within your regular expression. How many lines are there? Ignore case (whether or not parts of the names are capitalized or not). 

**Relevant topics:** *[grep](#grep), [basic matches](https://r4ds.had.co.nz/strings.html#basic-matches), [escaping characters](#faq-escape-characters)*

```{block, type="bbox"}
**Item(s) to submit:**

- The UNIX command(s) used to solve this problem.
- The number of lines where either Jim/Pam/Michael/Dwight's name is followed by an exclamation mark.
```

##### 5. Find all lines that contain the text "that's what" followed by any amount of any text and then "said". How many lines are there?

**Relevant topics:** *[grep](#grep)*

```{block, type="bbox"}
**Item(s) to submit:**

- The UNIX command used to solve this problem.
- The number of lines that contain the text "that's what" followed by any amount of text and then "said".
```

Regular expressions are really a useful semi language-agnostic tool. What this means is regardless of the programming language your are using, there will be some package that allows you to use regular expressions. In fact, we can use them in both R and Python! This can be particularly useful when dealing with strings. Load up the dataset you discovered in (1) using `read.csv`. Name the resulting data.frame `dat`.

##### 6. The `text_w_direction` column in `dat` contains the characters' lines with inserted direction that helps characters know what to do as they are reciting the lines. Direction is shown between square brackets "[" "]". In this two-part question, we are going to use regular expression to detect the directions.

##### (a) Create a new column called `has_direction` that is set to `TRUE` if the `text_w_direction` column has direction, and `FALSE` otherwise. Use the `grepl` function in R to accomplish this.

**Hint:** *Make sure all opening brackets "[" have a corresponding closing bracket "]".*

**Hint:** *Think of the pattern as any line that has a [, followed by any amount of any text, followed by a ], followed by any amount of any text.*

##### (b) Modify your regular expression to find lines with 2 or more sets of direction. How many lines have more than 2 directions? Modify your code again and find how many have more than 5.

We count the sets of direction in each line by the pairs of square brackets. The following are two simple example sentences.

```{txt}
This is a line with [emphasize this] only 1 direction!
This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug].
```

Your solution to part (a) should find both lines a match. However, in part (b) we want the regular expression pattern to find only lines with 2+ directions, so the first line would not be a match.

In our actual dataset, for example, `dat$text_w_direction[2789]` is a line with 2 directions.

**Relevant topics:** *[grep](#r-grep), [grepl](#r-grep), [basic matches](https://r4ds.had.co.nz/strings.html#basic-matches), [escaping characters](#faq-escape-characters)*

```{block, type="bbox"}
**Item(s) to submit:**

- The R code and regular expression used to solve the first part of this problem.
- The R code and regular expression used to solve the second part of this problem.
- How many lines have >= 2 directions?
- How many lines have >= 5 directions?
```

##### OPTIONAL QUESTION. Use the `str_extract_all` function from the `stringr` package to extract the direction(s) as well as the text between direction(s) from each line. Put the strings in a new column called `direction`.

```{txt}
This is a line with [emphasize this] only 1 direction!
This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug].
```

In this question, your solution may have extracted:

```{txt}
[emphasize this]
[emphasize this] 2 sets of direction, do you see the difference [shrug]
```

(It is okay to keep the text between neighboring pairs of "[" and "]" for the second line.)

**Relevant topics:** *[str_extract_all](#r-str-extract), [basic matches](https://r4ds.had.co.nz/strings.html#basic-matches), [escaping characters](#faq-escape-characters)*

```{block, type="bbox"}
**Item(s) to submit:**

- The R code used to solve this problem.
```

---

### Project 5 {#p05-290}

---

**Motivation:** Becoming comfortable stringing together commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc.

**Context:** We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called piping.

**Scope:** grep, regular expression basics, UNIX utilities, redirection, piping

**Learning objectives:**

```{block, type="bbox"}
- Use `cut` to section off and slice up data from the command line.
- Use piping to string UNIX commands together.
- Use `sort` and it's options to sort data in different ways.
- Use `head` to isolate _n_ lines of output.
- Use `wc` to summarize the number of lines in a file or in output.
- Use `uniq` to filter out non-unique lines.
- Use `grep` to search files effectively.
```

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

Don't forget the very useful documentation shortcut `?` for R code. To use, simply type `?` in the console, followed by the name of the function you are interested in. In the Terminal, you can use the `man` command to check the documentation of `bash` code.

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/amazon/amazon_fine_food_reviews.csv`

A public sample of the data can be found here: [amazon_fine_food_reviews.csv](https://www.datadepot.rcac.purdue.edu/datamine/amazon/amazon_fine_food_reviews.csv)

Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset.

#### Questions

##### 1. What is the `Id` of the most helpful review if we consider the review with highest `HelpfulnessNumerator` to be an indicator of helpfulness (higher is more helpful)?

**Important note:** You can always pipe output to `head` in case you want the first few values of a lot of output. Note that if you used `sort` before `head`, you may see the following error messages:

````
sort: write failed: standard output: Broken pipe
sort: write error
````

This is because `head` would truncate the output from `sort`. This is okay. See [this discussion](https://stackoverflow.com/questions/46202653/bash-error-in-sort-sort-write-failed-standard-output-broken-pipe) for more details.

**Relevant topics:** *[cut](#cut), [sort](#sort), [head](#head), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
- The `Id` of the most helpful review.
```

##### 2. Some entries under the `Summary` column appear more than once. Calculate the proportion of unique summaries over the total number of summaries. Use two lines of UNIX commands to find the numerator and the denominator, and manually calculate the proportion.

##### To further clarify what we mean by _unique_, if we had the following vector in R, `c("a", "b", "a", "c")`, its unique values are `c("a", "b", "c")`. 

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [wc](#wc), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- Two lines of UNIX commands used to solve the problem.
- The ratio of unique `Summary`'s.
```

##### 3. Use a simple UNIX command to create a frequency table of `Score`.

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
- The frequency table.
```

##### 4. Who is the user with the highest number of reviews? There are two columns you could use to answer this question, but which column do you think would be most appropriate and why?

**Hint:** *You may need to pipe the output to `sort` multiple times.*

**Hint:** *To create the frequency table, read through the `man` pages for `uniq`. Man pages are the "manual" pages for UNIX commands. You can read through the man pages for uniq by running the following:*

```{bash, eval=F}
man uniq
```

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [head](#head), [piping](#piping-and-redirection), [man](#man)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
- The frequency table.
```

##### 5. Anecdotally, there seems to be a tendency to leave reviews when we feel strongly (either positive or negative) about a product. For the user with the highest number of reviews, would you say that they follow this pattern of extremes? Let's consider 5 star reviews to be strongly positive and 1 star reviews to be strongly negative. Let's consider anything in between neither strongly positive nor negative.

**Hint:** *You may find the solution to problem (3) useful.*

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [grep](#grep), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
```

##### 6. We want to compare the most helpful review with a `Score` of 5 with the most helpful review with a `Score` of 1. Use UNIX commands to calculate these values. Write down the `ProductId` of both reviews. In the case of a tie, write down all `ProductId`'s to get full credit. In this case we are considering the most helpful review to be the review with the highest `HelpfulnessNumerator`.

**Hint:** *You can use multiple lines to solve this problem.*

**Relevant topics:** *[sort](#sort), [head](#head), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The lines of UNIX commands used to solve the problem.
- `ProductId`'s of both requested reviews.
```

##### 7. Using the `ProductId`'s from the previous question, create a new dataset called `reviews.csv` which contains the `ProductId`'s and `Score` of all reviews with the corresponding `ProductId`'s. (Remember, a good place to store data temporarily is `/scratch/scholar/$USER`.)

**Relevant topics:** *[grep](#grep), [redirection](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
```

##### 8. Use R to load up `reviews.csv` into a new data.frame called `dat`. Create a histogram for each products' `Score`. Compare the most helpful review `Score` with the `Score`'s given in the histogram. Based on this comparison, point out some curiosities about the product that may be worth exploring. For example, if a product receives many high scores, but has a super helpful review that gives the product 1 star, I may tend to wonder if the product is not as great as it seems to be.

**Relevant topics:** *[read.csv](#r-reading-and-writing-data), [hist](#r-plotting)*

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to create the histograms.
- 3 histograms, 1 for each `ProductId`.
- 1-2 sentences describing the curious pattern that you would like to further explore.
```

---

### Project 6 {#p06-290}

---

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
```

#### Dataset: 

The following questions will use the dataset found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/YYYY.csv) or in Scholar:

`/class/datamine/data/flights/subset/YYYY.csv` 

An example from 1987 data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv) or in Scholar:

`/class/datamine/data/flights/subset/1987.csv`

#### Questions 

##### 1. In previous projects we learned how to get a single column of data from a csv file. Write 1 line of UNIX commands to print the 17th column, the `Origin`, from `1987.csv`. Write another line, this time using `awk` to do the same thing. Which one do you prefer, and why?

**Relevant topics:** [cut](#cut), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- One line of UNIX commands to solve the problem *without* using `awk`.
- One line of UNIX commands to solve the problem using `awk`.
- 1-2 sentences describing which method you prefer and why.
```

##### 2. Write a bash script that accepts a year (1987, 1988, etc.) and a column *n* and returns the *nth* column of the associated year of data.

**Relevant topics:** [awk](#awk), [bash scripts](#writing-scripts)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
```

##### 3. How many flights came into Indianapolis (IND) in 2008? First solve this problem without using `awk`, then solve this problem using *only* `awk`.

**Relevant topics:** [cut](#cut), [grep](#grep), [wc](#wc), [awk](#awk), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:** 

- One line of UNIX commands to solve the problem *without* using `awk`.
- One line of  UNIX commands to solve the problem using `awk`. 
- The number of flights that came into Indianapolis (IND) in 2008.
```

##### 4. Do you expect the number of unique origins and destinations to be the same? Find out using any command line tool you'd like. Are they indeed the same? How many unique values do we have per category (`Origin`, `Dest`)?

**Relevant topics:** [cut](#cut), [sort](#sort), [uniq](#uniq), [wc](#wc), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- 1-2 sentences explaining whether or not you expect the number of unique origins and destinations to be the same.
- The UNIX command(s) used to figure out if the number of unique origins and destinations are the same. 
- The number of unique values per category (`Origin`, `Dest`).
```

##### 5. In (4) we found that there are not the same number of unique `Origin`'s as `Dest`'s. Find the IATA airport code for all `Origin`'s that dont appear in a `Dest` and all `Dest`'s that don't appear in an `Origin`.

**Hint:** https://www.tutorialspoint.com/unix_commands/comm.html

**Relevant topics:** [comm](#unix), [cut](#cut), [sort](#sort), [uniq](#uniq), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The line(s) of UNIX command(s) used to answer the question.
- The list of `Origin`s that don't appear in `Dest`.
- The list of `Dest`s that don't appear in `Origin`.
```


##### 6. What was the average number of flights in 2008 per unique `Origin` with the `Dest` of "IND"? How does "PHX" (as a unique `Origin`) compare to the average?

**Hint:** You manually do the average calculation by dividing the result from (3) by the number of unique `Origin`'s that have a `Dest` of "IND".

**Relevant topics:** [awk](#awk), [sort](#sort), [grep](#grep), [wc](#wc)

```{block, type="bbox"}
**Item(s) to submit:**

- The average number of flights in 2008 per unique `Origin` with the `Dest` of "IND".
- 1-2 sentences explaining how "PHX" compares (as a unique `Origin`) to the average?
```

##### 7. Write a bash script that takes a year and IATA airport code and returns the year, and the total number of flights to and from the given airport. Example rows may look like:

```{txt, eval=F}
1987, 12345
1988, 44
```

Run the script with inputs: `1991` and `ORD`. Include the output in your submission.

**Relevant topics:** [bash scripts](#writing-scripts), [cut](#cut), [piping](#piping-and-redirection), [grep](#grep), [wc](#wc)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The output of the script given `1991` and `ORD` as inputs.
```

---

### Project 7 {#p07-290}

---

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:
`/class/datamine/data/flights/subset/YYYY.csv` 

An example of the data for the year 1987 can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv).

Sometimes if you are about to dig into a dataset, it is good to quickly do some sanity checks early on to make sure the data is what you expect it to be. 

##### 1. Write a line of code that prints a list of the unique values in the `DayOfWeek` column. Write a line of code that prints a list of the unique values in the `DayOfMonth` column. Write a line of code that prints a list of the unique values in the `Month` column. Use the `1987.csv` dataset. Are the results what you expected?

**Relevant topics:** [cut](#cut), [sort](#sort)

```{block, type="bbox"}
**Item(s) to submit:**

- 3 lines of code used to get a list of unique values for the chosen columns.
- 1-2 sentences explaining whether or not the results are what you expected.
```

##### 2. Our files should have 29 columns. Write a line of code that prints any lines in a file that do *not* have 29 columns. Test it on `1987.csv`, were there any rows without 29 columns?

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of code used to solve the problem.
- 1-2 sentences explaining whether or not there were any rows without 29 columns.
```

##### 3. Write a bash script that, given a "begin" year and "end" year, cycles through the associated files and prints any lines that do *not* have 29 columns.

**Relevant topics:** [awk](#awk), [bash scripts](#writing-scripts)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The results of running your bash scripts from year 1987 to 2008.
```

##### 4. `awk` is a really good tool to quickly get some data and manipulate it a little bit. For example, let's see the number of kilometers and miles traveled in 1990. To convert from miles to kilometers, simply multiply by 1.609344. 

**Example output:**

```{txt, eval=F}
Miles: 12345
Kilometers: 19867.35168
```

**Relevant topics:** [awk](#awk), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem. 
- The results of running the code.
```

##### 5. Use `awk` to calculate the number of `DepDelay` minutes by `DayOfWeek`. Use `2007.csv`.

**Example output:**

```{txt, eval=F}
DayOfWeek:  0
1:  1234567
2:  1234567
3:  1234567
4:  1234567
5:  1234567
6:  1234567
7:  1234567
```

**Note:** 1 is Monday.

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

##### 6. It wouldn't be fair to compare the total `DepDelay` minutes by `DayOfWeek` as the number of flights may vary. One way to take this into account is to instead calculate an average. Modify (5) to calculate the average number of `DepDelay` minutes by the number of flights per `DayOfWeek`. Use `2007.csv`.

**Example output:**

```{txt, eval=F}
DayOfWeek:  0
1:  1.234567
2:  1.234567
3:  1.234567
4:  1.234567
5:  1.234567
6:  1.234567
7:  1.234567
```

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

##### 7. As a quick follow-up, _slightly_ modify (6) to perform the same calculation for `ArrDelay`. Do the `ArrDelay`s and `DepDelay`s appear to have the highest delays on the same day? Use `2007.csv`.

**Example output:**

```{txt, eval=F}
DayOfWeek:  0
1:  1.234567
2:  1.234567
3:  1.234567
4:  1.234567
5:  1.234567
6:  1.234567
7:  1.234567
```

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
- 1-2 sentences explaining whether or not the `ArrDelay`s and `DepDelay`s appear to have the highest delays on the same day.
```

---

### Project 8 {#p08-290}

---

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:
`/class/datamine/data/flights/subset/YYYY.csv` 

An example of the data for the year 1987 can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv).


Let's say we have a theory that there are more flights on the weekend days (Friday, Saturday, Sunday) than the rest of the days, on average. We can use awk to quickly check it out and see if maybe this looks like something that is true!

##### 1. Write a line of `awk` code that, prints the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code that solves the problem.
- The result: the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008.
```

##### 2. Note that in (1), we are comparing 3 days to 4! Write a line of `awk` code that, prints the average number of flights on a weekend day, followed by the average number of flights on the weekdays. Continue to use data for 2008.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code that solves the problem.
- The result: the average number of flights on the weekend days, followed by the average number of flights on the weekdays for the flights during 2008.
```

We want to look to see if there may be some truth to the whole "snow bird" concept where people will travel to warmer states like Florida and Arizona during the Winter. Let's use the tools we've learned to explore this a little bit. 

##### 3. Take a look at `airports.csv`. In particular run the following:

```{bash, eval=F}
head airports.csv
```

Notice how all of the non-numeric text is surrounded by quotes. The surrounding quotes would need to be escaped for any comparison within `awk`. This is messy and we would prefer to create a new file called `new_airports.csv` without any quotes. Write a line of code to do this. 

**Hint:** You could use `gsub` within `awk` to replace '"' with ''.

**Hint:** If you leave out the column number argument to `gsub` it will apply the substitution to every field in every column.

**Relevant topics:** [awk](#awk), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code used to create the new dataset.
```

##### 4. Write a line of commands that create a new dataset called `az_fl_airports.txt` that contains a list of airport codes for all airports from both Arizona (AZ) and Florida (FL). Use the file we created in (3),`new_airports.csv`.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands to create an array called `airports`.
```

##### 5. Wow! In (4) we discovered a lot of airports! How many airports are there? Did you expect this? Use a line of bash code to answer this question.

**Relevant topics:** [echo](#echo), [wc](#wc), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
- The number of airports.
- 1-2 sentences explaining whether you expected this result and why or why not.
```

##### 6. Create a new dataset that contains all of the data for flights into or out of Florida and Arizona using 2008.csv, use the newly created dataset, `az_fl_airports.txt` in (4) to do so.

**Hint:** https://unix.stackexchange.com/questions/293684/basic-grep-awk-help-extracting-all-lines-containing-a-list-of-terms-from-one-f

**Relevant topics:** [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
```

##### 7. Now that you have code to complete (6), write a bash script that accepts the start year, end year, and filename containing airport codes (`az_fl_airports.txt`), and outputs the data for flights into or out of any of the airports listed in the provided filename containing airport codes using _all_ of the years of data in the provided range. Run the bash script to create a new file called `az_fl_flights.csv`.

**Relevant topics:** [bash scripts](#writing-scripts), [grep](#grep), [for loop](#r-for-loops), [redirection](#redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The line of UNIX code you used to execute the script and create the new dataset.
```

---

### Project 9 {#p09-290}

---

**Motivation:** Structured Query Language (SQL) is a language used for querying and manipulating data in a database. SQL can handle much larger amounts of data than R and Python can alone. SQL is incredibly powerful. In fact, [cloudflare](https://www.cloudflare.com/), a billion dollar company, had much of its starting infrastructure built on top of a Postgresql database (per [this thread on hackernews](https://news.ycombinator.com/item?id=22878136)). Learning SQL is _well_ worth your time!

**Context:** There are a multitude of RDBMSs (relational database management systems). Among the most popular are: MySQL, MariaDB, Postgresql, and SQLite. As we've spent much of this semester in the terminal, we will start in the terminal using SQLite. 

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Explain the advantages and disadvantages of using a database over a tool like a spreadsheet.
- Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/lahman/lahman.db`

#### Questions

##### 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal and access the Lahman database. How many tables are available?

**Hint:** To connect to the database, do the following:

```{bash, eval=F}
sqlite3 /class/datamine/data/lahman/lahman.db
```

**Relevant topics:** [sqlite3](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- How many tables are available in the Lahman database?
- The sqlite3 commands used to figure out how many tables are available.
```

##### 2. Some people like to try to [visit all 30 MLB ballparks](https://www.washingtonpost.com/graphics/2017/sports/how-many-mlb-parks-have-you-visited/) in their lifetime.  Use SQL commands to get a list of `parks` and the cities they're located in. For your final answer, limit the output to 10 records/rows.

**Note:** There may be more than 30 parks in your result, this is ok. For long results, you can limit the number of printed results using the `LIMIT` clause.

**Hint:** Make sure you take a look at the data dictionary for the table and column names.

**Hint:** To see the header row as a part of each query result, run the following:

```{sql, eval=F}
.headers on
```

**Relevant topics:** [SELECT](#sql-examples), [FROM](#sql-examples), [LIMIT](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 3. There is nothing more exciting to witness than a home run hit by a batter. It's impressive if a player hits more than 40 in a season. Find the hitters who have hit 60 or more home runs (`HR`) in a season. List their `playerID`, `yearID`, home run total, and the `teamID` they played for.

**Hint:** There are 8 occurrences of home runs greater than 60.

**Hint:** The `batting` table is where you should look for this question.

**Relevant topics:** [SELECT](#sql-examples), [FROM](#sql-examples), [LIMIT](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 4. Make a list of players born on your birth day (don't worry about the year). Display their first names, last names, and birth year. Order the list descending by their birth year.

**Hint:** The `people` table is where you should look for this question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 5. Get the Cleveland (CLE) Pitching Roster from the 2016 season (`playerID`, `W`, `L`, `SO`). Order the pitchers by number of Strikeouts (SO).

**Hint:** The `pitching` table is where you should look for this question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

##### 6. Find the top 10 team total of Errors between 1960 and 1970. Display their Win and Loss totals too. What is the name of the 3rd place team?

**Hint:** The `BETWEEN` clause is useful here.

**Hint:** It is OK to use multiple queries to answer the question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

##### 7. Find the `playerID` for Bob Lemon. What year and team was he on when he pitched the most wins (use table `pitching`)? What year and team did he win the most games as a manager (use table `managers`)?

**Hint:** It is OK to use multiple queries to answer the question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

---

### Project 10 {#p10-290}

---

**Motivation:** Although SQL syntax may still feel unnatural and foreign, with more practice it _will_ start to make more sense. The ability to read and write SQL queries is a bread-and-butter skill for anyone working with data. 

**Context:** We are in the second of a series of projects that focus on learning the basics of SQL. In this project, we will continue to harden our understanding of SQL syntax, and introduce common SQL functions like `AVG`, `MIN`, and `MAX`.

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Explain the advantages and disadvantages of using a database over a tool like a spreadsheet.
- Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
- Utilize SQL functions like min, max, avg, sum, and count to solve data-driven problems.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/lahman/lahman.db`

#### Questions 

##### 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and, rather than navigating to the terminal like we did in the previous project, instead, create a connection to our MariaDB lahman database using the `RMariaDB` package in R, and the credentials below. Confirm the connection by running the following code chunk:

```{r, eval=F}
host <- "scholar-db.rcac.purdue.edu"
dbname <- "lahmandb"
user <- "lahman_user"
password <- "HitAH0merun"
head(dbGetQuery(con, "SHOW tables;"))
```

**Hint:** In the example provided, the variable `con` is the connection. Change `con` to whatever you name the result of `dbConnect`.

**Relevant topics:** [RMariaDB](#sql), [dbConnect](#sql), [dbGetQuery](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Output from running your (potentially modified) `head(dbGetQuery(con, "SHOW tables;"))`.
```

##### 2. Find Corey Kluber's totals for his career. Include his strikeouts (`SO`), walks (`BB`), and his Strikeouts to Walks ratio. A Strikeout to Walks ratio is calculated by this equation: $\frac{Strikeouts}{Walks}$. 

**Important note:** In our [project template](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), we show 2 primary ways to run SQL queries from within R/RMarkdown. In question 5, we wrap our queries in R code. In question 6, we use the database connection, `con`, to run SQL queries directly within an SQL code chunk. In this project, we will just use the first method as it has the advantage of having the result of the query ready to be used within our R environment. 
**Important note:** Questions in this project need to be solved using SQL when possible. You will not receive credit for a question if you use `sum` in R rather than `SUM` in SQL. 

**Relevant topics:** [dbGetQuery](#sql), [SUM](#sql), [SELECT](#sql), [FROM](#sql), [WHERE](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem. 
- The result of running the R code.
```

##### 3. How many times has Giancarlo Stanton struck out in years in which he played for "MIA" or "FLO"?

**Relevant topics:** [dbGetQuery](#sql), [AND/OR](#sql), [COUNT](#sql), [SUM](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 4. Calculate the Batting Average of batters between 2000 and 2010, with more than 300 at-bats (ABs). List the top 5 batting averages next to the `playerID` (with team and year). Batting Averages are calculated as $\frac{H}{AB}$.

**Relevant topics:** [dbGetQuery](#sql), [ORDER BY](#sql-order-by), [BETWEEEN](#sql-between)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 5. How many unique players have hit > 50 home runs (`HR`) in a season? 

**Hint:** If you view `DISTINCT` as being paired with `SELECT`, instead, think of it as being paired with one of the fields you are selecting.

**Relevant topics:** [dbGetQuery](#sql), [DISTINCT](#sql-distinct), [COUNT](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 6. How many players are members of the 40/40 club? These are players that have stolen more than 40 bases (`SB`) and hit more than 40 home runs (`HR`).

**Relevant topics:** [dbGetQuery](#sql), [AND/OR](#sql), [DISTINCT](#sql-distinct), [COUNT](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 7. Find the number of unique players that attended Purdue University. Start by finding the `schoolID` for Purdue and then find the number of players who played there. Who had more? Purdue or IU? Use the information you have in the database, and the power of R to create a misleading graphic that makes Purdue look better than IU, even if just at first glance. Make sure you label the graphic.

**Hint:** You can mess with the scale of the y-axis. You could (potentially) filter the data to start from a certain year or be between two dates.

**Hint:** To find IU's id, try the following query: `SELECT schoolID FROM schools WHERE name_full LIKE '%indiana%';`. 

**Relevant topics:** [dbGetQuery](#sql), [plotting in R](#r-plotting), [COUNT](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

---

### Project 11 {#p11-290}

---

**Motivation:** Being able to use results of queries as tables in new queries (also known as writing sub-queries), and calculating values like MIN, MAX, and AVG in aggregate are key skills to have in order to write more complex queries. In this project we will learn about aliasing, writing sub-queries, and calculating aggregate values.

**Context:** We are in the middle of a series of projects focused on working with databases and SQL. In this project we introduce aliasing, sub-queries, and calculating aggregate values using a much larger dataset!

**Scope:** sql, sql in R

**Learning objectives:**

```{block, type="bbox"}
- Demonstrate the ability to interact with popular database management systems within R.
- Solve data-driven problems using a combination of SQL and R.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
- Showcase the ability to filter, alias, and write subqueries.
- Perform grouping and aggregate data using group by and the following functions: count, max, sum, avg, like, having. Explain when to use having, and when to use where.
```

#### Dataset

The following questions will use the `elections` database and the following database found in Scholar:

`/class/datamine/data/election/itcontYYYY.txt` (for example, data for year 1980 would be `/class/datamine/data/electionitcont1980.txt`)

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcontYYYY.txt)

Up until now, you've been working with a neatly organized database containing baseball data. As fantastic as this database is, it would be trivial to load up the entire database in R or Python and do your analysis using `merge`-like functions. Now, we are going to deal with a much larger set of data.

#### Questions

##### 1. Approximately how large was the lahman database (use the sqlite database in Scholar: `/class/datamine/data/lahman/lahman.db`)? Use UNIX utilities you've learned about this semester to write a line of code to return the amount of data (in MB) in the elections folder `/class/datamine/data/election/`. How much data (in MB) is there? 

The data in that folder has been added to the `elections` database in the `elections` table. Write a SQL query that returns how many rows of data are in the database. How many rows of data are in the database?

**Hint:** This will take some time! Be patient.

**Relevant topics:** [sql](#sql), [sql in R](#sql-in-r), [awk](#awk), [ls](#ls)

```{block, type="bbox"}
**Item(s) to submit:**

- Approximate size of the lahman database in mb.
- Line of code (bash/awk) to calculate the size (in mb) of the entire elections dataset in `/class/datamine/data/election`.
- The size of the elections data in mb.
- SQL query used to find the number of rows of data in the `elections` table in the `elections` database.
- The number of rows in the `elections` table in the `elections` database.
```

##### 2. Write a SQL query using the `LIKE` command to find a unique list of `zip_code`s that start with "479". How many unique `zip_code`s are there that begin with "479"?

**Hint:** Make sure you only select `zip_code`s.

**Relevant topics:** [sql](#sql), like

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to answer the question.
- The first 5 results from running the query.
```

##### 3. Write a SQL query that counts the number of donations (rows) that are from Indiana. How many donations are from Indiana? Rewrite the query and create an _alias_ for our field so it doesn't read `COUNT(*)` but rather `Indiana Donations`. 

**Relevant topics:** [sql](#sql), [where](#sql-where), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
- The result of the SQL query.
```

##### 4. Rewrite the query in (3) so the result is displayed like the following:

```{txt}
+-------------+
| Donations   |
+-------------+
| IN: 1111778 |
+-------------+
```

**Hint:** Use CONCAT and aliasing to accomplish this.

**Relevant topics:** [sql](#sql), [aliasing](#sql-aliasing), concat

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
```

##### 5. In (2) we wrote a query that returns a unique list of `zip_code`s that start with "479". In (3) we wrote a query that counts the number of donations that are from Indiana. Use our query from (2) as a sub-query to find how many donations come from areas with `zip_code`s starting with "479". What percent of donations in Indiana come from said `zip_code`s?

**Relevant topics:** [sql](#sql), [aliasing](#sql-aliasing), subqueries

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to answer the question.
- The percentage of donations from Indiana from `zip_code`s starting with "479".
```

##### 6. In (3) we wrote a query that counts the number of donations that are from Indiana. When running queries like this, a natural "next question" is to ask the same question about another state. SQL gives us the ability to calculate functions in aggregate when grouping by a certain column. Write a SQL query that returns the state, number of donations from each state, the sum of the donations (`transaction_amt`). Which 5 states gave the most donations (highest count)? Order you result from most to least.

**Hint:** You may want to create an alias in order to sort.

**Relevant topics:** [sql](#sql), [group by](#sql-groupby)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question. 
- Which 5 states gave the most donations?
```

---

### Project 12 {#p12-290}

--- 

**Motivation:** Databases are comprised of many tables. It is imperative that we learn how to combine data from multiple tables using queries. To do so we perform joins! In this project we will explore learn about and practice using joins on a database containing bike trip information from the Bay Area Bike Share. 

**Context:** We've introduced a variety of SQL commands that let you filter and extract information from a database in an systematic way. In this project we will introduce joins, a powerful method to combine data from different tables.

**Scope:** SQL, sqlite, joins

**Learning objectives:**

```{block, type="bbox"}
- Briefly explain the differences between left and inner join and demonstrate the ability to use the join statements to solve a data-driven problem.
- Perform grouping and aggregate data using group by and the following functions: count, max, sum, avg, like, having. 
- Showcase the ability to filter, alias, and write subqueries.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/bay_area_bike_share/bay_area_bike_share.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/bay_area_bike_share/bay_area_bike_share.db).

#### Questions 

##### 1. There are a variety of ways to join data using SQL. With that being said, if you are able to understand and use a LEFT JOIN and INNER JOIN, you can perform *all* of the other types of joins (RIGHT JOIN, FULL OUTER JOIN). Given the following two tables, use [RMarkdown](#r-rmarkdown) to display the result of performing the following query as a table:

```{sql, eval=F}
SELECT * FROM users AS u INNER JOIN dorms AS d ON u.dorm=d.id;
```

**users:**

id | first_name | last_name | dorm
---|------------|-----------|-----
1  | Alice      | Smith     | 1
2  | Bob        | Johnson   | 2
3  | Susan      | Marques   | 3
4  | Amare      | Keita     | 3
5  | Kristen    | Lakehold  | 4

**dorms:**

id | name | capacity | address
---|------|----------|--------
1  | Windsor Halls | NULL | Windsor Halls, West Lafayette, IN, 47906
2  | Cary Quadrangle | 1200 | 1016 W Stadium Ave, West Lafayette, IN 47906
3  | Hillenbrand Hall | NULL | 1301 3rd Street, West Lafayette, IN 47906

**Relevant topics:** [sql](#sql), [inner join](#sql-inner-join)

```{block, type="bbox"}
**Item(s) to submit:**

- [RMarkdown](#r-rmarkdown) table displaying the result of performing the following query as a table.
```

##### 2. Using the same two tables from (1), use [RMarkdown](#r-rmarkdown) to display the result of performing the following query as a table. Explain the difference between an INNER JOIN and LEFT JOIN.

```{sql, eval=F}
SELECT * FROM users AS u LEFT JOIN dorms AS d ON u.dorm=d.id;
```

**Relevant topics:** [sql](#sql), left join

```{block, type="bbox"}
**Item(s) to submit:**

- [RMarkdown](#r-rmarkdown) table displaying the result of performing the following query as a table.
- 1-2 sentences explaining (in your own words) what the difference between and INNER and LEFT JOIN is.
```

##### 3. Aliases can be created for tables, fields, and even results of aggregate functions (like MIN, MAX, COUNT, AVG, etc.). In addition, you can combine fields using the `sqlite` concatenate operator `||` (see [here](https://www.sqlitetutorial.net/sqlite-string-functions/sqlite-concat/)). Write a query that returns the first 5 records of information from the `station` table formatted in the following way:

`(id) name @ (lat, long)`

For example:

`(84) Ryland Park @ (37.342725,-121.895617)`

**Relevant topics:** [aliasing](#sql-examples), [concat](#cat)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem. 
- The first 5 records of information from the `station` table.
```

##### 4. There is a variety of interesting weather information in the `weather` table. Write a query that finds the average `mean_temperature_f` by `zip_code`. Which is on average the warmest `zip_code`?

Use aliases to format the result in the following way:

```{txt}
Zip Code|Avg Temperature
94041|61.3808219178082
```

**Relevant topics:** [aliasing](#sql-aliasing), [group by](#sql-groupby), [avg](#sql-avg)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
- The results of the query copy and pasted.
```

##### 5. From (4) we can see that there are only 5 `zip_code`s with weather information. How many unique `zip_code`s do we have in the `trip` table? Write a query that finds the number of unique `zip_code`s in the `trip` table. Write another query that lists the `zip_code` and count of the number of times the `zip_code` appears. If we had originally assumed that the `zip_code` was related to the location of the trip itself, we were wrong. Can you think of a likely explanation?

Relevant topics: [group by](#sql-examples), [count](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to solve this problem.
- 1-2 sentences explainging what a possible explanation for the `zip_code`s could be.
```

##### 6. In (4) we wrote a query that finds the average `mean_temperature_f` by `zip_code`. What if we want to tack on to our results information from each row in the `station` table based on the `zip_code`s? To do, use an INNER JOIN. INNER JOIN combines tables based on specified fields, and returns only rows where there is a match in both the "left" and "right" tables.

**Hint:** Use the query from (4) as a sub query within your solution.

**Relevant topics:** [inner join](#sql-inner-join), subqueries, [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
```

##### 7. In (5) we eluded that the `zip_code`s in the `trip` table aren't very consistent. Users can enter a zip code when using the app. This means that `zip_code` can be from anywhere in the world! With that being said, if the `zip_code` is one of the 5 `zip_code`s for which we have weather data (from question 4), we can add that weather information to matching rows of the `trip` table. In (6) we used an INNER JOIN to append some weather information to each row in the `station` table. For this question, write a query that performs an INNER JOIN and appends weather data from the `weather` table to the trip data from the `trip` table. Limit your solution to 5 lines. 

**Hint:** You will want to wrap your dates and datetimes in [sqlite's `date` function](https://www.sqlitetutorial.net/sqlite-date-functions/sqlite-date-function/) prior to comparison.

**Important note:** Notice that the weather data has about 1 row of weather information for each date and each zip code. This means you may have to join your data based on multiple constraints instead of just 1 like in (6).

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
- First 5 lines of output.
```

--- 

### Project 13 {#p13-290}

---

**Motivation:** Databases you will work with won't necessarily come organized in the way that you like. Getting really comfortable writing longer queries where you have to perform many joins, alias fields and tables, and aggregate results, is important. In addition, gaining some familiarity with terms like _primary key_, and _foreign key_ will prove useful when you need to search for help online. In this project we will write some more complicated queries with a fun database. Proper preparation prevents poor performance, and that means practice!

**Context:** We are towards the end of a series of projects that give you an opportunity to practice using SQL. In this project, we will reinforce topics you've already learned, with a focus on subqueries and joins. 

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Write and run SQL queries in `sqlite` on real-world data.
- Identify primary and foreign keys in a SQL table.
```

#### Dataset 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/movies_and_tv/imdb.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/imdb.db).

#### Questions

##### 1. A primary key is a field in a table which uniquely identifies a row in the table. Primary keys _must_ be unique values, and this is enforced at the database level. A foreign key is a field whose value matches a primary key in a different table. A table can have 0-1 primary key, but it can have 0+ foreign keys. Examine the `titles` table. Do you think there are any primary keys? How about foreign keys? 

**Relevant topics:** primary key, foreign key

```{block, type="bbox"}
**Item(s) to submit:**

- List any primary or foreign keys in the `titles` table.
```

##### 2. Examine the `episodes` table. Based on observation and the column names, do you think there are any primary keys? How about foreign keys?

**Relevant topics:** primary key, foreign key

```{block, type="bbox"}
**Item(s) to submit:**

- List any primary or foreign keys in the `episodes` table.
```

If you paste a `title_id` to the end of the following url, it will pull up the page for the title. For example, https://www.imdb.com/title/tt0413573 leads to the page for the TV series Grey's Anatomy. 

##### 3. Write a query to confirm that the `title_id` tt0413573 does indeed belong to Grey's Anatomy.

**Relevant topics:** [select](#sql), [where](#sql-where)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- Output of the query.
```

##### 4. The `episode_title_id` column in the `episodes` table references titles of individual episodes of a tv series. The `show_title_id` references the titles of the show itself. With that in mind, write a query that gets a list of all of the episodes and titles of Grey's Anatomy.

**Relevant topics:** [inner join](#sql-inner-join)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
```

##### 5. Like we explained in (3), you can find the `title_id` of a tv show, a tv show episodes, or a movie by browsing imdb.com and getting the `title_id` directly from the url. Browse imdb.com and find your favorite tv show. Get the `title_id` from the url and run the following query to confirm that the tv show is in our database:

```{sql, eval=F}
SELECT * FROM titles WHERE title_id='<title id here>';
```

Make sure to replace "\<title id here\>" with the `title_id` of your favorite show. If your show does not appear, or has only a single season, pick another show until you find one we have in our database with multiple seasons.

```{block, type="bbox"}
**Item(s) to submit:**

- The `title_id` of your favorite tv show.
- The output from running the provided (modified) query.
```

##### 6. We want to write a query that returns the title and rating of the highest rated episode of the tv show you chose in (5). In order to do so, first write a query that returns a list of `episode_title_id`s (found in the `episodes` table), with the `primary_title` (found in the `titles` table) of the episode. 

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- The first 5 results from your query.
```

##### 7. Write a query that adds the rating to the end of each episode. To do so, use the query you wrote in (6) as a subquery. Was this also your favorite episode?

**Relevant topics:** inner join, aliasing, subqueries, desc, limit, order by

**Note**: Various helpful examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- The `episode_title_id`, `primary_title`, and `rating` of the top rated episode from the tv series from (5).
- A statement saying whether it is also your favorite episode.
```

---

### Project 14 {#p14-290}

---

**Motivation:** As we learned earlier in the semester, bash scripts are a powerful tool when you need to perform repeated tasks in a UNIX-like system. In addition, sometimes preprocessing data using UNIX tools prior to analysis in R or Python is useful. Ample practice is integral to becoming proficient with these tools. As such, we will be reviewing topics learned earlier in the semester.

**Context:** We've just ended a series of projects focused on SQL. In this project we will begin to review topics learned throughout the semester, starting writing bash scripts using the various UNIX tools we learned about.

**Scope:** awk, UNIX utilities, bash scripts, fread

**Learning objectives:**

```{block, type="bbox"}
- Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc.
- Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc.
- Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc.
- Use grep to search files effectively.
- Use cut to section off data from the command line.
- Use piping to string UNIX commands together.
- Use awk for data extraction, and preprocessing.
- Create bash scripts to automate a process or processes.
```

#### Dataset 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/forest`

To read more about the two files from this dataset that you will be working with:

PLOTSNAP.csv: 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/plot-level-data-gathered-through-forest/metadata#fields

TREE.csv: 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/metadata 

AND 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/data

#### Questions

##### 1. Take a look at at `PLOTSNAP.csv`. Write a line of awk code that displays the `STATECD` followed by the number of rows with that `STATECD`.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- Count of the following `STATECD`s: 1, 2, 4, 5, 6
```

##### 2. Unfortunately, there isn't a very accessible list available that shows which state each `STATECD` represents. This is no problem for us though, the dataset has `LAT` and `LON`! Write some bash that prints just the `STATECD`, `LAT`, and `LON`.

**Note:** There are 92 columns in our dataset: `awk -F, 'NR==1{print NF}' PLOTSNAP.csv`. To create a list of `STATECD` to state, we only really need `STATECD`, `LAT`, and `LON`. Keeping the other 89 variables will keep our data at 2.1gb.

**Relevant topics:** [cut](#cut), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The output of your code piped to `head`.
```

##### 3. `fread` is a "Fast and Friendly File Finagler". It is part of the very popular `data.table` package in R. We will learn more about this package next semester. For now, read the documentation [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/fread) and use the `cmd` argument in conjunction with your bash code from (2) to preprocess data prior to reading it into a `data.table` in your R environment. 

**Relevant topics:** fread

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The `head` of the resulting `data.table`.
```

##### 4. Follow the directions [here]() to install `ggmap` and get an API key. There are over 4 million rows in our dataset -- we do _not_ want to hit Google's API that many times, nor would that work. Instead, do the following:

- Unless you feel comfortable using `data.table`, convert your `data.table` to a `data.frame`:

```{r, eval=F}
my_dataframe <- data.frame(my_datatable)
```

- Calculate the average `LAT` and `LON` for each `STATECD`, and call the new `data.frame` `dat`. 
- For each row in `dat`, run a reverse geocode and append the state to a new column called `ADDRESS`.

**Hint:** To calculate the average `LAT` and `LON` for each `STATECD`, you could use the [`sqldf`](https://www.rdocumentation.org/packages/sqldf/versions/0.4-11) package to run SQL queries on your `data.frame`.

**Hint:** To get the address, given `LAT` and `LON`:

```{r, eval=F}
geo <- revgeocode(c(-86.916576, 40.433663), output = "address")
geo
```

**Hint:** `mapply` is a useful apply function to use to solve this problem.

**Important note:** It is okay to get NA's for some of the addresses.

**Relevant topics:** [ggmap](#r-ggmap), [functions](#r-apply-functions), sqldf

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The `head` of the resulting `data.frame`.
```

---

### Project 15 {#p15-290}

---

**Motivation:** We've done a lot of work with SQL this semester. Let's review concepts in this project and mix and match R, Python, and SQL to solve data-driven problems.

**Context:** In this project, we will reinforce topics you've already learned, with a focus on SQL.

**Scope:** SQL, sqlite, R, Python

**Learning objectives:**

```{block, type="bbox"}
- Write and run SQL queries in `sqlite` on real-world data.
- Use SQL from within R and Python.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/movies_and_tv/imdb.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/imdb.db).

In this project we want to offer the flexibility of using your choice of R and/or Python. To keep things as consistent as possible, please use Rmarkdown on https://rstudio.scholar.rcac.purdue.edu/. See [here](#python-on-scholar) to learn how to run Python in this environment.

#### Questions 

##### 1. What is the first year where our database has > 1000 titles? Use the `premiered` column in the `titles` table as our year. What year has the most titles?

**Relevant topics:** [count](#sql-examples), [group by](#sql-groupby), [order by](#sql-order-by), [desc](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- 1 or more SQL queries used to answer the questions.
- What year is the first year to have > 1000 titles? 
- What year has the most titles?
```

##### 2. How many, and what are the unique `type`s from the `titles` table? From the year from (1) with the most `titles`, how many titles of each `type` are there?

```{block, type="bbox"}
**Item(s) to submit:**

- 1 or more SQL queries used to answer the questions.
- How many and what are the unique `types` from the `titles` table?
- A list of `type` and and count for the year (`premiered`) 2017.
```

F.R.I.E.N.D.S is a popular tv show. They have an interesting naming convention for the names of their episodes. They all begin with the text "The One ...". There are 6 primary characters in the show: Chandler, Joey, Monica, Phoebe, Rachel, and Ross. Let's use SQL and R to take a look at how many times each characters' names appear in the title of the episodes.

##### 3. Write a query that gets the `episode_title_id`, `primary_title`, `rating`, and `votes`, of all of the episodes of Friends (`title_id` is tt0108778). 

**Hint:** You can slightly modify the solution to question (7) in project 13.

**Relevant topics:** [inner join](#sql-inner-join), subqueries, [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
- First 5 results of the query.
```

The next couple of questions should be complete in the same language. You can use either R or Python, but you must use the same for both questions.

##### 4. Now that you have a working query, connect to the database and run the query to get the data into an R or `pandas` data frame. In previous projects, we learned how to used regular expressions to search for text. For each character, how many episodes `primary_title`s contained their name? 

**Relevant topics:** [SQL in R](#sql-in-r), [SQL in Python](#sql-in-python), [grep](#r-grep)

```{block, type="bbox"}
**Item(s) to submit:**

- R or Python code in a code chunk that was used to find the solution.
- The solution pasted below the code chunk.
```

##### 5. Create a graphic showing our results in (2) using your favorite package. Make sure the plot has a good title, x-label, y-label, and try to incorporate some of the following colors: #273c8b, #bd253a, #016f7c, #f56934, #016c5a, #9055b1, #eaab37.

**Relevant topics:** [plotting](#r-plotting)

```{block, type="bbox"}
**Item(s) to submit:**

- The R or Python code used to generate the graphic.
- The graphic in a png or jpg/jpeg format.
```

##### 6. Use any combination of SQL, R, and Python you'd like in order to find which of the following 3 genres has the highest average rating for movies (see `type` column from `titles` table): Romance, Comedy, Animation. In the `titles` table, you can find the genres in the `genres` column. There may be some overlap (i.e. a movie may have more than one genre), this is ok. 

To query rows which have the genre Action as one of its genres:

```{sql, eval=F}
SELECT * FROM titles WHERE genres LIKE '%action%';
```

**Relevant topics:** [like](#sql-examples), [inner join](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- Any code you used to solve the problem in a code chunk.
- The average rating of each of the genres listed for movies.
```

---

## STAT 39000

### Project 1 {#p01-390}

---

**Motivation:** In this project we will jump right into an R review. In this project we are going to break one larger data-wrangling problem into discrete parts. There is a slight emphasis on writing functions and dealing with strings. At the end of this project we will have greatly simplified a dataset, making it easy to dig into.

**Context:** We just started the semester and are digging into a large dataset, and in doing so, reviewing R concepts we've previously learned.

**Scope:** data wrangling in R, functions

**Learning objectives:**

```{block, type="bbox"}
- Comprehend what a function is, and the components of a function in R.
- Read and write basic (csv) data.
- Utilize apply functions in order to solve a data-driven problem.
```

Make sure to read about, and use the template found [here](#templates), and the important information about projects submissions [here](#submissions).

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

**Important note:** It is highly recommended that you use https://rstudio.scholar.rcac.purdue.edu/. Simply click on the link and login using your Purdue account credentials.

We decided to move away from ThinLinc and away from the version of RStudio used last year (https://desktop.scholar.rcac.purdue.edu).  The version of RStudio is known to have some strange issues when running code chunks.

Remember the very useful documentation shortcut `?`. To use, simply type `?` in the console, followed by the name of the function you are interested in. 

You can also look for package documentation by using `help(package=PACKAGENAME)`, so for example, to see the documentation for the package `ggplot2`, we could run:

```{r, eval=F}
help(package=ggplot2)
```

Sometimes it can be helpful to see the source code of a defined function. A [function](https://www.tutorialspoint.com/r/r_functions.htm) is any chunk of organized code that is used to perform an operation. Source code is the underlying `R` or `c` or `c++` code that is used to create the function. To see the source code of a defined function, type the function's name without the `()`. For example, if we were curious about what the function `Reduce` does, we could run:

```{r, eval=F}
Reduce
```

Occasionally this will be less useful as the resulting code will be code that calls `c` code we can't see. Other times it will allow you to understand the function better.

#### Dataset: 

`/class/datamine/data/airbnb`

Often times (maybe even the majority of the time) data doesnt come in one nice file or database. Explore the datasets in `/class/datamine/data/airbnb`.

##### 1. You may have noted that, for each country, city, and date we can find 3 files: `calendar.csv.gz`, `listings.csv.gz`, and `reviews.csv.gz` (for now, we will ignore all files in the "visualisations" folders).

##### Let's take a look at the data in each of the three types of files. Pick a country, city and date, and read the first 50 rows of each of the 3 datasets (`calendar.csv.gz`, `listings.csv.gz`, and `reviews.csv.gz`). Provide 1-2 sentences explaining the type of information found in each, and what variable(s) could be used to join them. 

**Hint:** `read.csv` has an argument to select the number of rows we want to read.

**Hint:** Depending on the country that you pick, the listings and/or the reviews might not display properly in RMarkdown.  So you do not need to display the first 50 rows of the listings and/or reviews, in your RMarkdown document.  It is OK to just display the first 50 rows of the calendar entries.

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code used to read the first 50 rows of each dataset.
- 1-2 sentences briefly describing the information contained in each dataset.
- Name(s) of variable(s) that could be used to join them.
```

To read a compressed csv, simply use the `read.csv` function:

```{r, eval=F}
dat <- read.csv("/class/datamine/data/airbnb/brazil/rj/rio-de-janeiro/2019-06-19/data/calendar.csv.gz")
head(dat)
```

Let's work towards getting this data into an easier format to analyze. From now on, we will focus on the `listings.csv.gz` datasets.


##### 2. Write a function called `get_paths_for_country`, that, given a string with the country name, returns a vector with the full paths for all `listings.csv.gz` files, starting with `/class/datamine/data/airbnb/...`.

##### For example, the output from `get_paths_for_country("united-states")` should have 28 entries.  Here are the first 5 entries in the output:

```{txt}
 [1] "/class/datamine/data/airbnb/united-states/ca/los-angeles/2019-07-08/data/listings.csv.gz"       
 [2] "/class/datamine/data/airbnb/united-states/ca/oakland/2019-07-13/data/listings.csv.gz"           
 [3] "/class/datamine/data/airbnb/united-states/ca/pacific-grove/2019-07-01/data/listings.csv.gz"     
 [4] "/class/datamine/data/airbnb/united-states/ca/san-diego/2019-07-14/data/listings.csv.gz"         
 [5] "/class/datamine/data/airbnb/united-states/ca/san-francisco/2019-07-08/data/listings.csv.gz"     
```

**Hint:** `list.files` is useful with the `recursive=T` option.

**Hint:** Use `grep` to search for the pattern `listings.csv.gz` (within the results from the first hint), and use the option `value=T` to display the values found by the `grep` function.

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `get_paths_for_country` function.
```



##### 3. Write a function called `get_data_for_country` that, given a string with the country name, returns a data.frame containing the all listings data for that country. Use your previously written function to help you. 

**Hint:** Use `stringsAsFactors=F` in the `read.csv` function.

**Hint:** Use `do.call(rbind, <listofdataframes>)` to combine a list of dataframes into a single dataframe.

**Relevant topics:** [rbind](#r-bind), [lapply](#r-lapply), [function](#r-writing-functions)

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `get_data_for_country` function.
```



##### 4. Use your `get_data_for_country` to get the data for a country of your choice, and make sure to name the data.frame `listings`. Take a look at the following columns: `host_is_superhost`, `host_has_profile_pic`, `host_identity_verified`, and  `is_location_exact`. What is the data type for each column?  (You can use `class` or `typeof` or `str` to see the data type.)

##### These columns would make more sense as logical values (TRUE/FALSE/NA).

##### Write a function called `transform_column` that, given a column containing lowercase "t"s and "f"s, your function will transform it to logical (TRUE/FALSE/NA) values. Note that NA values for these columns appear as blank (`""`), and we need to be careful when transforming the data. Test your function on column `host_is_superhost`.

**Relevant topics:** class, typeof, str, toupper, as.logical

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code for your `transform_column` function.
- Type of `transform_column(listings$host_is_superhost)`.
```



##### 5. Create a histogram for response rates (`host_response_rate`) for super hosts (where `host_is_superhost` is `TRUE`). If your listings do not contain any super hosts, load data from a different country. Note that we first need to convert `host_response_rate` from a character containing "%" signs to a numeric variable.

**Relevant topics:** [gsub](#r-writing-functions), as.numeric

```{block, type="bbox"}
**Item(s) to submit:**

- Chunk of code used to answer the question.
- Histogram of response rates for super hosts.
```



---

### Project 2 {#p02-390}

---

**Motivation:** The ability to quickly reproduce an analysis is important. It is often necessary that other individuals will need to be able to understand and reproduce an analysis. This concept is so important there are classes solely on reproducible research! In fact, there are papers that investigate and highlight the lack of reproducibility in various fields. If you are interested in reading about this topic, a good place to start is the paper titled ["Why Most Published Research Findings Are False"](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124), by John Ioannidis (2005). 

**Context:** Making your work reproducible is extremely important. We will focus on the computational part of reproducibility. We will learn RMarkdown to document your analyses so others can easily understand and reproduce the computations that led to your conclusions. Pay close attention as future project templates will be RMarkdown templates.

**Scope:** Understand Markdown, RMarkdown, and how to use it to make your data analysis reproducible.

**Learning objectives:**

```{block, type="bbox"}
- Use Markdown syntax within an Rmarkdown document to achieve various text transformations.
- Use RMarkdown code chunks to display and/or run snippets of code.
```

#### Questions

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_8rsq5yrn&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_bjrv34ss" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

##### 1. Make the following text (including the asterisks) bold: `This needs to be **very** bold`. Make the following text (including the underscores) italicized: `This needs to be _very_ italicized.`

**Important note:** Surround your answer in 4 backticks. This will allow you to display the markdown _without_ having the markdown "take effect". For example:

`````markdown
````
Some *marked* **up** text.
````
`````

**Hint:** *Be sure to check out the [Rmarkdown Cheatsheet](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf) and our section on [Rmarkdown in the book](https://thedatamine.github.io/the-examples-book/r.html#r-rmarkdown).*

**Note:** *Rmarkdown is essentially Markdown + the ability to run and display code chunks. In this question, we are actually using Markdown within Rmarkdown!*

**Relevant topics:** [rmarkdown](#r-rmarkdown), [escaping characters](#escape-characters)

```{block, type="bbox"}
**Item(s) to submit:**
- 2 lines of markdown text, surrounded by 4 backticks. Note that when compiled, this text will be unmodified, regular text.
```


##### 2. Create an unordered list of your top 3 favorite academic interests (some examples could include: machine learning, operating systems, forensic accounting, etc.). Create another *ordered* list that ranks your academic interests in order of most interested to least interested.

**Hint:** *You can learn what ordered and unordered lists are [here](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).*

**Note:** *Similar to (1), in this question we are dealing with Markdown. If we were to copy and paste the solution to this problem in a Markdown editor, it would be the same result as when we Knit it here.*

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**
- Create the lists, this time don't surround your code in backticks. Note that when compiled, this text will appear as nice, formatted lists.
```

##### 3. Browse https://www.linkedin.com/ and read some profiles. Pay special attention to accounts with an "About" section. Write your own personal "About" section using Markdown. Include the following:

- A header for this section (your choice of size) that says "About".
- The text of your personal "About" section that you would feel comfortable uploading to linkedin, including at least 1 link.

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**
- Create the described profile, don't surround your code in backticks.
```


##### 4. LaTeX is a powerful editing tool where you can create beautifully formatted equations and formulas. Replicate the equation found [here](https://wikimedia.org/api/rest_v1/media/math/render/svg/87c061fe1c7430a5201eef3fa50f9d00eac78810) as closely as possible.

**Hint:** *Lookup "latex mid" and "latex frac".*

```{block, type="bbox"}
**Item(s) to submit:**

- Replicate the equation using LaTeX under the Question 4 header in your template.
```

##### 5. Your co-worker wrote a report, and has asked you to beautify it. Knowing Rmarkdown, you agreed. Make improvements to this section. At a minimum:

- Make the title pronounced.
- Make all links appear as a word or words, rather than the long-form URL.
- Organize all code into code chunks where code and output are displayed. If the output is really long, just display the code.
- Make the calls to the `library` function be evaluated but not displayed. 
- Make sure all warnings and errors that may eventually occur, do not appear in the final document.

Feel free to make any other changes that make the report more visually pleasing.

````markdown
`r ''````{r my-load-packages}
library(ggplot2)
```

`r ''````{r declare-variable-390, eval=FALSE}
my_variable <- c(1,2,3)
```

All About the Iris Dataset

This paper goes into detail about the `iris` dataset that is built into r. You can find a list of built-in datasets by visiting https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html or by running the following code:

data()

The iris dataset has 5 columns. You can get the names of the columns by running the following code:

names(iris)

Alternatively, you could just run the following code:

iris

The second option provides more detail about the dataset.

According to https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html there is another dataset built-in to r called `iris3`. This dataset is 3 dimensional instead of 2 dimensional.

An iris is a really pretty flower. You can see a picture of one here:

https://www.gardenia.net/storage/app/public/guides/detail/83847060_mOptimized.jpg

In summary. I really like irises, and there is a dataset in r called `iris`.
````

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**

- Make improvements to this section, and place it all under the Question 5 header in your template.
```



##### 6. Create a plot using a built-in dataset like `iris`, `mtcars`, or `Titanic`, and display the plot using a code chunk. Make sure the code used to generate the plot is hidden. Include a descriptive caption for the image. Make sure to use an RMarkdown chunk option to create the caption.

**Relevant topics:** *[rmarkdown](#r-rmarkdown), [plotting in r](#r-plotting)*

```{block, type="bbox"}
**Item(s) to submit:**

- Code chunk under that creates and displays a plot using a built-in dataset like `iris`, `mtcars`, or `Titanic`.
```

##### 7. Insert the following code chunk under the Question 7 header in your template. Try knitting the document. Two things will go wrong. What is the first problem? What is the second problem?
       
````markdown
```{r my-load-packages}`r ''`
plot(my_variable)
```
````

**Hint:** *Take a close look at the name we give our code chunk.*

**Hint:** *Take a look at the code chunk where `my_variable` is declared.*

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**

- The modified version of the inserted code that fixes both problems.
- A sentence explaining what the first problem was.
- A sentence explaining what the second problem was.
```


##### For Project 2, please submit your .Rmd file and the resulting .pdf file.  (For this project, you do not need to submit a .R file.)

##### OPTIONAL QUESTION. RMarkdown is also an excellent tool to create a slide deck. Use the information [here](https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf) or [here](https://thedatamine.github.io/the-examples-book/r.html#how-do-i-create-a-set-of-slides-using-rmarkdown) to convert your solutions into a slide deck rather than the regular PDF. You may experiment with `slidy`, `ioslides` or `beamer`, however, make your final set of solutions use `beamer` as the output is a PDF. Make any needed modifications to make the solutions knit into a well-organized slide deck (For example, include slide breaks and make sure the contents are shown completely.).  Modify (2) so the bullets are incrementally presented as the slides progress.

**Important note:** You do _not_ need to submit the original PDF for this project, just the `beamer` slide version of the PDF.

**Relevant topics:** *[rmarkdown](#r-rmarkdown)*

```{block, type="bbox"}
**Item(s) to submit:**

- The modified version of the solutions in `beamer` slide form.
```

---

### Project 3 {#p03-390}

---

**Motivation:** The ability to navigate a shell, like `bash`, and use some of its powerful tools, is very useful. The number of disciplines utilizing data in new ways is ever-growing, and as such, it is very likely that many of you will eventually encounter a scenario where knowing your way around a terminal will be useful. We want to expose you to some of the most useful `bash` tools, help you navigate a filesystem, and even run `bash` tools from within an RMarkdown file in RStudio.

**Context:** At this point in time, you will each have varying levels of familiarity with Scholar. In this project we will learn how to use the terminal to navigate a UNIX-like system, experiment with various useful commands, and learn how to execute bash commands from within RStudio in an RMarkdown file.

**Scope:** bash, RStudio

**Learning objectives:**

```{block, type="bbox"}
- Distinguish differences in /home, /scratch, and /class.
- Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc.
- Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc.
- Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc.
- Utilize other Scholar resources: rstudio.scholar.rcac.purdue.edu, notebook.scholar.rcac.purdue.edu, desktop.scholar.rcac.purdue.edu, etc.
- Use `man` to read and learn about UNIX utilities.
- Run `bash` commands from within and RMarkdown file in RStudio.
```

There are a variety of ways to connect to Scholar. In this class, we will _primarily_ connect to RStudio Server by opening a browser and navigating to https://rstudio.scholar.rcac.purdue.edu/, entering credentials, and using the excellent RStudio interface. 

Here is a video to remind you about some of the basic tools you can use in UNIX/Linux:

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/983291/sp/98329100/embedIframeJs/uiconf_id/29134031/partner_id/983291?iframeembed=true&playerId=kaltura_player&entry_id=1_9mz5s0wd&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_0y4x1feo" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Kaltura Player"></iframe>

This is the easiest book for learning this stuff; it is short and gets right to the point:

https://go.oreilly.com/purdue-university/library/view/-/0596002610

you just log in and you can see it all; we suggest Chapters 1, 3, 4, 5, 7 (you can basically skip chapters 2 and 6 the first time through).

It is a very short read (maybe, say, 2 or 3 hours altogether?), just a thin book that gets right to the details.

##### 1. Navigate to https://rstudio.scholar.rcac.purdue.edu/ and login. Take some time to click around and explore this tool. We will be writing and running Python, R, SQL, and `bash` all from within this interface. Navigate to `Tools > Global Options ...`. Explore this interface and make at least 2 modifications. List what you changed.

Here are some changes Kevin likes:

- Uncheck "Restore .Rdata into workspace at startup".
- Change tab width 4.
- Check "Soft-wrap R source files".
- Check "Highlight selected line".
- Check "Strip trailing horizontal whitespace when saving".
- Uncheck "Show margin".

(Dr Ward does not like to customize his own environment, but he does use the emacs key bindings: Tools > Global Options > Code > Keybindings, but this is only recommended if you already know emacs.)

```{block, type="bbox"}
**Item(s) to submit:**

- List of modifications you made to your Global Options.
```

##### 2. There are four primary panes, each with various tabs. In one of the panes there will be a tab labeled "Terminal". Click on that tab. This terminal by default will run a `bash` shell right within Scholar, the same as if you connected to Scholar using ThinLinc, and opened a terminal. Very convenient! 

##### What is the default directory of your bash shell? 

**Hint:** Start by reading the section on `man`. `man` stands for manual, and you can find the "official" documentation for the command by typing `man <command_of_interest>`. For example:

```{bash, eval=F}
# read the manual for the `man` command
# use "k" or the up arrow to scroll up, "j" or the down arrow to scroll down
man man 
```

**Relevant topics:** [man](#man), [pwd](#pwd), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- The full filepath of default directory (home directory). Ex: Kevin's is: `/home/kamstut`
- The `bash` code used to show your home directory or current directory (also known as the working directory) when the `bash` shell is first launched.
```

##### 3. Learning to navigate away from our home directory to other folders, and back again, is vital. Perform the following actions, in order:

- Write a single command to navigate to the folder containing our full datasets: `/class/datamine/data`. 
- Write a command to confirm you are in the correct folder. 
- Write a command to list the files and directories within the data directory. (You do not need to recursively list subdirectories and files contained therein.) What are the names of the files and directories?
- Write another command to return back to your home directory. 
- Write a command to confirm you are in the correct folder.

Note: `/` is commonly referred to as the root directory in a linux/unix filesystem. Think of it as a folder that contains _every_ other folder in the computer. `/home` is a folder within the root directory. `/home/kamstut` is the full filepath of Kevin's home directory. There is a folder `home` inside the root directory. Inside `home` is another folder named `kamstut` which is Kevin's home directory. 

**Relevant topics:** [man](#man), [cd](#cd), [pwd](#pwd), [ls](#ls), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to the data directory.
- Command used to confirm you are in the data directory.
- Command used to list files and folders.
- List of files and folders in the data directory.
- Command used to navigate back to the home directory.
- Commnad used to confirm you are in the home directory.
```

##### 4. Let's learn about two more important concepts. `.` refers to the current working directory, or the directory displayed when you run `pwd`. Unlike `pwd` you can use this when navigating the filesystem! So, for example, if you wanted to see the contents of a file called `my_file.txt` that lives in `/home/kamstut` (so, a full path of `/home/kamstut/my_file.txt`), and you are currently in `/home/kamstut`, you could run: `cat ./my_file.txt`. 

##### `..` represents the parent folder or the folder in which your current folder is contained. So let's say I was in `/home/kamstut/projects/` and I wanted to get the contents of the file `/home/kamstut/my_file.txt`. You could do: `cat ../my_file.txt`. 

##### When you navigate a directory tree using `.` and  `..` you create paths that are called _relative_ paths because they are _relative_ to your current directory. Alternatively, a _full_ path or (_absolute_ path) is the path starting from the root directory. So `/home/kamstut/my_file.txt` is the _absolute_ path for `my_file.txt` and `../my_file.txt` is a _relative_ path. Perform the following actions, in order:

- Write a single command to navigate to the data directory.
- Write a single command to navigate back to your home directory using a _relative_ path. Do not use `~` or the `cd` command without a path argument.

**Relevant topics:** [man](#man), [cd](#cd), [pwd](#pwd), [ls](#ls), [~](#dots), [..](#dots), [.](#dots)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to the data directory.
- Command used to navigate back to your home directory that uses a _relative_ path.
```

##### 5. In Scholar, when you want to deal with _really_ large amounts of data, you want to access scratch (you can read more [here](https://www.rcac.purdue.edu/policies/scholar/)). Your scratch directory on Scholar is located here: `/scratch/scholar/$USER`. `$USER` is an environment variable containing your username. Test it out: `echo /scratch/scholar/$USER`. Perform the following actions:

- Navigate to your scratch directory. 
- Confirm you are in the correct location.
- Execute `myquota`.
- Find the location of the `myquota` bash script.
- Output the first 5 and last 5 lines of the bash script. 
- Count the number of lines in the bash script.
- How many kilobytes is the script?

**Hint:** You could use each of the commands in the relevant topics once.

**Hint:** When you type `myquota` on Scholar there are sometimes two warnings about `xauth` but sometimes there are no warnings.  If you get a warning that says `Warning: untrusted X11 forwarding setup failed: xauth key data not generated` it is safe to ignore this error.

**Hint:** Commands often have _options_. _Options_ are features of the program that you can trigger specifically. You can see the _options_ of a command in the `DESCRIPTION` section of the `man` pages. For example: `man wc`. You can see `-m`, `-l`, and `-w` are all options for `wc`. To test this out:

```{bash, eval=F}
# using the default wc command. "/class/datamine/data/flights/1987.csv" is the first "argument" given to the command.
wc /class/datamine/data/flights/1987.csv
# to count the lines, use the -l option
wc -l /class/datamine/data/flights/1987.csv
# to count the words, use the -w option
wc -w /class/datamine/data/flights/1987.csv
# you can combine options as well
wc -w -l /class/datamine/data/flights/1987.csv
# some people like to use a single tack `-`
wc -wl /class/datamine/data/flights/1987.csv
# order doesn't matter
wc -lw /class/datamine/data/flights/1987.csv
```

**Hint:** The `-h` option for the `du` command is useful.

**Relevant topics:** [cd](#cd), [pwd](#pwd), [type](#type), [head](#head), [tail](#tail), [wc](#wc), [du](#du)

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to your scratch directory.
- Command used to confirm your location.
- Output of `myquota`.
- Command used to find the location of the `myquota` script.
- Absolute path of the `myquota` script.
- Command used to output the first 5 lines of the `myquota` script.
- Command used to output the last 5 lines of the `myquota` script.
- Command used to find the number of lines in the `myquota` script.
- Number of lines in the script.
- Command used to find out how many kilobytes the script is.
- Number of kilobytes that the script takes up.
```

##### 6. Perform the following operations:

- Navigate to your scratch directory.
- Copy and paste the file: `/class/datamine/data/flights/1987.csv` to your current directory (scratch).
- Create a new directory called `my_test_dir` in your scratch folder.
- Move the file you copied to your scratch directory, into your new folder.
- Use `touch` to create an empty file named `im_empty.txt` in your scratch folder.
- Remove the directory `my_test_dir` _and_ the contents of the directory.
- Remove the `im_empty.txt` file.

**Hint:** `rmdir` may not be able to do what you think, instead, check out the options for `rm` using `man rm`.

**Relevant topics:** [cd](#cd), [cp](#cp), [mv](#mv), mkdir, touch, rmdir, rm

```{block, type="bbox"}
**Item(s) to submit:**

- Command used to navigate to your scratch directory.
- Command used to copy the file, `/class/datamine/data/flights/1987.csv` to your current directory (scratch).
- Command used to create a new directory called `my_test_dir` in your scratch folder.
- Command used to move the file you copied earlier `1987.csv` into your new `my_test_dir` folder.
- Command used to create an empty file named `im_empty.txt` in your scratch folder.
- Command used to remove the directory _and_ the contents of the directory `my_test_dir`.
- Command used to remove the `im_empty.txt` file.
```

##### 7. Please include a statement in Project 3 that says, "I acknowledge that the STAT 19000/29000/39000 1-credit Data Mine seminar will be recorded and posted on Piazza, for participants in this course." or if you disagree with this statement, please consult with us at datamine@purdue.edu for an alternative plan.

---

### Project 4 {#p04-390}

---

**Motivation:** The need to search files and datasets based on the text held within is common during various parts of the data wrangling process. `grep` is an extremely powerful UNIX tool that allows you to do so using regular expressions. Regular expressions are a structured method for searching for specified patterns. Regular expressions can be very complicated, [even professionals can make critical mistakes](https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/). With that being said, learning some of the basics is an incredible tool that will come in handy regardless of the language you are working in.

**Context:** We've just begun to learn the basics of navigating a file system in UNIX using various terminal commands. Now we will go into more depth with one of the most useful command line tools, `grep`, and experiment with regular expressions using `grep`, R, and later on, Python.

**Scope:** grep, regular expression basics, utilizing regular expression tools in R and Python

**Learning objectives:**

```{block, type="bbox"}
- Use `grep` to search for patterns within a dataset.
- Use `cut` to section off and slice up data from the command line.
- Use `wc` to count the number of lines of input.
```

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

**Important note:** I would highly recommend using single quotes `'` to surround your regular expressions. Double quotes can have unexpected behavior due to some shell's expansion rules. In addition, pay close attention to [escaping](#faq-escape-characters) certain [characters](https://unix.stackexchange.com/questions/20804/in-a-regular-expression-which-characters-need-escaping) in your regular expressions. 

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/movies_and_tv/the_office_dialogue.csv`

A public sample of the data can be found here: [the_office_dialogue.csv](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/the_office_dialogue.csv)

Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset.

`grep` stands for (g)lobally search for a (r)egular (e)xpression and (p)rint matching lines. As such, to best demonstrate `grep`, we will be using it with textual data. You can read about and see examples of `grep` [here](https://thedatamine.github.io/the-examples-book/unix.html#grep).

##### 1. Login to Scholar and use `grep` to find the dataset we will use this project. The dataset we will use is the only dataset to have the text "Bears. Beets. Battlestar Galactica.". What is the name of the dataset and where is it located?

**Relevant topics:** [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- The `grep` command used to find the dataset.
- The name and location in Scholar of the dataset.
- Use `grep` and `grepl` within R to solve a data-driven problem.
```

##### 2. `grep` prints the line that the text you are searching for appears in. In project 3 we learned a UNIX command to quickly print the first _n_ lines from a file. Use this command to get the headers for the dataset. As you can see, each line in the tv show is a row in the dataset. You can count to see which column the various bits of data live in.

##### Write a line of UNIX commands that searches for "bears. beets. battlestar galactica." and, rather than printing the entire line, prints only the character who speaks the line, as well as the line itself.

**Hint:** *The result if you were to search for "bears. beets. battlestar galactica." should be:*

```{txt}
"Jim","Fact. Bears eat beets. Bears. Beets. Battlestar Galactica."
```

**Hint:** *One method to solve this problem would be to [pipe](https://thedatamine.github.io/the-examples-book/unix.html#piping-and-redirection) the output from `grep` to [`cut`](https://thedatamine.github.io/the-examples-book/unix.html#cut).*

**Relevant topics:** [cut](#cut), [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to find the character and original dialogue line that contains "bears. beets. battlestar galactica.".
```

##### 3. Find all of the lines where Pam is called "Beesley" instead of "Pam" or "Pam Beesley".

**Hint:** *A negative lookbehind would be one way to solve this, in order to use a negative lookbehind with `grep` make sure to add the -P option. In addition, make sure to use single quotes to make sure your regular expression is taken literally. If you use double quotes, variables are expanded.*

**Relevant topics:** [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- The UNIX command used to solve this problem.
```

Regular expressions are really a useful semi language-agnostic tool. What this means is regardless of the programming language your are using, there will be some package that allows you to use regular expressions. In fact, we can use them in both R and Python! This can be particularly useful when dealing with strings. Load up the dataset you discovered in (1) using `read.csv`. Name the resulting data.frame `dat`.

##### 4. The `text_w_direction` column in `dat` contains the characters' lines with inserted direction that helps characters know what to do as they are reciting the lines. Direction is shown between square brackets "[" "]". In this two-part question, we are going to use regular expression to detect the directions.

##### (a) Create a new column called `has_direction` that is set to `TRUE` if the `text_w_direction` column has direction, and `FALSE` otherwise. Use the `grepl` function in R to accomplish this.

**Hint:** *Make sure all opening brackets "[" have a corresponding closing bracket "]".*

**Hint:** *Think of the pattern as any line that has a [, followed by any amount of any text, followed by a ], followed by any amount of any text.*

##### (b) Modify your regular expression to find lines with 2 or more sets of direction. How many lines have more than 2 directions? Modify your code again and find how many have more than 5.

We count the sets of direction in each line by the pairs of square brackets. The following are two simple example sentences.

```{txt}
This is a line with [emphasize this] only 1 direction!
This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug].
```

Your solution to part (a) should find both lines a match. However, in part (b) we want the regular expression pattern to find only lines with 2+ directions, so the first line would not be a match.

In our actual dataset, for example, `dat$text_w_direction[2789]` is a line with 2 directions.

**Relevant topics:** *[grep](#r-grep), [grepl](#r-grep), [basic matches](https://r4ds.had.co.nz/strings.html#basic-matches), [escaping characters](#faq-escape-characters)*

```{block, type="bbox"}
**Item(s) to submit:**

- The R code and regular expression used to solve the first part of this problem.
- The R code and regular expression used to solve the second part of this problem.
- How many lines have >= 2 directions?
- How many lines have >= 5 directions?
```

##### 5. Use the `str_extract_all` function from the `stringr` package to extract the direction(s) as well as the text between direction(s) from each line. Put the strings in a new column called `direction`.

```{txt}
This is a line with [emphasize this] only 1 direction!
This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug].
```

In this question, your solution may have extracted:

```{txt}
[emphasize this]
[emphasize this] 2 sets of direction, do you see the difference [shrug]
```

(It is okay to keep the text between neighboring pairs of "[" and "]" for the second line.)

**Relevant topics:** *[str_extract_all](#r-str-extract), [basic matches](https://r4ds.had.co.nz/strings.html#basic-matches), [escaping characters](#faq-escape-characters)*

```{block, type="bbox"}
**Item(s) to submit:**

- The R code used to solve this problem.
```

##### OPTIONAL QUESTION. Repeat (5) but this time make sure you only capture the brackets and text within the brackets. Save the results in a new column called `direction_correct`. You can test to see if it is working by running the following code:

```{r, eval=F}
dat$direction_correct[747]
```

```{txt}
This is a line with [emphasize this] only 1 direction!
This is a line with [emphasize this] 2 sets of direction, do you see the difference [shrug].
```

In (5), your solution may have extracted:

```{txt}
[emphasize this]
[emphasize this] 2 sets of direction, do you see the difference [shrug]
```

This is ok for (5). In this question, however, we want to fix this to only extract:

```{txt}
[emphasize this]
[emphasize this] [shrug]
```

**Hint:** *This regular expression will be hard to read.*

**Hint:** *The pattern we want is: literal opening bracket, followed by 0+ of any character other than the literal [ or literal ], followed by a literal closing bracket.*

**Relevant topics:** *[str_extract_all](#r-str-extract)*

```{block, type="bbox"}
**Item(s) to submit:**

- The R code used to solve this problem.
```

---

### Project 5 {#p05-390}

---

**Motivation:** Becoming comfortable stringing together commands and getting used to navigating files in a terminal is important for every data scientist to do. By learning the basics of a few useful tools, you will have the ability to quickly understand and manipulate files in a way which is just not possible using tools like Microsoft Office, Google Sheets, etc.

**Context:** We've been using UNIX tools in a terminal to solve a variety of problems. In this project we will continue to solve problems by combining a variety of tools using a form of redirection called piping.

**Scope:** grep, regular expression basics, UNIX utilities, redirection, piping

**Learning objectives:**

```{block, type="bbox"}
- Use `cut` to section off and slice up data from the command line.
- Use piping to string UNIX commands together.
- Use `sort` and it's options to sort data in different ways.
- Use `head` to isolate _n_ lines of output.
- Use `wc` to summarize the number of lines in a file or in output.
- Use `uniq` to filter out non-unique lines.
- Use `grep` to search files effectively.
```

You can find useful examples that walk you through relevant material in The Examples Book:

https://thedatamine.github.io/the-examples-book

It is highly recommended to read through, search, and explore these examples to help solve problems in this project.

Don't forget the very useful documentation shortcut `?` for R code. To use, simply type `?` in the console, followed by the name of the function you are interested in. In the Terminal, you can use the `man` command to check the documentation of `bash` code.

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/amazon/amazon_fine_food_reviews.csv`

A public sample of the data can be found here: [amazon_fine_food_reviews.csv](https://www.datadepot.rcac.purdue.edu/datamine/amazon/amazon_fine_food_reviews.csv)

Answers to questions should all be answered using the full dataset located on Scholar. You may use the public samples of data to experiment with your solutions prior to running them using the full dataset.

#### Questions

##### 1. What is the `Id` of the most helpful review if we consider the review with highest `HelpfulnessNumerator` to be an indicator of helpfulness (higher is more helpful)?

**Important note:** You can always pipe output to `head` in case you want the first few values of a lot of output. Note that if you used `sort` before `head`, you may see the following error messages:

````
sort: write failed: standard output: Broken pipe
sort: write error
````

This is because `head` would truncate the output from `sort`. This is okay. See [this discussion](https://stackoverflow.com/questions/46202653/bash-error-in-sort-sort-write-failed-standard-output-broken-pipe) for more details.

**Relevant topics:** *[cut](#cut), [sort](#sort), [head](#head), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
- The `Id` of the most helpful review.
```

##### 2. Some entries under the `Summary` column appear more than once. Calculate the proportion of unique summaries over the total number of summaries. Use two lines of UNIX commands to find the numerator and the denominator, and manually calculate the proportion.

##### To further clarify what we mean by _unique_, if we had the following vector in R, `c("a", "b", "a", "c")`, its unique values are `c("a", "b", "c")`. 

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [wc](#wc), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- Two lines of UNIX commands used to solve the problem.
- The ratio of unique `Summary`'s.
```

##### 3. Use a simple UNIX command to create a frequency table of `Score`.

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
- The frequency table.
```

##### 4. Who is the user with the highest number of reviews? There are two columns you could use to answer this question, but which column do you think would be most appropriate and why?

**Hint:** *You may need to pipe the output to `sort` multiple times.*

**Hint:** *To create the frequency table, read through the `man` pages for `uniq`. Man pages are the "manual" pages for UNIX commands. You can read through the man pages for uniq by running the following:*

```{bash, eval=F}
man uniq
```

**Relevant topics:** *[cut](#cut), [uniq](#uniq), [sort](#sort), [head](#head), [piping](#piping-and-redirection), [man](#man)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
- The frequency table.
```

##### 5. Anecdotally, there seems to be a tendency to leave reviews when we feel strongly (either positive or negative) about a product. For the user with the highest number of reviews, would you say that they follow this pattern of extremes? Let's consider 5 star reviews to be strongly positive and 1 star reviews to be strongly negative. Let's consider anything in between neither strongly positive nor negative.

**Hint:** *You may find the solution to problem (3) useful.*

**Relevant topics:** *[cut](#cut), [uniq](https://thedatamine.github.io/the-examples-book/unix.html#uniq), [sort](#sort), [grep](#grep), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
```

##### 6. We want to compare the most helpful review with a `Score` of 5 with the most helpful review with a `Score` of 1. Use UNIX commands to calculate these values. Write down the `ProductId` of both reviews. In the case of a tie, write down all `ProductId`'s to get full credit. In this case we are considering the most helpful review to be the review with the highest `HelpfulnessNumerator`.

**Hint:** *You can use multiple lines to solve this problem.*

**Relevant topics:** *[sort](#sort), [head](#head), [piping](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The lines of UNIX commands used to solve the problem.
- `ProductId`'s of both requested reviews.
```

##### 7. Using the `ProductId`'s from the previous question, create a new dataset called `reviews.csv` which contains the `ProductId`'s and `Score` of all reviews with the corresponding `ProductId`'s.  (Remember, a good place to store data temporarily is `/scratch/scholar/$USER`.)   

**Relevant topics:** *[cut](#cut), [grep](#head), [redirection](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
```

##### 8. If we didn't use `cut` prior to searching for the `ProductId`'s in (7), we would get unwanted results. Modify the solution to (7) and explore. What is happening?

**Relevant topics:** *[cat](#cat), [grep](#head), [redirection](#piping-and-redirection)*

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands used to solve the problem.
- 1-2 sentences explaining why we need to use `cut` first.
```

##### 9. Use R to load up `reviews.csv` into a new data.frame called `dat`. Create a histogram for each products' `Score`. Compare the most helpful review `Score` with the `Score`'s given in the histogram. Based on this comparison, point out some curiosities about the product that may be worth exploring. For example, if a product receives many high scores, but has a super helpful review that gives the product 1 star, I may tend to wonder if the product is not as great as it seems to be.

**Relevant topics:** *[read.csv](#r-reading-and-writing-data), [hist](#r-plotting)*

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to create the histograms.
- 3 histograms, 1 for each `ProductId`.
- 1-2 sentences describing the curious pattern that you would like to further explore.
```

---

### Project 6 {#p06-390}

---

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
- Use output created from the terminal to create a plot using R.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:
`/class/datamine/data/flights/subset/YYYY.csv` 

An example of the data for the year 1987 can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv).

#### Questions

##### 1. In previous projects we learned how to get a single column of data from a csv file. Write 1 line of UNIX commands to print the 17th column, the `Origin`, from `1987.csv`. Write another line, this time using `awk` to do the same thing. Which one do you prefer, and why?

**Relevant topics:** [cut](#cut), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- One line of UNIX commands to solve the problem _without_ using `awk`.
- One line of UNIX commands to solve the problem using `awk`.
- 1-2 sentences describing which method you prefer and why.
```

##### 2. Write a bash script that accepts a year (1987, 1988, etc.) and a column *n* and returns the *nth* column of the associated year of data.

**Relevant topics:** [awk](#awk), [bash scripts](#writing-scripts)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
```

##### 3. How many flights came into Indianapolis (IND) in 2008? First solve this problem without using `awk`, then solve this problem using *only* `awk`.

**Relevant topics:** [cut](#cut), [grep](#grep), [wc](#wc), [awk](#awk), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- One line of UNIX commands to solve the problem *without* using `awk`.
- One line of  UNIX commands to solve the problem using `awk`. 
- The number of flights that came into Indianapolis (IND) in 2008.
```

##### 4. Do you expect the number of unique origins and destinations to be the same? Find out using any command line tool you'd like. Are they indeed the same? How many unique values do we have per category (`Origin`, `Dest`)?

**Relevant topics:** [cut](#cut), [sort](#sort), [uniq](#uniq), [wc](#wc), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- 1-2 sentences explaining whether or not you expect the number of unique origins and destinations to be the same.
- The UNIX command(s) used to figure out if the number of unique origins and destinations are the same. 
- The number of unique values per category (`Origin`, `Dest`).
```

##### 5. In (4) we found that there are not the same number of unique `Origin`'s as `Dest`'s. Find the IATA airport code for all `Origin`'s that dont appear in a `Dest` and all `Dest`'s that don't appear in an `Origin`.

**Hint:** https://www.tutorialspoint.com/unix_commands/comm.htm

**Relevant topics:** [comm](#unix), [cut](#cut), [sort](#sort), [uniq](#uniq), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The line(s) of UNIX command(s) used to answer the question.
- The list of `Origin`s that don't appear in `Dest`.
- The list of `Dest`s that don't appear in `Origin`.
```

##### 6. What was the average number of flights in 2008 per unique `Origin` with the `Dest` of "IND"? How does "PHX" (as a unique `Origin`) compare to the average?

**Hint:** You manually do the average calculation by dividing the result from (3) by the number of unique `Origin`'s that have a `Dest` of "IND".

**Relevant topics:** [awk](#awk), [sort](#sort), [grep](#grep), [wc](#wc)

```{block, type="bbox"}
**Item(s) to submit:**

- The average number of flights in 2008 per unique `Origin` with the `Dest` of "IND".
- 1-2 sentences explaining how "PHX" compares (as a unique `Origin`) to the average?
```

##### 7. Write a bash script that takes a year and IATA airport code and returns the year, and the total number of flights to and from the given airport. Example rows may look like:

```{txt, eval=F}
1987, 12345
1988, 44
```

Run the script with inputs: `1991` and `ORD`. Include the output in your submission.

**Relevant topics:** [bash scripts](#writing-scripts), [cut](#cut), [piping](#piping-and-redirection), [grep](#grep), [wc](#wc)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The output of the script given `1991` and `ORD` as inputs.
```

##### 8. Pick your favorite airport and get its IATA airport code. Write a bash script that, given the first year, last year, and airport code, runs the bash script from (7) for all years in the provided range for your given airport, or loops through all of the files for the given airport, appending all of the data to a new file called `my_airport.csv`.

**Relevant topics:** [bash scripts](#writing-scripts), [cut](#cut), [grep](#grep), [wc](#wc), [for loops](#r-for-loops), [echo](#echo), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
```

##### 9. In R, load `my_airport.csv` and create a line plot showing the year-by-year change. Label your x-axis "Year", your y-axis "Num Flights", and your title the name of the IATA airport code. Write 1-2 sentences with your observations.

**Relevant topics:** [read.csv](#r-reading-and-writing-data), [lines](#r-lines)

```{block, type="bbox"}
**Item(s) to submit:**

- Line chart showing year-by-year change in flights into and out of the chosen airport.
- R code used to create the chart.
- 1-2 sentences with your observations.
```

---

### Project 7 {#p07-390}

---

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/flights/subset/YYYY.csv` 

An example of the data for the year 1987 can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv).

Sometimes if you are about to dig into a dataset, it is good to quickly do some sanity checks early on to make sure the data is what you expect it to be. 

#### Questions

##### 1. Write a line of code that prints a list of the unique values in the `DayOfWeek` column. Write a line of code that prints a list of the unique values in the `DayOfMonth` column. Write a line of code that prints a list of the unique values in the `Month` column. Use the `1987.csv` dataset. Are the results what you expected?

**Relevant topics:** [cut](#cut), [sort](#sort)

```{block, type="bbox"}
**Item(s) to submit:**

- 3 lines of code used to get a list of unique values for the chosen columns.
- 1-2 sentences explaining whether or not the results are what you expected.
```

##### 2. Our files should have 29 columns. Write a line of code that prints any lines in a file that do *not* have 29 columns. Test it on `1987.csv`, were there any rows without 29 columns?

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of code used to solve the problem.
- 1-2 sentences explaining whether or not there were any rows without 29 columns.
```

##### 3. Write a bash script that, given a "begin" year and "end" year, cycles through the associated files and prints any lines that do *not* have 29 columns.

**Relevant topics:** [awk](#awk), [bash scripts](#writing-scripts)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The results of running your bash scripts from year 1987 to 2008.
```

##### 4. `awk` is a really good tool to quickly get some data and manipulate it a little bit. For example, let's see the number of kilometers and miles traveled in 1990. To convert from miles to kilometers, simply multiply by 1.609344. 

Example output:

```txt
Miles: 12345
Kilometers: 19867.35168
```

**Relevant topics:** [awk](#awk), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem. 
- The results of running the code.
```

##### 5. Use `awk` to calculate the number of `DepDelay` minutes by `DayOfWeek`. Use `2007.csv`.

Example output:

```txt
DayOfWeek:  0
1:  1234567
2:  1234567
3:  1234567
4:  1234567
5:  1234567
6:  1234567
7:  1234567
```

Note: 1 is Monday.

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

##### 6. It wouldn't be fair to compare the total `DepDelay` minutes by `DayOfWeek` as the number of flights may vary. One way to take this into account is to instead calculate an average. Modify (5) to calculate the average number of `DepDelay` minutes by the number of flights per `DayOfWeek`. Use `2007.csv`.

Example output:

```txt
DayOfWeek:  0
1:  1.234567
2:  1.234567
3:  1.234567
4:  1.234567
5:  1.234567
6:  1.234567
7:  1.234567
```

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

##### 7. As a quick follow-up, _slightly_ modify (6) to perform the same calculation for `ArrDelay`. Do the `ArrDelay`s and `DepDelay`s appear to have the highest delays on the same day? Use `2007.csv`.

Example output:

```txt
DayOfWeek:  0
1:  1.234567
2:  1.234567
3:  1.234567
4:  1.234567
5:  1.234567
6:  1.234567
7:  1.234567
```

**Relevant topics:** [awk](#awk), [sort](#sort), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
- 1-2 sentences explaining whether or not the `ArrDelay`s and `DepDelay`s appear to have the highest delays on the same day.
```

##### 8. Anyone who has flown knows how frustrating it can be waiting for takeoff, or deboarding the aircraft. These roughly translate to `TaxiOut` and `TaxiIn` respectively. If you were to fly into or out of IND what is your expected total taxi time? Use `2007.csv`.

Note: Taxi times are in minutes.

**Relevant topics:** [awk](#awk), [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

##### 9.  What are the IATA airport codes of the 5 airports with the greatest total taxi time for 2007? Show the total taxi time for each.

Example output:

```txt
DayOfWeek:  0
IND: 1234567
IND: 1234567
IND: 1234567
IND: 1234567
IND: 1234567
```

**Relevant topics:** [awk](#awk), [head](#head), [sort](#sort)

```{block, type="bbox"}
**Item(s) to submit:**

- The code used to solve the problem.
- The output from running the code.
```

---

### Project 8 {#p08-390}

--- 

**Motivation:** A bash script is a powerful tool to perform repeated tasks. RCAC uses bash scripts to automate a variety of tasks. In fact, we use bash scripts on Scholar to do things like link Python kernels to your account, fix potential isues with Firefox, etc. `awk` is a programming language designed for text processing. The combination of these tools can be really powerful and useful for a variety of quick tasks.

**Context:** This is the first part in a series of projects that are designed to exercise skills around UNIX utilities, with a focus on writing bash scripts and `awk`. You will get the opportunity to manipulate data without leaving the terminal. At first it may seem overwhelming, however, with just a little practice you will be able to accomplish data wrangling tasks really efficiently. 

**Scope:** awk, UNIX utilities, bash scripts

**Learning objectives:**

```{block, type="bbox"}
- Use `awk` to process and manipulate textual data.
- Use piping and redirection within the terminal to pass around data between utilities.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/flights/subset/YYYY.csv` 

An example of the data for the year 1987 can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/flights/subset/1987.csv).

Let's say we have a theory that there are more flights on the weekend days (Friday, Saturday, Sunday) than the rest of the days, on average. We can use awk to quickly check it out and see if maybe this looks like something that is true!

##### 1. Write a line of `awk` code that, prints the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code that solves the problem.
- The result: the number of flights on the weekend days, followed by the number of flights on the weekdays for the flights during 2008.
```

##### 2. Note that in (1), we are comparing 3 days to 4! Write a line of `awk` code that, prints the average number of flights on a weekend day, followed by the average number of flights on the weekdays. Continue to use data for 2008.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code that solves the problem.
- The result: the average number of flights on the weekend days, followed by the average number of flights on the weekdays for the flights during 2008.
```

We want to look to see if there may be some truth to the whole "snow bird" concept where people will travel to warmer states like Florida and Arizona during the Winter. Let's use the tools we've learned to explore this a little bit. 

##### 3. Take a look at `airports.csv`. In particular run the following:

```{bash, eval=F}
head airports.csv
```

Notice how all of the non-numeric text is surrounded by quotes. The surrounding quotes would need to be escaped for any comparison within `awk`. This is messy and we would prefer to create a new file called `new_airports.csv` without any quotes. Write a line of code to do this. 

**Hint:** You could use `gsub` within `awk` to replace '"' with ''.

**Hint:** If you leave out the column number argument to `gsub` it will apply the substitution to every field in every column.

**Relevant topics:** [awk](#awk), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of `awk` code used to create the new dataset.
```

##### 4. Write a line of commands that create a new dataset called `az_fl_airports.txt` that contains a list of airport codes for all airports from both Arizona (AZ) and Florida (FL). Use the file we created in (3),`new_airports.csv`.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- The line of UNIX commands to create an array called `airports`.
```

##### 5. Wow! In (4) we discovered a lot of airports! How many airports are there? Did you expect this? Use a line of bash code to answer this question.

**Relevant topics:** [echo](#echo), [wc](#wc), [piping](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
- The number of airports.
- 1-2 sentences explaining whether you expected this result and why or why not.
```

##### 6. Create a new dataset that contains all of the data for flights into or out of Florida and Arizona using 2008.csv, use the newly created dataset, `az_fl_airports.txt` in (4) to do so.

**Hint:** https://unix.stackexchange.com/questions/293684/basic-grep-awk-help-extracting-all-lines-containing-a-list-of-terms-from-one-f

**Relevant topics:** [grep](#grep)

```{block, type="bbox"}
**Item(s) to submit:**

- Line of UNIX commands used to solve the problem.
```

##### 7. Now that you have code to complete (6), write a bash script that accepts the start year, end year, and filename containing airport codes (`az_fl_airports.txt`), and outputs the data for flights into or out of any of the airports listed in the provided filename containing airport codes using _all_ of the years of data in the provided range. Run the bash script to create a new file called `az_fl_flights.csv`.

**Relevant topics:** [bash scripts](#writing-scripts), [grep](#grep), [for loop](#r-for-loops), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script (starting with "#!/bin/bash") in a code chunk.
- The line of UNIX code you used to execute the script and create the new dataset.
```

##### 8. Use the newly created `az_fl_flights.csv` dataset to calculate the total number of flights into and out of both states by month, and by year, for a total of 3 columns (year, month, flights). Export this information to a new file called `snowbirds.csv`.

**Relevant topics:** [awk](#awk), [redirection](#piping-and-redirection)

```{block, type="bbox"}
**Item(s) to submit:**

- The line of `awk` code used to create the new dataset, `snowbirds.csv`.
```

##### 9. Load up your newly created dataset and use either R or Python (or some other tool) to create a graphic that illustrates whether or not we believe the "snowbird effect" effects flights. Include a description of your graph, as well as your (anecdotal) conclusion.

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to create the visualization in a code chunk.
- The generated plot as either a png or jpg/jpeg.
- 1-2 sentences describing your plot and your conclusion.
```

---

### Project 9 {#p09-390}

---

**Motivation:** Structured Query Language (SQL) is a language used for querying and manipulating data in a database. SQL can handle much larger amounts of data than R and Python can alone. SQL is incredibly powerful. In fact, [cloudflare](https://www.cloudflare.com/), a billion dollar company, had much of its starting infrastructure built on top of a Postgresql database (per [this thread on hackernews](https://news.ycombinator.com/item?id=22878136)). Learning SQL is _well_ worth your time!

**Context:** There are a multitude of RDBMSs (relational database management systems). Among the most popular are: MySQL, MariaDB, Postgresql, and SQLite. As we've spent much of this semester in the terminal, we will start in the terminal using SQLite. 

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Explain the advantages and disadvantages of using a database over a tool like a spreadsheet.
- Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/lahman/lahman.db` 

#### Questions

##### 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and navigate to the terminal and access the Lahman database. How many tables are available?

**Hint:** To connect to the database, do the following:

```{bash, eval=F}
sqlite3 /class/datamine/data/lahman/lahman.db
```

**Relevant topics:** [sqlite3](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- How many tables are available in the Lahman database?
- The sqlite3 commands used to figure out how many tables are available.
```

##### 2. Some people like to try to [visit all 30 MLB ballparks](https://www.washingtonpost.com/graphics/2017/sports/how-many-mlb-parks-have-you-visited/) in their lifetime.  Use SQL commands to get a list of `parks` and the cities they're located in. For your final answer, limit the output to 10 records/rows.

**Note:** There may be more than 30 parks in your result, this is ok. For long results, you can limit the number of printed results using the `LIMIT` clause.

**Hint:** Make sure you take a look at the data dictionary for the table and column names.

**Hint:** To see the header row as a part of each query result, run the following:

```{sql, eval=F}
.headers on
```

**Relevant topics:** [SELECT](#sql-examples), [FROM](#sql-examples), [LIMIT](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 3. There is nothing more exciting to witness than a home run hit by a batter. It's impressive if a player hits more than 40 in a season. Find the hitters who have hit 60 or more home runs (`HR`) in a season. List their `playerID`, `yearID`, home run total, and the `teamID` they played for.

**Hint:** There are 8 occurrences of home runs greater than 60.

**Hint:** The `batting` table is where you should look for this question.

**Relevant topics:** [SELECT](#sql-examples), [FROM](#sql-examples), [LIMIT](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 4. Make a list of players born on your birth day (don't worry about the year). Display their first names, last names, and birth year. Order the list descending by their birth year.

**Hint:** The `people` table is where you should look for this question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 5. Get the Cleveland (CLE) Pitching Roster from the 2016 season (`playerID`, `W`, `L`, `SO`). Order the pitchers by number of Strikeouts (SO).

**Hint:** The `pitching` table is where you should look for this question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 6. Find the top 10 team total of Errors between 1960 and 1970. Display their Win and Loss totals too. What is the name of the 3rd place team?

**Hint:** The `BETWEEN` clause is useful here.

**Hint:** It is OK to use multiple queries to answer the question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query. 
```

##### 7. Find the `playerID` for Bob Lemon. What year and team was he on when he pitched the most wins (use table `pitching`)? What year and team did he win the most games as a manager (use table `managers`)?

**Hint:** It is OK to use multiple queries to answer the question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

##### 8. Find the AL West (use `lgID` and `divID` to specify AL West) home run (`HR`), walk (`BB`), and stolen base (`SB`) totals by team between 2000 and 2010. Which team led in each category in the decade?

**Hint:** It is OK to use multiple queries to answer the question.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

##### 9. Get a list of the following by year: wins (`W`), losses (`L`), Home Runs Hit (`HR`), homeruns allowed (`HRA`), and attendance for the Detroit Tigers when appearing in a World Series (`WSWin`) or when league champion (`LgWin`).

**Hint:** Be careful with the order of operations for `AND` and `OR`. Remember you can force order of operations using parentheses.

**Relevant topics:** SELECT, FROM, WHERE, AND, ORDER BY, DESC, LIMIT, BETWEEN

**Note**: Examples that utilize the relevant topics in this problem can be found [here](#sql-examples).

```{block, type="bbox"}
**Item(s) to submit:**

- SQL code used to solve the problem.
- The first 10 results of the query.
```

---

### Project 10 {#p10-390}

---

**Motivation:** Although SQL syntax may still feel unnatural and foreign, with more practice it _will_ start to make more sense. The ability to read and write SQL queries is a bread-and-butter skill for anyone working with data. 

**Context:** We are in the second of a series of projects that focus on learning the basics of SQL. In this project, we will continue to harden our understanding of SQL syntax, and introduce common SQL functions like `AVG`, `MIN`, and `MAX`.

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Explain the advantages and disadvantages of using a database over a tool like a spreadsheet.
- Describe basic database concepts like: rdbms, tables, indexes, fields, query, clause.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
- Utilize SQL functions like min, max, avg, sum, and count to solve data-driven problems.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/lahman/lahman.db`

#### Questions 

##### 1. Connect to RStudio Server https://rstudio.scholar.rcac.purdue.edu, and, rather than navigating to the terminal like we did in the previous project, instead, create a connection to our MariaDB lahman database using the `RMariaDB` package in R, and the credentials below. Confirm the connection by running the following code chunk:

```{r, eval=F}
host <- "scholar-db.rcac.purdue.edu"
dbname <- "lahmandb"
user <- "lahman_user"
password <- "HitAH0merun"
head(dbGetQuery(con, "SHOW tables;"))
```

**Hint:** In the example provided, the variable `con` is the connection. Change `con` to whatever you name the result of `dbConnect`.

**Relevant topics:** [RMariaDB](#sql), [dbConnect](#sql), [dbGetQuery](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- Output from running your (potentially modified) `head(dbGetQuery(con, "SHOW tables;"))`.
```

##### 2. Find Corey Kluber's totals for his career. Include his strikeouts (`SO`), walks (`BB`), and his Strikeouts to Walks ratio. A Strikeout to Walks ratio is calculated by this equation: $\frac{Strikeouts}{Walks}$. 

**Important note:** In our [project template](https://raw.githubusercontent.com/TheDataMine/the-examples-book/master/files/project_template.Rmd), we show 2 primary ways to run SQL queries from within R/RMarkdown. In question 5, we wrap our queries in R code. In question 6, we use the database connection, `con`, to run SQL queries directly within an SQL code chunk. In this project, we will just use the first method as it has the advantage of having the result of the query ready to be used within our R environment.

**Important note:** Questions in this project need to be solved using SQL when possible. You will not receive credit for a question if you use `sum` in R rather than `SUM` in SQL. 

**Relevant topics:** [dbGetQuery](#sql), [SUM](#sql-examples), [SELECT](#sql-examples), [FROM](#sql-examples), [WHERE](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem. 
- The result of running the R code.
```

##### 3. How many times has Giancarlo Stanton struck out in years in which he played for "MIA" or "FLO"?

**Relevant topics:** [dbGetQuery](#sql), [AND/OR](#sql-examples), [COUNT](#sql-count), [SUM](#sql-examples)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 4. Calculate the Batting Average of batters between 2000 and 2010, with more than 300 at-bats (ABs). List the top 5 batting averages next to the `playerID` (with team and year). Batting Averages are calculated as $\frac{H}{AB}$.

**Relevant topics:** [dbGetQuery](#sql), [ORDER BY](#sql-order-by), [BETWEEEN](#sql-between)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 5. How many unique players have hit > 50 home runs (`HR`) in a season? 

**Hint:** If you view `DISTINCT` as being paired with `SELECT`, instead, think of it as being paired with one of the fields you are selecting.

**Relevant topics:** [dbGetQuery](#sql), [DISTINCT](#sql-distinct), [COUNT](#sql-count)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 6. How many players are members of the 40/40 club? These are players that have stolen more than 40 bases (`SB`) and hit more than 40 home runs (`HR`).

**Relevant topics:** [dbGetQuery](#sql), [AND/OR](#sql-examples), [DISTINCT](#sql-distinct), [COUNT](#sql-count)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 7. Find the number of unique players that attended Purdue University. Start by finding the `schoolID` for Purdue and then find the number of players who played there. Who had more? Purdue or IU? Use the information you have in the database, and the power of R to create a misleading graphic that makes Purdue look better than IU, even if just at first glance. Make sure you label the graphic.

**Hint:** You can mess with the scale of the y-axis. You could (potentially) filter the data to start from a certain year or be between two dates.

**Hint:** To find IU's id, try the following query: `SELECT schoolID FROM schools WHERE name_full LIKE '%indiana%';`. 

**Relevant topics:** [dbGetQuery](#sql), [plotting in R](#r-plotting), [COUNT](#sql)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

##### 8. Use R, SQL and the lahman database to create an interesting infographic. For those of you who are not baseball fans, try doing a Google image search for "baseball plots" for inspiration. Make sure the plot is polished, has appropriate labels, color, etc.

**Relevant topics:** [SQL](#sql), [plotting in R](#r-plotting)

```{block, type="bbox"}
**Item(s) to submit:**

- R code used to solve the problem.
- The result of running the R code.
```

---

### Project 11 {#p11-390}

--- 

**Motivation:** Being able to use results of queries as tables in new queries (also known as writing sub-queries), and calculating values like MIN, MAX, and AVG in aggregate are key skills to have in order to write more complex queries. In this project we will learn about aliasing, writing sub-queries, and calculating aggregate values.

**Context:** We are in the middle of a series of projects focused on working with databases and SQL. In this project we introduce aliasing, sub-queries, and calculating aggregate values using a much larger dataset!

**Scope:** sql, sql in R

**Learning objectives:**

```{block, type="bbox"}
- Demonstrate the ability to interact with popular database management systems within R.
- Solve data-driven problems using a combination of SQL and R.
- Basic clauses: select, order by, limit, desc, asc, count, where, from, etc.
- Showcase the ability to filter, alias, and write subqueries.
- Perform grouping and aggregate data using group by and the following functions: count, max, sum, avg, like, having. Explain when to use having, and when to use where.
```

#### Dataset

`elections` database & `/class/datamine/data/election/itcontYYYY.txt` (for example, data for year 1980 would be `/class/datamine/data/electionitcont1980.txt`)

A public sample of the data can be found here:

https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcontYYYY.txt (for example, data for year 1980 would be https://www.datadepot.rcac.purdue.edu/datamine/data/election/itcont1980.txt)

Up until now, you've been working with a neatly organized database containing baseball data. As fantastic as this database is, it would be trivial to load up the entire database in R or Python and do your analysis using `merge`-like functions. Now, we are going to deal with a much larger set of data.

##### 1. Approximately how large was the lahman database (use the sqlite database in Scholar: `/class/datamine/data/lahman/lahman.db`)? Use UNIX utilities you've learned about this semester to write a line of code to return the amount of data (in MB) in the elections folder `/class/datamine/data/election/`. How much data (in MB) is there? 

The data in that folder has been added to the `elections` database in the `elections` table. Write a SQL query that returns how many rows of data are in the database. How many rows of data are in the database?

**Hint:** This will take some time! Be patient.

**Relevant topics:** [sql](#sql), [sql in R](#r-sql), [awk](#awk), [ls](#ls)

```{block, type="bbox"}
**Item(s) to submit:**

- Approximate size of the lahman database in mb.
- Line of code (bash/awk) to calculate the size (in mb) of the entire elections dataset in `/class/datamine/data/election`.
- The size of the elections data in mb.
- SQL query used to find the number of rows of data in the `elections` table in the `elections` database.
- The number of rows in the `elections` table in the `elections` database.
```

##### 2. Write a SQL query using the `LIKE` command to find a unique list of `zip_code`s that start with "479". How many unique `zip_code`s are there that begin with "479"?

**Hint:** Make sure you only select `zip_code`s.

**Relevant topics:** [sql](#sql), like

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to answer the question.
- The first 5 results from running the query.
```

##### 3. Write a SQL query that counts the number of donations (rows) that are from Indiana. How many donations are from Indiana? Rewrite the query and create an _alias_ for our field so it doesn't read `COUNT(*)` but rather `Indiana Donations`. 

**Relevant topics:** [sql](#sql), [where](#sql-where), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
- The result of the SQL query.
```

##### 4. Rewrite the query in (3) so the result is displayed like the following:

```{txt}
+-------------+
| Donations   |
+-------------+
| IN: 1111778 |
+-------------+
```

**Hint:** Use CONCAT and aliasing to accomplish this.

**Relevant topics:** [sql](#sql), [aliasing](#aliasing), concat

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
```

##### 5. In (2) we wrote a query that returns a unique list of `zip_code`s that start with "479". In (3) we wrote a query that counts the number of donations that are from Indiana. Use our query from (2) as a sub-query to find how many donations come from areas with `zip_code`s starting with "479". What percent of donations in Indiana come from said `zip_code`s?

**Relevant topics:** [sql](#sql), [aliasing](#sql-aliasing), subqueries

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to answer the question.
- The percentage of donations from Indiana from `zip_code`s starting with "479".
```

##### 6. In (3) we wrote a query that counts the number of donations that are from Indiana. When running queries like this, a natural "next question" is to ask the same question about another state. SQL gives us the ability to calculate functions in aggregate when grouping by a certain column. Write a SQL query that returns the state, number of donations from each state, the sum of the donations (`transaction_amt`). Which 5 states gave the most donations (highest count)? Order you result from most to least.

**Hint:** You may want to create an alias in order to sort.

**Relevant topics:** [sql](#sql), [group by](#sql-groupby)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question. 
- Which 5 states gave the most donations?
```

##### 7. Write a query that gets the number of donations, and sum of donations, by year, for Indiana. Create one or more graphics that highlights the year-by-year changes. Write a short 1-2 sentences explaining your graphic(s).

**Relevant topics:** [sql in R](#sql-in-R), [group by](#sql-groupby)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
- R code used to create your graphic(s).
- 1 or more graphics in png/jpeg format.
- 1-2 sentences summarizing your graphic(s).
```

---

### Project 12 {#p12-390}

---

**Motivation:** Databases are comprised of many tables. It is imperative that we learn how to combine data from multiple tables using queries. To do so we perform joins! In this project we will explore learn about and practice using joins on a database containing bike trip information from the Bay Area Bike Share. 

**Context:** We've introduced a variety of SQL commands that let you filter and extract information from a database in an systematic way. In this project we will introduce joins, a powerful method to combine data from different tables.

**Scope:** SQL, sqlite, joins

**Learning objectives:**

```{block, type="bbox"}
- Briefly explain the differences between left and inner join and demonstrate the ability to use the join statements to solve a data-driven problem.
- Perform grouping and aggregate data using group by and the following functions: count, max, sum, avg, like, having. 
- Showcase the ability to filter, alias, and write subqueries.
```

#### Dataset

The following questions will use the dataset found in Scholar:

`/class/datamine/data/bay_area_bike_share/bay_area_bike_share.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/bay_area_bike_share/bay_area_bike_share.db)

#### Questions

##### 1. There are a variety of ways to join data using SQL. With that being said, if you are able to understand and use a LEFT JOIN and INNER JOIN, you can perform *all* of the other types of joins (RIGHT JOIN, FULL OUTER JOIN). Given the following two tables, use [RMarkdown](#r-rmarkdown) to display the result of performing the following query as a table:

```{sql, eval=F}
SELECT * FROM users AS u INNER JOIN dorms AS d ON u.dorm=d.id;
```

**users:**

id | first_name | last_name | dorm
---|------------|-----------|-----
1  | Alice      | Smith     | 1
2  | Bob        | Johnson   | 2
3  | Susan      | Marques   | 3
4  | Amare      | Keita     | 3
5  | Kristen    | Lakehold  | 4

**dorms:**

id | name | capacity | address
---|------|----------|--------
1  | Windsor Halls | NULL | Windsor Halls, West Lafayette, IN, 47906
2  | Cary Quadrangle | 1200 | 1016 W Stadium Ave, West Lafayette, IN 47906
3  | Hillenbrand Hall | NULL | 1301 3rd Street, West Lafayette, IN 47906

**Relevant topics:** [sql](#sql), [inner join](#sql-inner-join)

```{block, type="bbox"}
**Item(s) to submit:**

- [RMarkdown](#r-rmarkdown) table displaying the result of performing the following query as a table.
```

##### 2. Using the same two tables from (1), use [RMarkdown](#r-rmarkdown) to display the result of performing the following query as a table. Explain the difference between an INNER JOIN and LEFT JOIN.

```{sql, eval=F}
SELECT * FROM users AS u LEFT JOIN dorms AS d ON u.dorm=d.id;
```

**Relevant topics:** [sql](#sql), left join

```{block, type="bbox"}
**Item(s) to submit:**

- [RMarkdown](#r-rmarkdown) table displaying the result of performing the following query as a table.
- 1-2 sentences explaining (in your own words) what the difference between and INNER and LEFT JOIN is.
```

##### 3. Aliases can be created for tables, fields, and even results of aggregate functions (like MIN, MAX, COUNT, AVG, etc.). In addition, you can combine fields using the `sqlite` concatenate operator `||` (see [here](https://www.sqlitetutorial.net/sqlite-string-functions/sqlite-concat/)). Write a query that returns the first 5 records of information from the `station` table formatted in the following way:

`(id) name @ (lat, long)`

For example:

`(84) Ryland Park @ (37.342725,-121.895617)`

**Relevant topics:** [aliasing](#sql-aliasing), concat

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem. 
- The first 5 records of information from the `station` table.
```

##### 4. There is a variety of interesting weather information in the `weather` table. Write a query that finds the average `mean_temperature_f` by `zip_code`. Which is on average the warmest `zip_code`?

Use aliases to format the result in the following way:

```{txt}
Zip Code|Avg Temperature
94041|61.3808219178082
```

**Relevant topics:** [aliasing](#sql-aliasing), [group by](#sql-groupby), [avg](#sql-avg)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
- The results of the query copy and pasted.
```

##### 5. From (4) we can see that there are only 5 `zip_code`s with weather information. How many unique `zip_code`s do we have in the `trip` table? Write a query that finds the number of unique `zip_code`s in the `trip` table. Write another query that lists the `zip_code` and count of the number of times the `zip_code` appears. If we had originally assumed that the `zip_code` was related to the location of the trip itself, we were wrong. Can you think of a likely explanation?

**Relevant topics:** [group by](#sql-groupby), [count](#sql-count)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL queries used to solve this problem.
- 1-2 sentences explainging what a possible explanation for the `zip_code`s could be.
```

##### 6. In (4) we wrote a query that finds the average `mean_temperature_f` by `zip_code`. What if we want to tack on to our results information from each row in the `station` table based on the `zip_code`s? To do, use an INNER JOIN. INNER JOIN combines tables based on specified fields, and returns only rows where there is a match in both the "left" and "right" tables.

**Hint:** Use the query from (4) as a sub query within your solution.

**Relevant topics:** [inner join](#sql-inner-join), subqueries, [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
```

##### 7. In (5) we eluded that the `zip_code`s in the `trip` table aren't very consistent. Users can enter a zip code when using the app. This means that `zip_code` can be from anywhere in the world! With that being said, if the `zip_code` is one of the 5 `zip_code`s for which we have weather data (from question 4), we can add that weather information to matching rows of the `trip` table. In (6) we used an INNER JOIN to append some weather information to each row in the `station` table. For this question, write a query that performs an INNER JOIN and appends weather data from the `weather` table to the trip data from the `trip` table. Limit your solution to 5 lines. 

**Hint:** You will want to wrap your dates and datetimes in [sqlite's `date` function](https://www.sqlitetutorial.net/sqlite-date-functions/sqlite-date-function/) prior to comparison.

**Important note:** Notice that the weather data has about 1 row of weather information for each date and each zip code. This means you may have to join your data based on multiple constraints instead of just 1 like in (6).

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve this problem.
- First 5 lines of output.
```

##### 8. How many rows are in the result from (7) (when not limiting to 5 lines)? How many rows are in the `trip` table? As you can see, a large proportion of the data from the `trip` table did not match the data from the `weather` table, and therefore was removed from the result. What if we want to keep all of the data from the `trip` table and add on data from the `weather` table if we have a match? Write a query to accomplish this. How many rows are in the result?

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to find how many rows from the result in (7).
- The number of rows in the result of (7).
- SQL query to find how many rows are in the `trip` table.
- The number of rows in the `trip` table.
- SQL query to keep all of the data from the `trip` table and add on matching data from the `weather` table when available.
- The number of rows in the result.
```

---

### Project 13 {#p13-390}

---

**Motivation:** Databases you will work with won't necessarily come organized in the way that you like. Getting really comfortable writing longer queries where you have to perform many joins, alias fields and tables, and aggregate results, is important. In addition, gaining some familiarity with terms like _primary key_, and _foreign key_ will prove useful when you need to search for help online. In this project we will write some more complicated queries with a fun database. Proper preparation prevents poor performance, and that means practice!

**Context:** We are towards the end of a series of projects that give you an opportunity to practice using SQL. In this project, we will reinforce topics you've already learned, with a focus on subqueries and joins. 

**Scope:** SQL, sqlite

**Learning objectives:**

```{block, type="bbox"}
- Write and run SQL queries in `sqlite` on real-world data.
- Identify primary and foreign keys in a SQL table.
```

#### Dataset

`/class/datamine/data/movies_and_tv/imdb.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/imdb.db).

#### Questions

##### 1. A primary key is a field in a table which uniquely identifies a row in the table. Primary keys _must_ be unique values, and this is enforced at the database level. A foreign key is a field whose value matches a primary key in a different table. A table can have 0-1 primary key, but it can have 0+ foreign keys. Examine the `titles` table. Do you think there are any primary keys? How about foreign keys? 

**Relevant topics:** primary key, foreign key

```{block, type="bbox"}
**Item(s) to submit:**

- List any primary or foreign keys in the `episodes` table.
```

##### 2. Examine the `episodes` table. Based on observation and the column names, do you think there are any primary keys? How about foreign keys?

**Relevant topics:** primary key, foreign key

```{block, type="bbox"}
**Item(s) to submit:**

- List any primary or foreign keys in the `episodes` table.
```

If you paste a `title_id` to the end of the following url, it will pull up the page for the title. For example, https://www.imdb.com/title/tt0413573 leads to the page for the TV series Grey's Anatomy. 

##### 3. Write a query to confirm that the `title_id` tt0413573 does indeed belong to Grey's Anatomy.

**Relevant topics:** [select](#sql-examples), [where](#sql-where)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- Output of the query.
```

##### 4. The `episode_title_id` column in the `episodes` table references titles of individual episodes of a tv series. The `show_title_id` references the titles of the show itself. With that in mind, write a query that gets a list of all of the episodes and titles of Grey's Anatomy.

**Relevant topics:** [inner join](#sql-inner-join)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
```

##### 5. Like we explained in (3), you can find the `title_id` of a tv show, a tv show episodes, or a movie by browsing imdb.com and getting the `title_id` directly from the url. Browse imdb.com and find your favorite tv show. Get the `title_id` from the url and run the following query to confirm that the tv show is in our database:

```{sql, eval=F}
SELECT * FROM titles WHERE title_id='<title id here>';
```

Make sure to replace "\<title id here\>" with the `title_id` of your favorite show. If your show does not appear, or has only a single season, pick another show until you find one we have in our database with multiple seasons.

```{block, type="bbox"}
**Item(s) to submit:**

- The `title_id` of your favorite tv show.
- The output from running the provided (modified) query.
```

##### 6. We want to write a query that returns the title and rating of the highest rated episode of the tv show you chose in (5). In order to do so, first write a query that returns a list of `episode_title_id`s (found in the `episodes` table), with the `primary_title` (found in the `titles` table) of the episode. 

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- The first 5 results from your query.
```

##### 7. Write a query that adds the rating to the end of each episode. To do so, use the query you wrote in (6) as a subquery. Was this also your favorite episode?

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing), subqueries, [desc](#sql-examples), [limit](#sql-examples), [order by](#sql-order-by)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to solve the problem in a code chunk.
- The `episode_title_id`, `primary_title`, and `rating` of the top rated episode from the tv series from (5).
- A statement saying whether it is also your favorite episode.
```

##### 8. Write a query that returns the `season_number` (from the `episodes` table), and average `rating` (from the `ratings` table) for each season. Write another query that only returns the season number and `rating` for the highest rated season. Consider the highest rated season the season with the highest average.

**Relevant topics:** [inner join](#sql-inner-join), [aliasing](#sql-aliasing), [group by](#sql-groupby), having, [avg](#sql-avg)

```{block, type="bbox"}
**Item(s) to submit:**

- The 2 SQL queries used to solve the problems in a code chunk.
```

##### 9. Write a query that returns the `primary_title`, and `rating` of the highest rated episode per season for your tv show from (5). 

**Relevant topics:** [max](#sql-max), subqueries, [group by](#sql-examples), having, [inner join](#sql-examples), [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- The SQL query used to solve the problem.
- The output from your query.
- 1-2 sentences explaining whether or not you agree.
```

---
 
### Project 14 {#p14-390}

---

**Motivation:** As we learned earlier in the semester, bash scripts are a powerful tool when you need to perform repeated tasks in a UNIX-like system. In addition, sometimes preprocessing data using UNIX tools prior to analysis in R or Python is useful. Ample practice is integral to becoming proficient with these tools. As such, we will be reviewing topics learned earlier in the semester.

**Context:** We've just ended a series of projects focused on SQL. In this project we will begin to review topics learned throughout the semester, starting writing bash scripts using the various UNIX tools we learned about.

**Scope:** awk, UNIX utilities, bash scripts, fread

**Learning objectives:**

```{block, type="bbox"}
- Navigating UNIX via a terminal: ls, pwd, cd, ., .., ~, etc.
- Analyzing file in a UNIX filesystem: wc, du, cat, head, tail, etc.
- Creating and destroying files and folder in UNIX: scp, rm, touch, cp, mv, mkdir, rmdir, etc.
- Use grep to search files effectively.
- Use cut to section off data from the command line.
- Use piping to string UNIX commands together.
- Use awk for data extraction, and preprocessing.
- Create bash scripts to automate a process or processes.
```

#### Dataset: 

The following questions will use the dataset found in Scholar:

`/class/datamine/data/forest` 

To read more about the two files from this dataset that you will be working with:

PLOTSNAP.csv: 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/plot-level-data-gathered-through-forest/metadata#fields

TREE.csv: 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/metadata 

AND 

https://www.uvm.edu/femc/data/archive/project/federal-forest-inventory-analysis-data-for/dataset/tree-level-data-gathered-through-forest/data

#### Questions

##### 1. Take a look at at `PLOTSNAP.csv`. Write a line of awk code that displays the `STATECD` followed by the number of rows with that `STATECD`.

**Relevant topics:** [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- Count of the following `STATECD`s: 1, 2, 4, 5, 6
```

##### 2. Unfortunately, there isn't a very accessible list available that shows which state each `STATECD` represents. This is no problem for us though, the dataset has `LAT` and `LON`! Write some bash that prints just the `STATECD`, `LAT`, and `LON`.

**Note:** There are 92 columns in our dataset: `awk -F, 'NR==1{print NF}' PLOTSNAP.csv`. To create a list of `STATECD` to state, we only really need `STATECD`, `LAT`, and `LON`. Keeping the other 89 variables will keep our data at 2.1gb.

**Relevant topics:** [cut](#cut), [awk](#awk)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The output of your code piped to `head`.
```

##### 3. `fread` is a "Fast and Friendly File Finagler". It is part of the very popular `data.table` package in R. We will learn more about this package next semester. For now, read the documentation [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/fread) and use the `cmd` argument in conjunction with your bash code from (2) to preprocess data prior to reading it into a `data.table` in your R environment. 

**Relevant topics:** fread

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The `head` of the resulting `data.table`.
```

##### 4. Follow the directions [here]() to install `ggmap` and get an API key. There are over 4 million rows in our dataset -- we do _not_ want to hit Google's API that many times, nor would that work. Instead, do the following:

- Unless you feel comfortable using `data.table`, convert your `data.table` to a `data.frame`:

```{r, eval=F}
my_dataframe <- data.frame(my_datatable)
```

- Calculate the average `LAT` and `LON` for each `STATECD`, and call the new `data.frame` `dat`. 
- For each row in `dat`, run a reverse geocode and append the state to a new column called `ADDRESS`.

**Hint:** To calculate the average `LAT` and `LON` for each `STATECD`, you could use the [`sqldf`](https://www.rdocumentation.org/packages/sqldf/versions/0.4-11) package to run SQL queries on your `data.frame`.

**Hint:** To get the address, given `LAT` and `LON`:

```{r, eval=F}
geo <- revgeocode(c(-86.916576, 40.433663), output = "address")
geo
```

**Hint:** `mapply` is a useful apply function to use to solve this problem.

**Important note:** It is okay to get NA's for some of the addresses.

**Relevant topics:** [ggmap](#r-ggmap), [functions](#r-writing-functions), sqldf

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to solve the problem.
- The `head` of the resulting `data.frame`.
```

##### 5. Use the `geom_point` function to add our latitude and longitude data to a map. Use the following code to create the initial map:

```{r, eval=F}
library(ggmap)
map <- get_map(location="United States", zoom=3)
ggmap(map)
```

**Hint:** See [here](https://stackoverflow.com/questions/23130604/plot-coordinates-on-map) for an example of adding points to a map.

**Relevant topics:** [ggmap](#r-ggmap)

```{block, type="bbox"}
**Item(s) to submit:**

- Code used to create the map.
- The map itself as output from running the code chunk.
```

##### 6. Write a bash script that accepts at least 1 argument, and performs a useful task using at least 1 dataset from the `forest` folder in `/class/datamine/data/forest`. An example of a useful task could be printing a report of summary statistics for the data. Feel free to get creative. Note that tasks must be non-trivial -- a bash script that counts the number of lines in a file is _not_ appropriate. Make sure to properly document (via comments) what your bash script does. If you are in STAT 39000 ensure that your script returns columnar data with appropriate separating characters (for example a csv).

**Relevant topics:** [bash scripts](#writing-scripts), [awk](#awk), [unix utilities](#unix-utilities)

```{block, type="bbox"}
**Item(s) to submit:**

- The content of your bash script starting from `#!/bin/bash`.
- Example output from running your script as intended.
- A description of what your script does.
```

##### 7. `fread` is a "Fast and Friendly File Finagler". It is part of the very popular `data.table` package in R. We will learn more about this package next semester. For now, read the documentation [here](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/fread) and use the `cmd` argument in conjunction with your script from (4) to preprocess data prior to reading it into a `data.table` in your R environment. 

**Relevant topics:** fread

```{block, type="bbox"}
**Item(s) to submit:**

- The R code used to read in and preprocess your data using your bash script from (3).
- The `head` of the resulting `data.table`.
```

---

### Project 15 {#p15-390}

---

**Motivation:** We've done a lot of work with SQL this semester. Let's review concepts in this project and mix and match R, Python, and SQL to solve data-driven problems.

**Context:** In this project, we will reinforce topics you've already learned, with a focus on SQL.

**Scope:** SQL, sqlite, R, Python

**Learning objectives:**

```{block, type="bbox"}
- Write and run SQL queries in `sqlite` on real-world data.
- Use SQL from within R and Python.
```

#### Dataset

`/class/datamine/data/movies_and_tv/imdb.db`

A public sample of the data can be found [here](https://www.datadepot.rcac.purdue.edu/datamine/data/movies-and-tv/imdb.db).

In this project we want to offer the flexibility of using your choice of R and/or Python. To keep things as consistent as possible, please use Rmarkdown on https://rstudio.scholar.rcac.purdue.edu/. See [here](#python-on-scholar) to learn how to run Python in this environment.

F.R.I.E.N.D.S is a popular tv show. They have an interesting naming convention for the names of their episodes. They all begin with the text "The One ...". There are 6 primary characters in the show: Chandler, Joey, Monica, Phoebe, Rachel, and Ross. Let's use SQL and R to take a look at how many times each characters' names appear in the title of the episodes.

#### Questions 

##### 1. Write a query that gets the `episode_title_id`, `primary_title`, `rating`, and `votes`, of all of the episodes of Friends (`title_id` is tt0108778). 

**Hint:** You can slightly modify the solution to question (7) in project 13.

**Relevant topics:** [inner join](#sql-inner-join), subqueries, [aliasing](#sql-aliasing)

```{block, type="bbox"}
**Item(s) to submit:**

- SQL query used to answer the question.
- First 5 results of the query.
```

The next couple of questions should be complete in the same language. You can use either R or Python, but you must use the same for both questions.

##### 2. Now that you have a working query, connect to the database and run the query to get the data into an R or `pandas` data frame. In previous projects, we learned how to used regular expressions to search for text. For each character, how many episodes `primary_title`s contained their name? 

**Relevant topics:** [SQL in R](#sql-in-r), [SQL in Python](#sql-in-python), [grep](#r-grep)

```{block, type="bbox"}
**Item(s) to submit:**

- R or Python code in a code chunk that was used to find the solution.
- The solution pasted below the code chunk.
```

##### 3. Create a graphic showing our results in (2) using your favorite package. Make sure the plot has a good title, x-label, y-label, and try to incorporate some of the following colors: #273c8b, #bd253a, #016f7c, #f56934, #016c5a, #9055b1, #eaab37.

**Relevant topics:** [plotting](#r-plotting)

```{block, type="bbox"}
**Item(s) to submit:**

- The R or Python code used to generate the graphic.
- The graphic in a png or jpg/jpeg format.
```

##### 4. Use any combination of SQL, R, and Python you'd like in order to find which of the following 3 genres has the highest average rating for movies (see `type` column from `titles` table): Romance, Comedy, Animation. In the `titles` table, you can find the genres in the `genres` column. There may be some overlap (i.e. a movie may have more than one genre), this is ok. 

To query rows which have the genre Action as one of its genres:

```{sql, eval=F}
SELECT * FROM titles WHERE genres LIKE '%action%';
```

**Relevant topics:** like, [inner join](#sql-inner-join)

```{block, type="bbox"}
**Item(s) to submit:**

- Any code you used to solve the problem in a code chunk.
- The average rating of each of the genres listed for movies.
```

##### 5. Write a function called `top_episode` in R or in Python which accepts the path to the `imdb.db` database, as well as the `title_id` of a tv series (for example, "tt0108778" or "tt1266020"), and returns the `season_number`, `episode_number`, `primary_title`, and `rating` of the highest rated episode in the series. Test it out on some of your favorite series, and share the results.

**Relevant topics:** [functions](#writing-functions), [inner join](#sql-inner-join), [order by](#sql-order-by)

```{block, type="bbox"}
**Item(s) to submit:**

- Any code you used to solve the problem in a code chunk.
- The results for at least 3 of your favorite tv series.
```
