= Time Series: Recurrent Neural Networks

Recurrent Neural Networks are neural networks with a bidirectional flow architecture, instead of the commonly used feed-forward architecture. This means that each node output will impact its neighbors, regardless of whether they are forward or behind it. RNN's are a popular choice for time series, because they consider the impacts of time data in both directions, which makes it easy to extrapolate trends and predict into the future.

== Common Applications

=== Common Industries

- Agriculture
- Tech
- Finance
- Any Industry Using NLP

=== Common Problem Types

- Language Recognition (textual, speech or otherwise)
- Time Series Predictions Into Future
- Language Translation
- Robotic Control

== A Brief History

While RNN's have their research roots in statistical mechanics in the 1920's, Hopfield Nets developed in the 70's and 80's, despite a general lack of interest in neural networks (the so called "AI winter"). Hopfield Nets were seen as allowing for associative memory structures, remembering the outputs of all or many nodes- a contrast to the feed forward architecture, which didn't need to remember what previous nodes' outputs were, just the current and previous layer nodes. The late 1990's came around and along with other neural network R&D, LSTM (Long Short-Term Memory) networks were developed; its success was partially due to its ability to handle much larger sequences of time-steps, along with the development of computing power since the 1970's. LSTM is used heavily in NLP, where it is only recently being challenged by transformer models (such as the well known ChatGPT: "Chat Generative Pre-trained Transformer"). RNN's are still heavily used wherever there is time series data, as well as NLP.

== Code Examples

NOTE: All of the code examples are written in Python, unless otherwise noted.

=== Containers

TIP: These are code examples in the form of Jupyter notebooks running in a container that come with all the data, libraries, and code you'll need to run it. https://the-examples-book.com/starter-guides/data-engineering/containers/using-data-mine-containers[Click here to learn why you should be using containers, along with how to do so.]

[source,bash]
----
#pull container, only needs to be run once
docker pull ghcr.io/thedatamine/starter-guides:time-series-rnn

#run container
docker run -p 8888:8888 -it ghcr.io/thedatamine/starter-guides:time-series-rnn
----

Need help implementing any of this code? Feel free to reach out to mailto:datamine-help@purdue.edu[datamine-help@purdue.edu] and we can help!

== Resources

All resources are chosen by Data Mine staff to be of decent quality, and most if not all content is free. 

=== Websites

https://www.ibm.com/topics/recurrent-neural-networks[Recurrent Neural Networks (IBM)]

https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks[Recurrent Neural Networks Cheatsheet (Stanford)]

https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85[Recurrent Neural Networks (Towards Data Science)]

https://www.mathworks.com/discovery/rnn.html[What is a Recurrent Neural Network? (MathWorks)]

=== Videos

https://www.youtube.com/watch?v=WCUNPb-5EYI[Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) (~25 minutes)]

https://www.youtube.com/watch?v=Y2wfIKQyd1I[What is Recurrent Neural Network (RNN)? (~16 minutes)]

https://www.youtube.com/watch?v=DFZ1UA7-fxY[Recurrent Neural Networks : Data Science Concepts (~27 minutes)]

https://www.youtube.com/watch?v=b61DPVFX03I[What is LSTM (Long Short Term Memory)? (~8 minutes)]

=== Books

https://www.statlearning.com[Introduction to Statistical Learning (ISL)]

Also known as the "machine learning bible", this book is very well known and highly recommended and very clear, *and has numerous code projects attached with each chapter*. See Chapter 10.5.

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99170200340801081[Recurrent neural networks: from simple to gated architectures (2022)]

Gentle introduction for those new to neural networks, but robust in its technical explanation for those who want to go further.

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/uc5e95/alma99170398531201081[Recurrent neural networks: concepts and applications (2023)]

Great for visual learners; great introduction and overview of the applications of RNN's that goes deep for those interested.

=== Articles

https://purdue.primo.exlibrisgroup.com/permalink/01PURDUE_PUWL/5imsd2/cdi_crossref_primary_10_1109_T_C_1972_223477[Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements (1972)]

Seminal paper on RNN's from the 1970's that advanced RNN's and culminated in https://en.wikipedia.org/wiki/Hopfield_network[Hopfield Networks].