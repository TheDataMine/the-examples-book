= Cross Validation: Training, Validation and Testing Splits

Cross Validation is a resampling technique that uses training, validation and testing splits to chunk up the data for resampling.

Splits in this context refers to splitting up data. It is standard practice to split up your data into Training, Validation, and (optionally although highly recommended) Test splits. For instance, maybe in a dataset of 100,000 images, we select 90% for training, 9% for validation, and 1% for testing. But what do these mean, and how can I make these splits in code?

== Differences between Train, Valid, Test

=== Train/Training Splits

The training split is the training data. As the name implies, this is the dataset we will use to train from.

=== Valid/Validation Splits

The Validation split is used to **validate** the training data. This is used in the process of fitting the model to the training data. The validation set provides an unbiased estimation of model fit (unbiased because it was not used during training), and is often used to slightly tweak hyperparameters during training and/or implement early stopping mechanisms (to prevent overfitting when validation metrics reach a certain level). 

=== Test/Testing Splits

Test splits are not involved in training a model at all. As such, they are used to assess the generalizability of the model. For instance, after training an unsupervised k-means model, we want to use our test data to see how well the model can predict the values of a given parameter in our data. If the model was trained well, the predictions should be pretty close to the test data; this suggests that the model has generalized well, that is, it is not *overfitting* too much.

As mentioned, test splits are optional. While you do need training and validation sets to build models, test sets are optional indicators used to show how generalizable a model is. But maybe you don't want a generalizable model; maybe you are just trying things out. Maybe you don't plan on deploying this model. Maybe you are the only person who will see it. Maybe you know precisely the data that will be fed into the model, and generalizability isn't an issue. In general, if you are going to deploy a model, you should use a test set, and it's also good practice to split your data into train/valid/test right from the beginning in any model building. 

== Train/Valid/Test Ratios

The eternal question about splits is: what is the correct ratio of splits? Is it 70/25/5? 90/9/1?

*There are no hard and fast rules about what the correct split ratio is.* Often times, the approximate answer to this question revolves around **whatever gives you the desired outcome for your model**. For instance, maybe you are developing critical software that must be 99.99% accurate in its predictions; finding a split ratio that reaches this bar is the goal, and what that ratio is often doesn't matter.

However, most models gain in accuracy with larger training datasets. As such, a common ratio is something like 80/15/5 or 80/10/10. But, if you have a dataset of millions, then maybe you are ok with training on 60/35/5; A higher validation set here would give more accuracy in fine tuning the model at every training cycle (or epoch).

It is common practice to start with some training set in between 70-90, validation between 5-25, and test between 5-25.

Maybe we only have a dataset of 10,000 records. In this case, a higher training split might be more appropriate, like 90/5/5, because then we would make sure we use as much of the data as we can to train on (having too few of training samples can lead to underfitting).

== Make Sure Your Splits Are Random!

One very important thing to remember about splits is ensuring they are random. For instance, if you have an alphabetically ordered dataset of names, then making a train split of 80/15/5 would get you all the names from A to O. This of course would cause problems when your validation set uses the names from O to X, and your test set uses Y and Z names. Be sure to shuffle your datasets before organizing them into the splits, and ensuring randomness across the splits.

== Challenges With Splits

Splits have two primary issues.

1. The validation estimate can be highly variable, depending on which observations are put into which split. But how will you know which data to go into which splits? 
2. Since only the training data is used to train the model, and we want our model to be trained on increasing amounts of data, the valid split error rate will tend to overestimate for a model fit on increasing proportions of the data set.

One way to solve these challenges is by using xref:data-modeling/resampling-methods/cross-validation/loocv-kfold.adoc[LOOCV or k-fold CV].

== Code Examples

NOTE: All of the code examples are written in Python, unless otherwise noted.

=== Notebooks

TIP: Download the Notebook if you are going to upload it to Anvil to run. 

xref:attachment$train-valid-test.zip[Train Valid Test Notebook]

=== Containers 

TIP: The Container and Notebook versions are the same, except the container file runs a Jupyter lab with the Notebook already in it along with all packages and data necessary to run the Notebook. https://the-examples-book.com/starter-guides/data-engineering/containers/using-data-mine-containers[Click here to learn when and why you should be using Containers, along with how to do so.]

[source,bash]
----
#pull container, only needs to be run once
docker pull ghcr.io/thedatamine/starter-guides:train-valid-test

#run container
docker run -p 8888:8888 -it ghcr.io/thedatamine/starter-guides:train-valid-test
----

Need help implementing any of this code? Feel free to reach out to mailto:datamine-help@purdue.edu[datamine-help@purdue.edu] and we can help!