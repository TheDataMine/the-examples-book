= STAT-LLC 2014 R Exercises

== Project 1

Question 1.

During which pair of years did the level of Lake Huron rise the most?
The data to use is from the built-in `LakeHuron` data set.
(E.g., during 1875 to 1876, Lake Huron rose 1.48 feet.)  It might help to use the `diff` command.

Solution:

`diff(LakeHuron)` gives the year-to-year rises and falls of the data

`diff(LakeHuron) == max(diff(LakeHuron))` gives a vector of `FALSE` values and one `TRUE` showing which of the positions are the max

`which(diff(LakeHuron) == max(diff(LakeHuron)))`  shows that the maximum occurs in the 85th year of the data set.

So the biggest rise of the LakeHuron data set is between the year 1959:

`1874 + which(diff(LakeHuron) == max(diff(LakeHuron)))`

and the year 1960:

`1874 + 1 + which(diff(LakeHuron) == max(diff(LakeHuron)))`

Question 2.

a. What is the average duration of an eruption in the `geyser` dataset in the `MASS` library?

b. What were the 10 longest durations?

c. How many durations were 3 minutes or longer?
(You do not need to install the `MASS` library; it is installed already.  You do, however, need to load the `MASS` library.)

Solution:

We first load the `MASS` library

`library(MASS)`

a. The average duration is 3.460814 minutes:

`mean(geyser$duration)`

b. We can sort the geyser durations, from largest to smallest, as follows:

`sort(geyser$duration, decreasing=TRUE)`

and then we can just extract the largest 10 entries of this vector:

`sort(geyser$duration, decreasing=TRUE)[1:10]`

c. There are 194 durations that last 3 minutes or longer:

`length(geyser$duration[geyser$duration >= 3])`


Question 3.

a.  Which car(s) in the `mtcars` data set had the highest gas mileage?

b.  Which car(s) had the highest horsepower?

c.  Which car(s) had the shortest (i.e., fastest) 1/4 mile time?

d.  How many cars had manual transmission?

e.  How many cars had manual transmission and also six cylinders?

Solution:

a. We see that the 20th car has the highest gas mileage

`which(mtcars$mpg == max(mtcars$mpg))`

so we can get the name of this car this way:

`row.names(mtcars[20,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$mpg == max(mtcars$mpg)),])`

b. We see that the 31st car has the highest horsepower

`which(mtcars$hp == max(mtcars$hp))`

so we can get the name of this car this way:

`row.names(mtcars[31,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$hp == max(mtcars$hp)),])`

c. We see that the 29th car has the highest horsepower

`which(mtcars$qsec == min(mtcars$qsec))`

so we can get the name of this car this way:

`row.names(mtcars[29,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$qsec == min(mtcars$qsec)),])`

d.

`mtcars$am == 1`

gives a vector of `TRUE` and `FALSE` values, for whether the cars do or do not have manual transmission

then we can sum to get the total number of `TRUE` values, because when we sum, each `TRUE` becomes 1 and each `FALSE` becomes 0.

`sum(mtcars$am == 1)`

e. Similar to 3d, but we add the condition that there are 6 cylinders:

`sum(mtcars$am == 1 & mtcars$cyl == 6)`


Question 4.

a.   Which states are (strictly) larger in population than Indiana but (strictly) smaller in population than Pennsylvania, according to the data in the `state` data set?
Hint: You can get the state populations using `state.x77[,"Population"]`.

b.   Which states are (strictly) larger in land area than Indiana but (strictly) smaller in land area than Pennsylvania, according to the data in the `state` data set, as listed in `state.x77[,"Area"]`?

Solution:

a. The populations of Indiana and Pennsylvania are 5313 and 11860, respectively:

`state.x77[,"Population"]["Indiana"]`

`state.x77[,"Population"]["Pennsylvania"]`

now we use these conditions to index the vector state.x77[,"Population"] as follows:

`state.x77[,"Population"][state.x77[,"Population"] > 5313 & state.x77[,"Population"] < 11860]`

or we can roll this into one line as follows:

`state.x77[,"Population"][  state.x77[,"Population"] > state.x77[,"Population"]["Indiana"] & state.x77[,"Population"] < state.x77[,"Population"]["Pennsylvania"]  ]`

b. the populations of Indiana and Pennsylvania are 5313 and 11860, respectively:

`state.x77[,"Area"]["Indiana"]`

`state.x77[,"Area"]["Pennsylvania"]`

now we use these conditions to index the vector state.x77[,"Area"] as follows:

`state.x77[,"Area"][state.x77[,"Area"] > 36097 & state.x77[,"Area"] < 44966]`

or we can roll this into one line as follows:

`state.x77[,"Area"][state.x77[,"Area"] > state.x77[,"Area"]["Indiana"] & state.x77[,"Area"] < state.x77[,"Area"]["Pennsylvania"]]`


Question 5.

If `Z` is a standard normal random variable, we know that `Z` has average 0 and variance 1.  Use `R` to simulate:

a. the value of the average of `|Z|`, and

b. the value of the variance of `|Z|`.

Here, `|Z|` is just the absolute value of `Z`.

Solution:

a. The value of the mean of |Z| is approximately

`mean(abs(rnorm(1000000)))`

b. The value of the var of |Z| is approximately

`var(abs(rnorm(1000000)))`


Question 6.

Write a function called `countas` that takes a sequence of words and returns the number of words that have 1 or more `a`s. For instance, `countas(  c("ate", "hello", "duolingo", "pat", "aa")  )` should return the value 3.  Hint:  It might help to use the `grep` function.

Solution:

We define the countas function to be:

[source,r]
----
countas <- function(v) {
	length(grep("a", v))
}
----

Question 7.

a.  Write a function called:  `firstthree` that returns the location of the first occurrence of 3 in a vector.  For instance, `firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 2.

b.  Write a function called:  `thirdthree` that returns the location of the third occurrence of 3 in a vector.  Ffor instance, `thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 9.

Solution:

a. We find which elements equal 3, and then take the first such element, so we define the firstthree function to be:

[source,r]
----
firstthree <- function(v) {
	which(v==3)[1]
}
----

and it works as required:

`firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )`

b. We find which elements equal 3, and then take the third such element, so we define the thirdthree function to be:

[source,r]
----
thirdthree <- function(v) {
	which(v==3)[3]
}
----

and it works as required:

`thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )`


Question 8.

Write a function called:  `topfive` that returns the most common five values in a vector, along with the counts for each of the 5 values.

Solution:

We make a table, sort it into decreasing order (i.e., biggest elements first) and then we take the biggest five elements, so we define the topfive function to be:

[source,r]
----
topfive <- function(v) {
	sort(table(v), decreasing=TRUE)[1:5]
}
----


Question 9.

a. Euler's number is 2.718281828459...  Euler's number is defined as `1 + 1/1 + 1/(1*2) + 1/(1*2*3) + 1/(1*2*3*4) + 1/(1*2*3*4*5) + ...` Find a good way to calculate this in `R`, with few keystrokes. If you subtract 2.718281828459 from your estimate, you should get something very small, e.g., roughly `4.5 * 10^{-14}.`

b.  Find a good way to approximate the value of Pi, using only the fact that `Pi^2 / 6 = 1/1^2 + 1/2^2 + 1/3^2 + 1/4^2 + 1/5^2 + 1/6^2 + 1/7^2 + ....`

Solution:

a. Euler's number is approximately

`sum(1/factorial(0:100))`

and indeed, if we subtract 2.718281828459, we get a very small number:

`sum(1/factorial(0:100)) - 2.718281828459`

b. The number Pi^2 / 6 is approximately equal to   sum(1/((1:100000000)^2))

so this means that Pi is approximately:

`sqrt(6*sum(1/((1:10000000)^2)))`


Question 10.

a. The triangular numbers are: `1, 3, 6, 10, 15, 21, 28, 36, 45, 55, ...` See: http://oeis.org/A000217 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

b. The tetrahedral numbers are: `1, 4, 10, 20, 35, 56, 84, 120, 165, 220, ...` See: http://oeis.org/A000292 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

Solution:

a. The first 1000 triangular numbers are:

`n <- 1000`

`(1:n)*(2:(n+1))/2`

b. The first 1000 tetrahedral numbers are:

`n <- 1000`

`(1:n)*(2:(n+1))*(3:(n+2))/6`


== Project 2

Question 1.

Consider the Columbia River Estuary dataset discussed in the `week 2 notes`

a.  Download the data set (we no longer need to do this).

b.  Import the `saturn03.240.A.CT_2012_06_PD0.csv` data set into `R`, using the `read.csv` function.

c.  Use the `strptime` function to convert the first column of the data into numerical times that `R` can easily handle.

Solution:

ab. Download the data set and import it to R:
`DF <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.240.A.CT_2012_06_PD0.csv")`

c. Use strptime to convert the times:
`timevec <- strptime(DF[ ,1],  "%Y/%m/%d %H:%M:%S")`

Question 2.

a.  What is the most common time (in seconds) between consecutive measurements, in the data set?  How often is the data sampled with this exact difference in time, between consecutive measurements?

b.  What is the mean time between consecutive measurements?  Why is this significantly different from the most common time, found in part `2a` above?

Solution:

a. The most common time (in seconds) between measurements is 3 seconds.  A 3 second difference occurs 188681 times.

`sort(table(diff(timevec)),decreasing=TRUE)[1]`

b. The mean time between consecutive measurements is 12.7873 seconds.  This is so skewed (to be large, in particular, much larger than 3 seconds!) because there are several large differences in times, e.g., when the machine collecting the data is broken for long periods.

`mean(diff(timevec))`


Question 3.

a.  Suppose that we treat "15 seconds" as a threshold in consecutive time measurements, i.e., if the machine goes more than 15 seconds without taking a measurement, we consider that the machine is temporarily broken/clogged/stuck/etc.  With this level of threshold, how many times did this particular machine (at this particular location) get stuck during June 2012?

b.  How long is longest duration when the machine was broken?  When did this occur? Specifically: when did it break, and when did it start working properly again?

c.  Find the ten longest durations for when the machine was broken; just give each such measurement in seconds.

Solution:

a. The machine gets stuck a total of 10688 times.

`sum(diff(timevec) > 15)`

b. The longest duration when the machine is broken can be achieved in either of these two equivalent ways:

`t <- which(diff(timevec) == max(diff(timevec)))`

`t <- which.max(diff(timevec))`

The longest duration when the machine was broken was 128661 seconds:

`diff(timevec)[t]`

or (equivalently) 1.489132 days:

`timevec[t+1] - timevec[t]`

The longest duration when the machine is broken is from June 17, 2012, 5:24:00 PM, to June 19, 2012, 5:08:21 AM.

`timevec[t]`

`timevec[t+1]`

c. The ten longest durations when the machine was broken (in seconds) are:

`128661 106344  62985  42282  33441  30930  26958  19416  12942   8613`

`sort(diff(timevec),decreasing=TRUE)[1:10]`


Question 4.

a. Does the device which measures the electrical conductivity ever give a false reading?  If so, when?  Give the specific times (e.g., the day(s), hours, minutes, seconds), when this occurs in June, for each such occurrence.

b. Are any of these times in `4a` the same as the one (unique) time when the temperature device gave a false reading?  (We saw, in the notes, that the temperature device had one false reading.)

c.  Does the device which measures the salinity ever give a false reading?  What evidence to you have to support this claim?

Solution:

a. We can see that there are one (or more) outliers, with **electrical conductivity** falsely reported to be about 25 or so

`plot(DF$water_electrical_conductivity)`

but the rest of the points are between 11.051 and 17.845

`range(DF$water_electrical_conductivity[DF$water_electrical_conductivity < 25])`

and there do not appear to be any outliers on the lower side:

`plot(DF$water_electrical_conductivity[DF$water_electrical_conductivity < 25])`

There are actually TWO outliers:

`which(DF$water_electrical_conductivity > 25)`

These two times both occur on June 26, 2012, at 2:36 PM and 3:33 PM:

`timevec[168443]`

`timevec[168742]`

b. The first outlier occurs at the same time as the outlier for the temperature data, i.e., at time index 168443.

c. We can see (visually) that there do not appear to be any outliers for the salinity data:

`plot(DF$water_salinity)`


Question 5.

a.  Repeat the questions from `2a`/`2b`/`3a`/`3b`/`3c`, but now use the data set from the same point on the Columbia River Estuary but at the depth of `8.2m` (the data from the questions above was measured at `2.4m` below the surface).  The data set from `8.2m` below the surface is available at `saturn03.820.A.CT_2012_06_PD0.csv`.

b.  Does the longest time in which the machine was broken in `3b` (at depth `2.4m`) correspond roughly to the same longest time in which the machine was broken in this current data set, at depth `8.2m`?  For this longest time interval, what are the times (at depth `8.2m`), when the machine did break, and when did it start working properly again?

c.  Make a plot of the temperature data at depth `8.2m`.  There is exactly one false reading in which the temperature is too high, and exactly one false reading in which the temperature is too low.  Be sure to remove these points before plotting.

Solution:

a. Download the data set from 8.2m and import it to R:

`DF820 <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.820.A.CT_2012_06_PD0.csv")`

Use strptime to convert the times:

`timevec820 <- strptime(DF820[ ,1],  "%Y/%m/%d %H:%M:%S")`

The most common time (in seconds) between measurements is 3 seconds.  A 3 second difference occurs 119805 times at depth 8.2m.

`sort(table(diff(timevec820)),decreasing=TRUE)[1]`

The mean time between consecutive measurements is 19.5147 seconds at depth 8.2m.

`mean(diff(timevec820))`

The machine gets stuck a total of 10447 times at depth 8.2m.

`sum(diff(timevec820) > 15)`

The longest duration when the machine is broken (at depth 8.2m) can be achieved in either of these two equivalent ways:

`t820 <- which(diff(timevec820) == max(diff(timevec820)))`

`t820 <- which.max(diff(timevec820))`

The longest duration when the machine was broken (at depth 8.2m) was 128853 seconds:

`diff(timevec820)[t820]`

or (equivalently) 1.491354 days:

`timevec820[t820+1] - timevec820[t820]`

The longest duration when the machine is broken (when working at depth 8.2m) is from June 17, 2012, 5:23:00 PM, to June 19, 2012, 5:10:33 AM.

`timevec820[t820]`

`timevec820[t820+1]`

The ten longest durations when the machine was broken (in seconds, at depth 8.2m) are:

`128853 100416  84018  65861  34356  26799  26070  24663  14208   8262`

`sort(diff(timevec820),decreasing=TRUE)[1:10]`

b. Yes, these are roughly the same times; again, we point out:

The longest duration when the machine is broken is from June 17, 2012, 5:24:00 PM, to June 19, 2012, 5:08:21 AM.

`timevec[t]`

`timevec[t+1]`

The longest duration when the machine is broken (when working at depth 8.2m) is from June 17, 2012, 5:23:00 PM, to June 19, 2012, 5:10:33 AM.

`timevec820[t820]`

`timevec820[t820+1]`

c. The temperature data at depth 8.2m looks like:

`plot(DF820$water_temperature)`

We remove the outlier that has temperature above 3000:

`plot(DF820$water_temperature[DF820$water_temperature < 3000])`

and then we also remove the outlier that has temperature below 7:

`plot(DF820$water_temperature[DF820$water_temperature < 3000 & DF820$water_temperature > 7])`


Question 6.

a.  We also have data from depth `13m` below the surface in the file `saturn03.1300.R.CT_2012_06_PD0.csv`.  Import this data into `R`.

b.  Is the water temperature generally highest, on average, at depth `2.4m`, `8.2m`, or `13m` below the surface?  Does your answer make intuitive sense?

Solution:

a. Download the data set from 13m and import it to R:

`DF1300 <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.1300.R.CT_2012_06_PD0.csv")`

b. The average water temperatures at depths 2.4m, 8.2m, and 13m are (respectively):

`mean(DF$water_temperature[DF$water_temperature < 500])`

`mean(DF820$water_temperature[DF820$water_temperature<3000])`

`mean(DF1300$water_temperature)`

So the average temperature is highest at depth 2.4m. This makes intuitive sense, since the water is warmer at depths that are more shallow (and gets colder and deeper depths)


Question 7.

a.  What is the average salinity of the water at depth `2.4m`?  At depth `8.2m`?  At depth `13m`?  What about the variance of the salinity at all 3 depths?  Be sure to remove any outliers, when appropriate.

b.  At depth `13m`, make a plot of time versus salinity.

c.  As we saw in `7b`, much more data is available during the first two weeks of June, as opposed to the second two weeks of June.  Make a revised plot, showing only the time versus salinity from the start of the day on June 6, through the end of the day on June 12 (i.e., for a full 7-day period).  How many cycles of the salinity do you think you see on this plot?  Is there a natural reason for this number of cycles?

Solution:

a. The average salinity at the 3 depths 2.4m, 8.2m, 13m are (respectively) 2.894666, 6.327593, 12.18957

`mean(DF$water_salinity)`

`mean(DF820$water_salinity[DF820$water_salinity<1000])`

`mean(DF1300$water_salinity)`

The variance of the salinity at the 3 depths 2.4m, 8.2m, 13m are (respectively) 10.58944, 44.93308, 93.84552

`var(DF$water_salinity)`

`var(DF820$water_salinity[DF820$water_salinity<1000])`

`var(DF1300$water_salinity)`

b. We first extract the times at depth 13m:

`timevec1300 <- strptime(DF1300[ ,1],  "%Y/%m/%d %H:%M:%S")`

and now we plot the salinity at depth 13m:

`plot(timevec1300, DF1300$water_salinity)`

c. We first convert the starttime and stoptime to seconds as needed by R (i.e., to seconds after Jan 1, 1970):

`starttime <- strptime("2012/06/06 00:00:00",  "%Y/%m/%d %H:%M:%S")`

`stoptime <- strptime("2012/06/12 23:59:59",  "%Y/%m/%d %H:%M:%S")`

Now we plot the salinity data:

`plot( timevec1300[timevec1300 >= starttime & timevec1300 <= stoptime], DF1300$water_salinity[timevec1300 >= starttime & timevec1300 <= stoptime])`

It appears that there are roughly 14 cycles of the salinity data, i.e., roughly two per day.  This makes sense, since it seems to fit with the fact that the tide comes twice per day, which has a significant effect on the salinity.


Question 8.

At depth `2.4m`, what fraction of the temperature data points are between 10 and 12?  Between 12 and 14?  Between 14 and 16?  Between 16 and 18?  Use the `tapply` function to answer all four of these questions with one line of code.

Solution:

The counts of the temperature data are as follows:

`tapply( DF$water_temperature[DF$water_temperature<500], cut(DF$water_temperature[DF$water_temperature<500], breaks=c(10,12,14,16,18)), length)`

and the percentages of the temperature in each category are:

`tapply( DF$water_temperature[DF$water_temperature<500], cut(DF$water_temperature[DF$water_temperature<500], breaks=c(10,12,14,16,18)), length)/length((DF$water_temperature[DF$water_temperature<500]))`

Question 9.

At depth `2.4m`, what is the average temperature between the start of the day on June 1 and the end of the day on June 7?

What is the average temperature between the start of the day on June 8 and the end of the day on June 14?

What is the average temperature between the start of the day on June 15 and the end of the day on June 21?

What is the average temperature between the start of the day on June 22 and the end of the day on June 28?

Use the `tapply` function to answer all four of these questions with one line of code.

[Note: The original problem statement had an off-by-one typographical error on some of the dates.]

Solution:

The average water temperatures, week by week, were: 13.80297, 14.80604, 15.38551, 16.06782

`time1 <-- strptime("2012/06/01 00:00:00",  "%Y/%m/%d %H:%M:%S")`

`time2 <- strptime("2012/06/07 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time3 <- strptime("2012/06/14 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time4 <- strptime("2012/06/21 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time5 <- strptime("2012/06/28 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`tapply( DF$water_temperature[DF$water_temperature<500], cut(timevec[DF$water_temperature<500], breaks=c(time1,time2,time3,time4,time5)), mean, na.rm=TRUE)`

Note: The original problem statement had a small typographical error on some of the dates, which is corrected here.


Question 10.

At depth `13m`, how many data points have salinity greater than 12 and temperature greater than 14?

How many data points have salinity greater than 12 and temperature at most 14?

How many data points have salinity at most 12 and temperature greater than 14?

How many data points have salinity at most 12 and temperature at most 14?

Use the `tapply` function to answer all four of these questions with one line of code.

Hint:  You will need to embed a `list` into your `tapply`, as we did in the notes file `CO2examplecontinued.R` (the second CO2 example).

Solution:

We use a tapply with two kinds of breaks, to see that the counts are as follows:

`salinity > 12 and temperature > 14 --- 387 points`

`salinity > 12 and temperature <= 14 --- 4533 points`

`salinity <= 12 and temperature > 14 --- 190603 points`

`salinity <= 12 and temperature <= 14 --- 7176 points`

[source,r]
----
tapply( DF$water_temperature[DF$water_temperature<500],
  list(
    cut( DF$water_salinity[DF$water_temperature<500], breaks=c(0,12,24) ),
    cut( DF$water_temperature[DF$water_temperature<500], breaks=c(0,14,18) )
  ), length)
----


== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]

There is also some `supplemental-data.html` provided by the ASA.

You can see http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site] too.  In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you.  Since the data itself is so large, I saved it into a common data directory:
`/data/public/dataexpo2009/`

Notes:  If you want to read ALL of the data into `R` at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data.  You are not expected to import all of the data while you are solving the questions.  You can wait until you have solved the questions, and then come back and try to get the answers with all of the data.  So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv("/data/public/dataexpo2009/2006.csv"), read.csv("/data/public/dataexpo2009/2007.csv"), read.csv("/data/public/dataexpo2009/2008.csv") )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years.

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Just loading the data itself (if you choose all of the years) might take roughly 15 or 20 minutes to accomplish.  It would be done with some code like this:  (WARNING! This will take quite a long time to load, if you load all years at once.)

`bigDF <- rbind(
read.csv("/data/public/dataexpo2009/1987.csv"),
read.csv("/data/public/dataexpo2009/1988.csv"),
read.csv("/data/public/dataexpo2009/1989.csv"),
read.csv("/data/public/dataexpo2009/1990.csv"),
read.csv("/data/public/dataexpo2009/1991.csv"),
read.csv("/data/public/dataexpo2009/1992.csv"),
read.csv("/data/public/dataexpo2009/1993.csv"),
read.csv("/data/public/dataexpo2009/1994.csv"),
read.csv("/data/public/dataexpo2009/1995.csv"),
read.csv("/data/public/dataexpo2009/1996.csv"),
read.csv("/data/public/dataexpo2009/1997.csv"),
read.csv("/data/public/dataexpo2009/1998.csv"),
read.csv("/data/public/dataexpo2009/1999.csv"),
read.csv("/data/public/dataexpo2009/2000.csv"),
read.csv("/data/public/dataexpo2009/2001.csv"),
read.csv("/data/public/dataexpo2009/2002.csv"),
read.csv("/data/public/dataexpo2009/2003.csv"),
read.csv("/data/public/dataexpo2009/2004.csv"),
read.csv("/data/public/dataexpo2009/2005.csv"),
read.csv("/data/public/dataexpo2009/2006.csv"),
read.csv("/data/public/dataexpo2009/2007.csv"),
read.csv("/data/public/dataexpo2009/2008.csv"))`

Therefore, it is probably better (instead) to test your code on (say) three years of data, e.g., 2006-2008, before working on the full data set.

Question 1.

a. Consider the departure times (`DepTime`).  What fraction of the data are missing, i.e., are stored as `NA` values?

b. Within the departure times that are recorded (i.e., that are not `NA` values), the times are stored in `hhmm` format.  So there should be at most `24*60 = 1440` such possible times.  Are there other `DepTime` values?  Are they correct or perhaps erroneous?  How many such `DepTime` values (overall) seem to be erroneous?

Solution:

a. We can find the NA values using is.na.  If we sum the result, the TRUE's (from the is.na) become 1's and the FALSE's become 0's, so we get the number of NA's.  Then we can divide by the total number of DepTime's to get the desired fraction, which is 0.01939045.

`sum(is.na(bigDF$DepTime))/length(bigDF$DepTime)`

b. There are several values that are not valid times, as we can see:

`levels(as.factor(bigDF$DepTime[!is.na(bigDF$DepTime)]))`

It is up to whether you included (or not) the times 0000 and/or 2400, i.e., how you handled the midnight time. For example, here I am allowing 0000 but not 2400, but other methods are possible. Once we have the times less than 2400, we can check to make sure that the number of minutes is less than 60, by taking a modulus by 100, i.e., by dividing by 100 and getting the remainder.

We see that, once we restrict to times less than 2400, all of the times have valid minutes:

`sum((bigDF$DepTime[!is.na(bigDF$DepTime) & (bigDF$DepTime < 2400)] %% 100) >= 60)`

So the only invalid minutes are those which are 2400 or larger:                 

`length(bigDF$DepTime[!is.na(bigDF$DepTime) & (bigDF$DepTime >= 2400)])`

So there are 2695 such values that are >= 2400.


Question 2.

a.  Which departure times are the best, for minimizing the arrival delay (`ArrDelay`)?  More specifically, if our goal is to minimize the arrival delay, which of these 4 time categories is best time of day for our departure?  Between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Or between 6 PM and 12 midnight?

b.  Which of the 4 time categories for the departure will have the highest variance for arrival delay?

c.  Now please solve `2a` and `2b` again, splitting the data not only by the best time of day but also by the airline too.  That way, we can know what time of day and which airline we might prefer to use.

Solution:

a. We use tapply to split the ArrDelay, according to the four suggested groupings of the DepTime's.

`tapply(bigDF$ArrDelay, cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), mean, na.rm=TRUE)`

The best times to depart (by this measure) are between 6 AM and 12 noon.

b. The highest variance in ArrDelay's occur for flights departing between 12 midnight and 6 AM.

`tapply(bigDF$ArrDelay, cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), var, na.rm=TRUE)`

c. Now we analyze by airline too.

`tapply(bigDF$ArrDelay, list(cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), bigDF$UniqueCarrier), mean, na.rm=TRUE)`

`tapply(bigDF$ArrDelay, list(cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), bigDF$UniqueCarrier), var, na.rm=TRUE)`


Question 3.

a.  Which 10 airports have the most departures?

b.  Which 10 airports have the most arrivals?

c.  If we reconsider `3a` and `3b`, by splitting the data year by year, are the answers to `3a` and `3b` relatively consistent from year to year?

d.  Which are the most 10 popular pairs of departure/arrival city pairs?  (For instance, `IND-to-ORD` might be one such popular pair.)

Solution:

a. The cities with the most departures are:

`ATL     ORD     DFW     DEN     LAX     PHX     IAH     LAS     DTW     EWR `

`sort(tapply(bigDF$Origin, bigDF$Origin, length),decreasing=TRUE)[1:10]`

b. The cities with the most arrivals are:

`ATL     ORD     DFW     DEN     LAX     PHX     IAH     LAS     DTW     EWR `

`sort(tapply(bigDF$Dest, bigDF$Dest, length),decreasing=TRUE)[1:10]`

c. Wrapping everything up into one line, we can do this as follows, with the columns in order from 2006 to 2008, reading left-to-right. Indeed, the answers are pretty consistent, from year to year. For most departures:

`sapply(1:3, function(j) names(sort(tapply(bigDF$Origin, list(bigDF$Origin, bigDF$Year), length)[,j], decreasing=TRUE))[1:10])`

For most arrivals:

`sapply(1:3, function(j) names(sort(tapply(bigDF$Dest, list(bigDF$Dest, bigDF$Year), length)[,j], decreasing=TRUE))[1:10])`

d. The 10 most popular departure/arrival pairs are:

`from OGG to HNL from HNL to OGG from LAX to LAS from LAS to LAX from SAN to LAX from LAX to SAN from BOS to LGA from LGA to BOS from SFO to LAX from HNL to LIH `

`sort(table(paste("from", bigDF$Origin, "to", bigDF$Dest)), decreasing=TRUE)[1:10]`


Question 4.

a.  Which 5 airports are most likely to be on time for arrivals (on average)?

b.  Which 5 airports are most likely to be on time for departures (on average)?

c.  Which 5 airports are most likely to be delayed for arrivals (on average)?

d.  Which 5 airports are most likely to be delayed for departures (on average)?

Solution:

a. these 5 airports are most likely to be on time for arrivals (on average)

`HVN        ITH        LIH        EAU        ITO        HTS        OGG        KOA        PIH        CDC`

`sort(tapply(bigDF$ArrDelay, bigDF$Dest, mean, na.rm=TRUE))[1:10]`

b. these 5 airports are most likely to be on time for departures (on average)

`GLH       WYS       PIH       HVN       ITO       COD       EKO       CDC       LIH       IYK`

`sort(tapply(bigDF$DepDelay, bigDF$Origin, mean, na.rm=TRUE))[1:10]`

c. these 5 airports are most likely to be delayed for arrivals (on average)

`MQT      OTH      ACK      SOP      HHH      ISO      MCN      EWR      TTN      CIC`

`sort(tapply(bigDF$ArrDelay, bigDF$Dest, mean, na.rm=TRUE), decreasing=TRUE)[1:10]`

d. these 5 airports are most likely to be delayed for departures (on average)

`ACK      PIR      PUB      SOP      OTH      CEC      ADK      LMT      CKB      AKN`

`sort(tapply(bigDF$DepDelay, bigDF$Origin, mean, na.rm=TRUE), decreasing=TRUE)[1:10]`


Question 5.

a.  Which is the best day of the week to fly, if you want to minimize delayed arrivals?

b.  Which portion of the flights depart on which days?

c.  What percent of flights depart between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Between 6 PM and 12 midnight?

d.  Can you study 5b and 5c simultaneously, e.g., can you give an analysis by day of the week and time of day (in tandem), so that we know precisely which days of the week and which portions of the days are busiest for departures, i.e., so that we have a finer breakdown of the departure data?

Solution:

a. The best day of the week to fly, to minimize delayed arrivals, is Saturday (DayOfWeek=6)

`tapply(bigDF$ArrDelay, bigDF$DayOfWeek, mean, na.rm=TRUE)`

b. The portions of flights, by day, is (Monday is DayOfWeek=1; Sunday is DayOfWeek=7) the following:

`0.1479634 0.1453808 0.1467945 0.1473238 0.1478121 0.1245277 0.1401976`

`tapply(bigDF$DayOfWeek, bigDF$DayOfWeek, length)/length(bigDF$DayOfWeek)`

c. The portions of flights, by time of day, is:

`12 midnight to 6 AM:  0.02610195`

`6 AM to 12 noon:      0.38311298`

`12 noon to 6 PM:      0.37239489`

`6 PM to 12 midnight:  0.21839018`

`tapply(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], cut(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], breaks=c(0,600,1200,1800,2400), include.lowest=TRUE), length)/length(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)])`

d. Also with a breakdown by day of the week, we have:

`tapply(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], list( cut(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], breaks=c(0,600,1200,1800,2400), include.lowest=TRUE),  bigDF$DayOfWeek[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)]), length)/length(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)])`


Question 6.

a.  Which 5 carriers are the most likely to be delayed?

b.  Which 5 carriers are the most likely to be on time?

Solution:

a. The most likely carriers to be delayed are:

`OH        F9        EV        NW        TZ`

`sort(tapply(bigDF$ArrDelay[bigDF$ArrDelay > 0], bigDF$UniqueCarrier[bigDF$ArrDelay > 0], length) / tapply(bigDF$ArrDelay, bigDF$UniqueCarrier, length))`

b. The most likely carriers to be on time are:

`FL        WN        9E        AQ        HA`

`sort(tapply(bigDF$ArrDelay[bigDF$ArrDelay <= 0], bigDF$UniqueCarrier[bigDF$ArrDelay <= 0], length) / tapply(bigDF$ArrDelay, bigDF$UniqueCarrier, length))`

Question 7.

a.  Give a month-by-month breakdown of the percentage of cancelled flights.

b.  What are the worst 3 months of the year for cancelled flights?  I.e., during which 3 months are the most flights cancelled?  (Since 1987 is an incomplete year, please avoid the data from 1987 for `7a` and `7b`, because we do not want to unfairly balance the months.)

Solution:

a. Here is a month-by-month breakdown of cancelled flights:

`tapply(bigDF$Cancelled[bigDF$Cancelled==1], list(bigDF$Year[bigDF$Cancelled==1], bigDF$Month[bigDF$Cancelled==1]), length)/tapply(bigDF$Cancelled, list(bigDF$Year, bigDF$Month), length)`

b. The worst months of the year for cancelled flights are, respectively, Feb (worst of all!), Dec (2nd worst), Jan (3rd worst)

`sort(tapply(bigDF$Cancelled[bigDF$Cancelled==1], list(bigDF$Month[bigDF$Cancelled==1]), length)/tapply(bigDF$Cancelled, list(bigDF$Month), length),decreasing=TRUE)[1:3]`


Question 8.

Make a plot that shows how the number of flights departing `ORD` has changed, year by year.  Then add similar data to the same plot, for the number of flights departing `IND`, year by year.

Solution:

First we get the counts:

[source,r]
----
ChiDF <- subset(bigDF, subset=bigDF$Origin == "ORD")
ChiCounts <- tapply(ChiDF$Year, ChiDF$Year, length)
IndDF <- subset(bigDF, subset=bigDF$Origin == "IND")
IndCounts <- tapply(IndDF$Year, IndDF$Year, length)
----

Then we build the dotchart:
`dotchart(rbind(ChiCounts, IndCounts))`


Question 9.

Read the documentation for the `dotchart` function.  Make a `dotchart` as follows:  The x-axis should be the percentage of the time that flights are delayed more than 30 minutes.  On the y-axis, the main groupings should be according to month, and within each month, please show O'Hare and Indianapolis as cities of departure for flights.  The data to be displayed are the `DepDelay` data for 2007 only.  So the overall plot will show, month-by-month, a comparison of the `DepDelay` data for O'Hare and Indianapolis.

Solution:

First we get the 2007 data only:

`Chi2007DF <- subset(bigDF, subset=bigDF$Origin == "ORD" & Year==2007)`

`Ind2007DF <- subset(bigDF, subset=bigDF$Origin == "IND" & Year==2007)`

Then we get the delay counts and the total counts:

`Chi2007DelayCounts <- tapply(Chi2007DF$DepDelay[Chi2007DF$DepDelay>30], Chi2007DF$Month[Chi2007DF$DepDelay>30], length)`

`Chi2007AllCounts <- tapply(Chi2007DF$DepDelay, Chi2007DF$Month, length)`

`Ind2007DelayCounts <- tapply(Ind2007DF$DepDelay[Ind2007DF$DepDelay>30], Ind2007DF$Month[Ind2007DF$DepDelay>30], length)`

`Ind2007AllCounts <- tapply(Ind2007DF$DepDelay, Ind2007DF$Month, length)`

Finally we make a matrix and then a dotchart:

`M <- rbind(Chi2007DelayCounts/Chi2007AllCounts, Ind2007DelayCounts/Ind2007AllCounts)`

`row.names(M) <- c("ORD","IND")`

`dotchart(M)`


Question 10.

Make another `dotchart`, similar to the one in question `9`, where the main groupings on the y-axis are O'Hare and Indianapolis, and within each city, display all 12 months.  Again, the data to be displayed are the `DepDelay` data for 2007.  The x-axis should again be the percentage of the time that flights are delayed more than 30 minutes.  So the overall plot will show, for each of the two cities, a month-by-month comparison of the `DepDelay` data.  If you are able, you can organize the months according to their percentage of time delayed more than 30 minutes, rather than according to alphabetic order.

Solution:

Now we rearrange the matrix from #9, switching the role of rows and columns, by taking a transpose:

`newM <- t(M)`

Then we plot the data:

`dotchart(newM)`



== Project 5

The code found in the `week6.html` Week 6 examples should be helpful in this problem set.

Question 1.

Practice using the sapply function:

a. Find, with only one line (altogether) of `sapply` code, the 5 lengths of the following 5 vectors:

* the `LakeHuron` vector,
* the `waiting` vector in the `geyser` data (remember to load the `MASS` library first)
* the `duration` vector in the `geyser` data
* the `chickwts$weight` vector
* the `mtcars$mpg` vector

b. Now find the average value stored in each of the 5 vectors, using `sapply`.

c. Check that `R` did the right thing in `1b` by manually taking the mean of each vector, using 5 separate lines of code.

d.  If you accidentally use `c` instead of `list` in `1b`, `R` just takes an average of individual values, but the average of 1 value is just the value itself, so `R` returns the full list of values.  Please give this (incorrect) behavior a try, just to see how it misbehaves!

e. Now find the variance of the values stored in each of the 5 vectors, using `sapply`.

f. Check that `R` did the right thing in `1e` by manually finding the variance of each vector, using 5 separate lines of code.

g.  If you accidentally use `c` instead of `list` in `1e`, `R` just takes the variance of each individual value, but `R` gives an `NA` when taking the variance of an individual value (you can try this, e.g., `var(3.79)` gives `NA`, so `R` returns `NA` for each value).  Please give this (incorrect) behavior a try!

h.  Examine the head of the `Cars93` data.  This data set has a lot of types of columns.  Use `sapply` to find out the kinds of classes for each of the 27 columns in this `data.frame` (using just one call to `sapply`; hint: use `class` for the function).

Solution:

a.

[source,r]
----
98 299 299  71  32
library(MASS)
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), length )
----

b.

[source,r]
----
579.004082  72.314381   3.460814 261.309859  20.090625
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), mean )
----


c.  We get the same answers as above

[source,r]
----
mean(LakeHuron)
mean(geyser$waiting)
mean(geyser$duration)
mean(chickwts$weight)
mean(mtcars$mpg)
----

d.

`sapply( c(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), mean )`

e.

[source,r]
----
1.737911  192.941101    1.317683 6095.502616   36.324103
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), var )
----

f.  We get the same answers as above

[source,r]
----
var(LakeHuron)
var(geyser$waiting)
var(geyser$duration)
var(chickwts$weight)
var(mtcars$mpg)
----

g.

`sapply( c(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), var )`

h.

[source,r]
----
head(Cars93)
sapply( Cars93, class )
----


Question 2.

a. Use the `mapply` function, with the `paste` function, and the vectors
`c("a","b","c","d","e")`
and
`c("A","B","C","D","E")`
and the parameters
`sep=""` and `USE.NAMES=FALSE`
to print these five sentences:
`[1] "The uppercase version of a is A" "The uppercase version of b is B" "The uppercase version of c is C"`
`[4] "The uppercase version of d is D" "The uppercase version of e is E"`

b. Use the `row.names` function, and the column of `population` data, both with the `state.x77` data set, as well as the `mapply` function, to print a vector of 50 sentences.  (It might be helpful to use the parameters `USE.NAMES=F` and `sep=""`.)  The vector should start with the following six sentences:
`[1] "Alabama has 3615 thousand people."        "Alaska has 365 thousand people."`
`[3] "Arizona has 2212 thousand people."        "Arkansas has 2110 thousand people."`
`[5] "California has 21198 thousand people."    "Colorado has 2541 thousand people."`

c. Revise your answer to `2a`, by actually multiplying the `population` data by 1000, so that the vector should start with the following six sentences:
`[1] "Alabama has 3615000 people."        "Alaska has 365000 people."`
`[3] "Arizona has 2212000 people."        "Arkansas has 2110000 people."`
`[5] "California has 21198000 people."    "Colorado has 2541000 people."`

Solution:

a.

`mapply( function(x,y) {paste("The uppercase version of ", x, " is ", y, sep="")} , x=c("a","b","c","d","e"), y=c("A","B","C","D","E"), USE.NAMES=FALSE )`

b.

`mapply( function(x,y) {paste(x, " has ", y, " thousand people.", sep="")} , x=row.names(state.x77), y=state.x77[ ,"Population"], USE.NAMES=FALSE )`

or we could use

`mapply( function(x,y) {paste(x, " has ", y, " thousand people.", sep="")} , x=row.names(state.x77), y=state.x77[ ,1], USE.NAMES=FALSE )`

c.

`mapply( function(x,y) {paste(x, " has ", y*1000, " people.", sep="")} , x=row.names(state.x77), y=state.x77[ ,"Population"], USE.NAMES=FALSE )`

or we could use

`mapply( function(x,y) {paste(x, " has ", y*1000, " people.", sep="")} , x=row.names(state.x77), y=state.x77[ ,1], USE.NAMES=FALSE )`

or we could even multiply the vector itself by 1000, e.g.,

`mapply( function(x,y) {paste(x, " has ", y, " people.", sep="")} , x=row.names(state.x77), y=1000*state.x77[ ,"Population"], USE.NAMES=FALSE )`


Question 3.

a. Make a `data.frame` containing all of the cars from `mtcars` with `hp>100` and `8` cylinders.  Create a new `data.frame` with these cars, displaying only the `mpg`, `cyl`, `hp`, and `qsec`.

b. Make a `data.frame` containing all of the rows describing provences from the `swiss` data set with 50% or more `Catholics` and 50% or more of `males` involved in agriculture.  Within this specific `data.frame`, find the mean and standard deviation of the `Fertility` data.

c. Make a `data.frame` containing all of the rows in the `chickwts` data for which the feed is either `horsebean` or `soybean`.  What is the average weight (altogether) across these two kinds of feed?

Solution:

a.  The cars with hp>100 and 8 cylinders have the following mpg, cyl, hp, and qsec:

`subset(mtcars, subset=hp>100 & cyl==8, select=c(mpg,cyl,hp,qsec))`

b.  The swiss data.frame, limited to data with 50% or more Catholics and 50% or more of males involved in agriculture is:

`subset(swiss, subset=Catholic>=50 & Agriculture>=50)`

The mean of the Fertility data from this smaller data.frame is  79.51667

`mean(subset(swiss, subset=Catholic>=50 & Agriculture>=50)$Fertility)`

and the standard deviation is  8.678797

`sd(subset(swiss, subset=Catholic>=50 & Agriculture>=50)$Fertility)`

c.  The chickwts data, for which the feed is either horsebean or soybean, is:

`subset( chickwts, subset=feed %in% c("horsebean","soybean"))`

The average weight across these two kinds of feed is  210.5

`mean(subset( chickwts, subset=feed %in% c("horsebean","soybean"))$weight)`


Question 4.

Step through Dr Ward's `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data.  It takes a little time to understand completely what is happening, but essentially we are able to read data from dozens of files with ease (i.e., without having to download them individually, by hand), and to extract and assemble the data in them.  Note that the time parameter in these files is the same data we had in the earlier project, but is stored differently (and, hence, extracted differently) than in the earlier project.

a.  Use the `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data to extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `2.4m`.

b.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `8.2m`, from:
`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.CT/` (Beware: The starting month is not the same for this data set, compared to the previous one.)

c.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `13.0m`, from: `http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.CT` (Beware: Again, the starting month is not the same for this data set, compared to the previous two.)

Solution:

a.

one-time code

`install.packages("ncdf")`

`library(ncdf)`

General function for getting the parameter names from SATURN 03, in a given folder (myfolder), for a given date (mydate)

[source,r]
----
getparameternames <- function(myfolder, mydate) {
  mync <- open.ncdf( paste("http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/", myfolder, "/", mydate, ".nc", sep="") )
  mycount <- mync$nvars
  mynames <- sapply( c(1:mycount), function(x) {mync$var[[x]]$name} )
  close.ncdf(mync)
  mynames
}
----

General function for getting the data from SATURN 03, in a given folder (myfolder), for a given date (mydate), for a given parameter (myparameter)

[source,r]
----
getdata <- function(myfolder, mydate, myparameter) {
  mync <- open.ncdf( paste("http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/", myfolder, "/", mydate, ".nc", sep="") )
  myvec <- get.var.ncdf(mync, mync$var[[myparameter]])
  close.ncdf(mync)
  myvec
}
----

General function for getting the times from SATURN 03, in a given folder (myfolder), for a given date (mydate)

[source,r]
----
gettimes <- function(myfolder, mydate) {
  mync <- open.ncdf( paste("http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/", myfolder, "/", mydate, ".nc", sep="") )
  mytimes <- get.var.ncdf(mync,"time")
  close.ncdf(mync)
  mytimes
}
----

retrieving the data from the 2.4m data sets

`v <- paste( rep(2009:2014,each=12),  rep(sprintf("%02d", 1:12), 6), sep="" )`

Finally, we throw away the months Oct, Nov, Dec 2014

`v <- v[-(70:72)]`

and we throw away Jan through July 2009:

`v <- v[-(1:7)]`

`v`

We get the name data for all 62 months for all 3 variables, at 2.4m:

`namematrix4a240 <- mapply( FUN=getparameternames, myfolder="saturn03.240.A.CT", mydate=v, USE.NAMES=FALSE)`

Parameters 1, 2, 3 are (respectively) temperature, electrical conductivity, salinity:

[source,r]
----
namematrix4a240[1, ]
namematrix4a240[2, ]
namematrix4a240[3, ]
----

this gives us a list of the month-by-month temperature data at 2.4m, which we called "temperature240":

`temperature240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.CT", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month conductivity data at 2.4m, which we called "conductivity240":

`conductivity240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.CT", mydate=v, myparameter=2, USE.NAMES=FALSE)`

this gives us a list of the month-by-month salinity data at 2.4m, which we called "salinity240":

`salinity240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.CT", mydate=v, myparameter=3, USE.NAMES=FALSE)`

Now we get the times for all months at 2.4m:

`mytimes4a240 <- mapply(FUN=gettimes, myfolder="saturn03.240.A.CT", mydate=v, USE.NAMES=FALSE)`

b.

retrieving the data from the 8.2m data sets

`v <- paste( rep(2009:2014,each=12),  rep(sprintf("%02d", 1:12), 6), sep="" )`

Finally, we throw away the months Oct, Nov, Dec 2014

`v <- v[-(70:72)]`

and we throw away Jan through Oct 2009:

`v <- v[-(1:10)]`

`v`

We get the name data for all 59 months for all 3 variables, at 8.2m:

`namematrix4b820 <- mapply( FUN=getparameternames, myfolder="saturn03.820.A.CT", mydate=v, USE.NAMES=FALSE)`

Parameters 1, 2, 3 are (respectively) temperature, electrical conductivity, salinity:

[source,r]
----
namematrix4b820[1, ]
namematrix4b820[2, ]
namematrix4b820[3, ]
----

this gives us a list of the month-by-month temperature data at 8.2m, which we called "temperature820":

`temperature820 <- mapply(FUN=getdata, myfolder="saturn03.820.A.CT", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month conductivity data at 8.2m, which we called "conductivity820":

`conductivity820 <- mapply(FUN=getdata, myfolder="saturn03.820.A.CT", mydate=v, myparameter=2, USE.NAMES=FALSE)`

this gives us a list of the month-by-month salinity data at 8.2m, which we called "salinity820":

`salinity820 <- mapply(FUN=getdata, myfolder="saturn03.820.A.CT", mydate=v, myparameter=3, USE.NAMES=FALSE)`

Now we get the times for all months at 8.2m:

`mytimes4b820 <- mapply(FUN=gettimes, myfolder="saturn03.820.A.CT", mydate=v, USE.NAMES=FALSE)`

If we observe carefully, we notice that some of the variables are stored inconsistently.

We fix that here.

[source,r]
----
sapply(46:59, function(x) {
  temporaryname1 <<- namematrix4b820[3,x];
  temporaryname2 <<- namematrix4b820[1,x];
  temporaryname3 <<- namematrix4b820[2,x];
  namematrix4b820[1,x] <<- temporaryname1;
  namematrix4b820[2,x] <<- temporaryname2;
  namematrix4b820[3,x] <<- temporaryname3;
  temporarytemperature820 <<- salinity820[[x]];
  temporaryconductivity820 <<- temperature820[[x]];
  temporarysalinity820 <<- conductivity820[[x]];
  temperature820[[x]] <<- temporarytemperature820;
  conductivity820[[x]] <<- temporaryconductivity820;
  salinity820[[x]] <<- temporarysalinity820;
} )
----

c.

retrieving the data from the 13.0m data sets

`v <- paste( rep(2008:2014,each=12),  rep(sprintf("%02d", 1:12), 7), sep="" )`

Finally, we throw away the months Oct, Nov, Dec 2014

`v <- v[-(82:84)]`

and we throw away Jan through Mar 2008:

`v <- v[-(1:3)]`

`v`

We get the name data for all 78 months for all 3 variables, at 13.0m:

`namematrix4c1300 <- mapply( FUN=getparameternames, myfolder="saturn03.1300.R.CT", mydate=v, USE.NAMES=FALSE)`

Parameters 1, 2, 3 are (respectively) temperature, electrical conductivity, salinity:

[source,r]
----
namematrix4c1300[1, ]
namematrix4c1300[2, ]
namematrix4c1300[3, ]
----

this gives us a list of the month-by-month temperature data at 13.0m, which we called "temperature1300":

`temperature1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.CT", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month conductivity data at 13.0m, which we called "conductivity1300":

`conductivity1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.CT", mydate=v, myparameter=2, USE.NAMES=FALSE)`

this gives us a list of the month-by-month salinity data at 13.0m, which we called "salinity1300":

`salinity1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.CT", mydate=v, myparameter=3, USE.NAMES=FALSE)`

Now we get the times for all months at 13.0m:

`mytimes4c1300 <- mapply(FUN=gettimes, myfolder="saturn03.1300.R.CT", mydate=v, USE.NAMES=FALSE)`

If we observe carefully, we notice that some of the variables are stored inconsistently.

We fix that here.

[source,r]
----
sapply(65:78, function(x) {
  temporaryname2 <<- namematrix4c1300[3,x];
  temporaryname3 <<- namematrix4c1300[2,x];
  namematrix4c1300[2,x] <<- temporaryname2;
  namematrix4c1300[3,x] <<- temporaryname3;
  temporaryconductivity1300 <<- salinity1300[[x]];
  temporarysalinity1300 <<- conductivity1300[[x]];
  conductivity1300[[x]] <<- temporaryconductivity1300;
  salinity1300[[x]] <<- temporarysalinity1300;
} )
----



Question 5.

Extract the `Phycoerythrin` and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Phycoerythrin/`

Solution:

retrieving the data from the 2.4m data sets

`v <- paste( rep(2010:2012,each=12),  rep(sprintf("%02d", 1:12), 3), sep="" )`

Finally, we throw away the months Sep, Oct, Nov, Dec 2012

`v <- v[-(33:36)]`

and we throw away Jan through June 2010:

`v <- v[-(1:6)]`

`v`

We get the name data for all 26 months for the 1 variable, at 2.4m:

`namematrix5a240 <- mapply( FUN=getparameternames, myfolder="saturn03.240.A.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`

`namematrix5a240`

this gives us a list of the month-by-month phycoerythrin data at 2.4m, which we called "phycoerythrin240":

`phycoerythrin240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.Phycoerythrin", mydate=v, myparameter=1, USE.NAMES=FALSE)`

Now we get the times for all months at 2.4m:

`mytimes5a240 <- mapply(FUN=gettimes, myfolder="saturn03.240.A.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`

retrieving the data from the 8.2m data sets

`v <- paste( rep(2010:2012,each=12),  rep(sprintf("%02d", 1:12), 3), sep="" )`

Finally, we throw away the months Sep, Oct, Nov, Dec 2012

`v <- v[-(33:36)]`

and we throw away Jan through June 2010:

`v <- v[-(1:6)]`

`v`

We get the name data for all 26 months for the 1 variable, at 8.2m:

`namematrix5b820 <- mapply( FUN=getparameternames, myfolder="saturn03.820.A.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`

`namematrix5b820`

this gives us a list of the month-by-month phycoerythrin data at 8.2m, which we called "phycoerythrin820":

`phycoerythrin820 <- mapply(FUN=getdata, myfolder="saturn03.820.A.Phycoerythrin", mydate=v, myparameter=1, USE.NAMES=FALSE)`

Now we get the times for all months at 8.2m:

`mytimes5b820 <- mapply(FUN=gettimes, myfolder="saturn03.820.A.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`

retrieving the data from the 13.0m data sets

`v <- paste( rep(2010:2012,each=12),  rep(sprintf("%02d", 1:12), 3), sep="" )`

Finally, we throw away the months Sep, Oct, Nov, Dec 2012

`v <- v[-(33:36)]`

and we throw away Jan through June 2010:

`v <- v[-(1:6)]`

`v`

We get the name data for all 26 months for the 1 variable, at 13.0m:

`namematrix5c1300 <- mapply( FUN=getparameternames, myfolder="saturn03.1300.R.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`

`namematrix5c1300`

this gives us a list of the month-by-month phycoerythrin data at 13.0m, which we called "phycoerythrin1300":

`phycoerythrin1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.Phycoerythrin", mydate=v, myparameter=1, USE.NAMES=FALSE)`

Now we get the times for all months at 13.0m:

`mytimes5c1300 <- mapply(FUN=gettimes, myfolder="saturn03.1300.R.Phycoerythrin", mydate=v, USE.NAMES=FALSE)`


Question 6.

Extract the `Oxygen Concentration` (`oxygen`), `Oxygen Saturation` (`oxygensat`), and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Oxygen/`

Solution:

retrieving the data from the 2.4m data sets

`v <- paste( rep(2010:2014,each=12),  rep(sprintf("%02d", 1:12), 5), sep="" )`

Finally, we throw away the months Oct, Nov, Dec 2014

`v <- v[-(58:60)]`

and we throw away Jan through May 2010:

`v <- v[-(1:5)]`

`v`

We get the name data for all 52 months for both variables, at 2.4m:

`namematrix6a240 <- mapply( FUN=getparameternames, myfolder="saturn03.240.A.Oxygen", mydate=v, USE.NAMES=FALSE)`

`namematrix6a240[1, ]`

`namematrix6a240[2, ]`

this gives us a list of the month-by-month oxygen data at 2.4m, which we called "oxygen240":

`oxygen240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.Oxygen", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month oxygensat data at 2.4m, which we called "oxygensat240":

`oxygensat240 <- mapply(FUN=getdata, myfolder="saturn03.240.A.Oxygen", mydate=v, myparameter=2, USE.NAMES=FALSE)`

Now we get the times for all months at 2.4m:

`mytimes6a240 <- mapply(FUN=gettimes, myfolder="saturn03.240.A.Oxygen", mydate=v, USE.NAMES=FALSE)`

retrieving the data from the 8.2m data sets

`v <- paste( rep(2010:2014,each=12),  rep(sprintf("%02d", 1:12), 5), sep="" )`

Finally, we throw away the months Oct, Nov, Dec 2014

`v <- v[-(58:60)]`

and we throw away Jan through May 2010:

`v <- v[-(1:5)]`

`v`

We get the name data for all 52 months for both variables, at 8.2m:

`namematrix6b820 <- mapply( FUN=getparameternames, myfolder="saturn03.820.A.Oxygen", mydate=v, USE.NAMES=FALSE)`

`namematrix6b820[1, ]`

`namematrix6b820[2, ]`

this gives us a list of the month-by-month oxygen data at 8.2m, which we called "oxygen820":

`oxygen820 <- mapply(FUN=getdata, myfolder="saturn03.820.A.Oxygen", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month oxygensat data at 8.2m, which we called "oxygensat820":

`oxygen820sat <- mapply(FUN=getdata, myfolder="saturn03.820.A.Oxygen", mydate=v, myparameter=2, USE.NAMES=FALSE)`

Now we get the times for all months at 8.2m:

`mytimes6b820 <- mapply(FUN=gettimes, myfolder="saturn03.820.A.Oxygen", mydate=v, USE.NAMES=FALSE)`

retrieving the data from the 13.0m data sets

`v <- paste( rep(2010:2014,each=12),  rep(sprintf("%02d", 1:12), 5), sep="" )`

Finally, we throw away the months Sep, Oct, Nov, Dec 2012

`v <- v[-(58:60)]`

and we throw away Jan through May 2010:

`v <- v[-(1:5)]`

`v`

We get the name data for all 52 months for both variables, at 13.0m:

`namematrix6c1300 <- mapply( FUN=getparameternames, myfolder="saturn03.1300.R.Oxygen", mydate=v, USE.NAMES=FALSE)`

`namematrix6c1300[1, ]`

`namematrix6c1300[2, ]`

this gives us a list of the month-by-month oxygen data at 13.0m, which we called "oxygen1300":

`oxygen1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.Oxygen", mydate=v, myparameter=1, USE.NAMES=FALSE)`

this gives us a list of the month-by-month oxygensat data at 13.0m, which we called "oxygensat1300":

`oxygensat1300 <- mapply(FUN=getdata, myfolder="saturn03.1300.R.Oxygen", mydate=v, myparameter=2, USE.NAMES=FALSE)`

Now we get the times for all months at 13.0m:

`mytimes6c1300 <- mapply(FUN=gettimes, myfolder="saturn03.1300.R.Oxygen", mydate=v, USE.NAMES=FALSE)`

If we observe carefully, we notice that some of the variables are stored inconsistently.

We fix that here.

[source,r]
----
sapply(39:52, function(x) {
  temporaryname1 <<- namematrix6c1300[2,x];
  temporaryname2 <<- namematrix6c1300[1,x];
  namematrix6c1300[1,x] <<- temporaryname1;
  namematrix6c1300[2,x] <<- temporaryname2;
  temporaryoxygen1300 <<- oxygensat1300[[x]];
  temporaryoxygensat1300 <<- oxygen1300[[x]];
  oxygen1300[[x]] <<- temporaryoxygen1300;
  oxygensat1300[[x]] <<- temporaryoxygensat1300;
} )
----



Question 7.

a. For each of the 3 types of data listed above (in questions `4`, `5`, `6`) at each of the 3 depths, find the number of data points per month.  For instance, starting with the `temperature`/`conductivity`/`salinity` data at depth `2.4m`, find the number of data points per month.  Then do the same for `8.2m` and for `13.0m`.  Then do this again for the `Phycoerythrin` data.  Then do it again for the `Oxygen` data.  To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 of the saturn03.240.A.CT data contains 202,702 data points at depth 2.4m.`

b. Re-calculate your answer to `7a`, so that it is normalized according to the number of days in the month. In other words, get the number of data points divided by the number of days in the month. To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 contains an average of 6756.733 data points per day, during the 30 day period, for a total of 202,702 data points during the month, at depth 2.4m.`

Solution:

a.  For the temperature/conductivity/salinity data:

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.CT data contains ",
        length(z), " data points at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 6)[-c(1:7,70:72)], 
        y=rep(2009:2014,each=12)[-c(1:7,70:72)], z=temperature240, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.820.A.CT data contains ",
        length(z), " data points at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 6)[-c(1:10,70:72)], 
        y=rep(2009:2014,each=12)[-c(1:10,70:72)], z=temperature820, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.1300.R.CT data contains ",
        length(z), " data points at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 7)[-c(1:3,82:84)], 
        y=rep(2008:2014,each=12)[-c(1:3,82:84)], z=temperature1300, USE.NAMES=FALSE )
----

For the phycoerythrin data:

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Phycoerythrin data contains ",
        length(z), " data points at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin240, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.820.A.Phycoerythrin data contains ",
        length(z), " data points at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin820, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.1300.R.Phycoerythrin data contains ",
        length(z), " data points at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin1300, USE.NAMES=FALSE )
----

For the oxygen data:

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Oxygen data contains ",
        length(z), " data points at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen240, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Oxygen data contains ",
        length(z), " data points at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen820, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Oxygen data contains ",
        length(z), " data points at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen1300, USE.NAMES=FALSE )
----

b.  For the temperature/conductivity/salinity data:

[source,r]
----
days<-rep(c(31,28,31,30,31,30,31,31,30,31,30,31), 6)[-c(1:7,70:72)]
days[31] <- 29
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.CT data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 6)[-c(1:7,70:72)], 
        y=rep(2009:2014,each=12)[-c(1:7,70:72)], z=temperature240, d=days, USE.NAMES=FALSE )
----

[source,r]
----
days<-rep(c(31,28,31,30,31,30,31,31,30,31,30,31), 6)[-c(1:10,70:72)]
days[28] <- 29
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.820.A.CT data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 6)[-c(1:10,70:72)], 
        y=rep(2009:2014,each=12)[-c(1:10,70:72)], z=temperature820, d=days, USE.NAMES=FALSE )
----

[source,r]
----
days<-rep(c(31,28,31,30,31,30,31,31,30,31,30,31), 7)[-c(1:3,82:84)]
days[47] <- 29
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.1300.R.CT data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 7)[-c(1:3,82:84)], 
        y=rep(2008:2014,each=12)[-c(1:3,82:84)], z=temperature1300, d=days, USE.NAMES=FALSE )
----

For the phycoerythrin data:

[source,r]
----
days<-rep(c(31,28,31,30,31,30,31,31,30,31,30,31), 3)[-c(1:6,33:36)]
days[20] <- 29
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Phycoerythrin data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin240, d=days, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.820.A.Phycoerythrin data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin820, d=days, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.1300.R.Phycoerythrin data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 3)[-c(1:6,33:36)], 
        y=rep(2010:2012,each=12)[-c(1:6,33:36)], z=phycoerythrin1300, d=days, USE.NAMES=FALSE )
----

For the oxygen data:

[source,r]
----
days<-rep(c(31,28,31,30,31,30,31,31,30,31,30,31), 5)[-c(1:5,58:60)]
days[21] <- 29
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.240.A.Oxygen data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 2.4m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen240, d=days, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.820.A.Oxygen data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 8.2m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen820, d=days, USE.NAMES=FALSE )
----

[source,r]
----
mapply( function(x,y,z,d) {paste("Month ", x, " of year ", y, " of the saturn03.1300.R.Oxygen data contains ",
        "an average of ", length(z)/d, " data points per day, during the ", d, " day period, for a total of ",                       
        length(z), " data points during the month, at depth 13.0m", sep="")}, x=rep(sprintf("%02d", 1:12), 5)[-c(1:5,58:60)], 
        y=rep(2010:2014,each=12)[-c(1:5,58:60)], z=oxygen1300, d=days, USE.NAMES=FALSE )
----



Question 8.

a.  Extract the `temperature` data from the `SATURN03` station for June 2012 at depth `2.4m`.  There are 202702 data points.  Save these in a variable called `tempdata`.  Also get the analogous 202702 `time` data points.  Save these in a variable called `temptimes`.

b.  Extract the `oxygen` saturation data (the 2nd parameter in the data set `saturn03.240.A.Oxygen`) from the `SATURN03` station for June 2012.  There are 15725 data points.  Save these in a variable called `oxydata`.  Also get the analogous 15725 time data points.  Save these in a variable called `oxytimes`.

c.  Notice that we would be hard-pressed to compare the `temperature` and `oxygen saturation` data, because there are vastly different amounts of data in the two vectors and (perhaps more importantly) they were measured at different points in time.  We can, however, build a function that predicts the behavior of the temperature data at ALL points in time, and then use it to figure out how the temperature data would have behaved, if it was measured at the same 15725 `time` points as the `oxygen saturation` data, and then we could compare the `temperature` and `oxygen saturation` data.  This can be done as follows:
`tempfunction <- approxfun(temptimes, tempdata)`
This makes `f` into a function that can predict the `temperature` behavior at any `time` we like.  Then we run the function on the `oxygen saturation` times, to see how `temperature` would have behaved at the 15725 times when `oxygen saturation` was measured:
`tempatoxygentimes <- tempfunction(oxytimes)`
Finally, we can plot the `temperature` versus the `oxygen saturation` data this way:
`plot(tempatoxygentimes,oxydata)`

Solution:

a. The temperature data are here:

`tempdata <- getdata(myfolder="saturn03.240.A.CT", mydate="201206", myparameter=1)`

The analogous times are here:

`temptimes <- gettimes(myfolder="saturn03.240.A.CT", mydate="201206")`

b. The oxygen data are here:

`oxydata <- getdata(myfolder="saturn03.240.A.Oxygen", mydate="201206", myparameter=1)`

The analogous times are here:

`oxytimes <- gettimes(myfolder="saturn03.240.A.Oxygen", mydate="201206")`

c.

`tempfunction <- approxfun(temptimes, tempdata)`

This makes f into a function that can predict the temperature behavior at any time we like. Then we run the function on the oxygen saturation times, to see how temperature would have behaved at the 15725 times when oxygen saturation was measured:

`tempatoxygentimes <- tempfunction(oxytimes)`

Finally, we can plot the temperature versus the oxygen saturation data this way:

`plot(tempatoxygentimes,oxydata)`


Question 9.

a.  Make comparisons between some of the other variables, in the style of how we did things in question `8`.

b.  Which pair of variables (in the June 2012 data sets) seem to be the most strongly correlated?  Why do you think so?

c.  What could go wrong with the method discussed in `8` and `9`?  Hint: for instance, in part `8c`, take a look at:
`range(temptimes)`
`plot( tempfunction(seq(1338537601,1341129599,by=100) ) )`
How could we potentially fix the problem that happens when missing data occurs?  [You do not have to actually fix it; but briefly mention some way that you might fix it.]  Can you see this problem in the plot?  [We will discuss this problem more, in a set of future questions, in another project.]

Solution:

a. The conductivity data are here:

`conductivitydata <- getdata(myfolder="saturn03.240.A.CT", mydate="201206", myparameter=2)`

`saldata <- getdata(myfolder="saturn03.240.A.CT", mydate="201206", myparameter=3)`

The analogous times are the same as temptimes

b. Answers will vary.

The salinity and conductivity data sure seem to be pretty strongly correlated. Perhaps this has something to do with the nature of electrical conductivity, and how it is affected by the salinity of the water.

`plot(saldata[conductivitydata<20], conductivitydata[conductivitydata<20])`

c. Answers will vary.

One definitely potential problem, however, is that there are "gaps" in the data, and any kind of imputation of values that happens within a gap will be somewhat artificial, e.g., would miss the natural oscillations in the data.

For instance, the temperature data has natural oscillations, but those are missed if we naively try to impute data within the gaps.


Question 10.

a.  There are 7 data sets inside the directory: `/data/public/NARR/pressure`
How many variables do they each contain?

b.  How many pieces of data does the `lat` variable contain in each file?  How about the `lon` variable?  How about the `Lambert Conformal` variable?  Are all of the `lat` variables identical across all 7 files?  If so, how do you know?  If not, how are they different?  What about the `lon` variable?  What about the `Lambert Conformal` variable?

c.  What are the sizes (i.e., dimensions) of the 4th variable in each of the 7 files?  What percent of the 4th variable is missing in each of the 7 files?

d.  If you store the `time` vector from a file in a vector `t`, then the code:  `format(as.POSIXct(3600*t, origin="1800-01-01"), tz="UTC+0:00")`  will convert the time into a human-readable format.  The `3600` converts the hours into seconds, and the seconds are given in units after January 1, 1800.  (Dr Ward fiddled around with this for awhile to figure this out.)  Question:  Do all 7 files have the same time vector?

e.  What is the `time` unit between consecutive times in each of these vectors?

Solution:

a. Each of the 7 files contains 4 variables:

[source,r]
----
getparameternames <- function(mytype, mydate) {
  mync <- open.ncdf( paste("/data/public/NARR/pressure/", mytype, ".", mydate, ".nc", sep="") )
  mycount <- mync$nvars
  mynames <- sapply( c(1:mycount), function(x) {mync$var[[x]]$name} )
  close.ncdf(mync)
  mynames
}
----

[source,r]
----
getparameternames("air", "201206")
getparameternames("hgt", "201206")
getparameternames("omega", "201206")
getparameternames("shum", "201206")
getparameternames("tke", "201206")
getparameternames("uwnd", "201206")
getparameternames("vwnd", "201206")
----

b. 

[source,r]
----
getdata <- function(mytype, mydate, myparameter) {
  mync <- open.ncdf( paste("/data/public/NARR/pressure/", mytype, ".", mydate, ".nc", sep="") )
  myvec <- get.var.ncdf(mync, mync$var[[myparameter]])
  close.ncdf(mync)
  myvec
}
----

Getting the lat data:

[source,r]
----
airlatdata <- getdata("air", "201206", 1)
hgtlatdata <- getdata("hgt", "201206", 1)
omegalatdata <- getdata("omega", "201206", 1)
shumlatdata <- getdata("shum", "201206", 1)
tkelatdata <- getdata("tke", "201206", 1)
uwndlatdata <- getdata("uwnd", "201206", 1)
vwndlatdata <- getdata("vwnd", "201206", 1)
----

Each of the lat variables contains 96673 pieces of data:

`sapply(list(airlatdata, hgtlatdata, omegalatdata, shumlatdata, tkelatdata, uwndlatdata, vwndlatdata), length)`

`dim(airlatdata)`

Each of the lat variables contains the exact same data:

`sapply(list(airlatdata, hgtlatdata, omegalatdata, shumlatdata, tkelatdata, uwndlatdata, vwndlatdata), function(x) { sum(x != airlatdata)})`

Getting the lon data:

[source,r]
----
airlondata <- getdata("air", "201206", 2)
hgtlondata <- getdata("hgt", "201206", 2)
omegalondata <- getdata("omega", "201206", 2)
shumlondata <- getdata("shum", "201206", 2)
tkelondata <- getdata("tke", "201206", 2)
uwndlondata <- getdata("uwnd", "201206", 2)
vwndlondata <- getdata("vwnd", "201206", 2)
----

Each of the lon variables contains 96673 pieces of data:

`sapply(list(airlondata, hgtlondata, omegalondata, shumlondata, tkelondata, uwndlondata, vwndlondata), length)`

`dim(airlondata)`

Each of the lon variables contains the exact same data:

`sapply(list(airlondata, hgtlondata, omegalondata, shumlondata, tkelondata, uwndlondata, vwndlondata), function(x) { sum(x != airlondata)})`

Getting the Lambert Conformal data:

[source,r]
----
airlcdata <- getdata("air", "201206", 3)
hgtlcdata <- getdata("hgt", "201206", 3)
omegalcdata <- getdata("omega", "201206", 3)
shumlcdata <- getdata("shum", "201206", 3)
tkelcdata <- getdata("tke", "201206", 3)
uwndlcdata <- getdata("uwnd", "201206", 3)
vwndlcdata <- getdata("vwnd", "201206", 3)
----

Each of the Lambert Conformal variables contains 96673 pieces of data:

`sapply(list(airlcdata, hgtlcdata, omegalcdata, shumlcdata, tkelcdata, uwndlcdata, vwndlcdata), length)`

`length(airlcdata)`

Each of the Lambert Conformal variables contains the exact same data:

`sapply(list(airlcdata, hgtlcdata, omegalcdata, shumlcdata, tkelcdata, uwndlcdata, vwndlcdata), function(x) { sum(x != airlcdata)})`

c. Getting the 4th variable:

[source,r]
----
airdata <- getdata("air", "201206", 4)
hgtdata <- getdata("hgt", "201206", 4)
omegadata <- getdata("omega", "201206", 4)
shumdata <- getdata("shum", "201206", 4)
tkedata <- getdata("tke", "201206", 4)
uwnddata <- getdata("uwnd", "201206", 4)
vwnddata <- getdata("vwnd", "201206", 4)
----

The lengths of each of these variables are:

`sapply(list(airdata,hgtdata,omegadata,shumdata,tkedata,uwnddata,vwnddata),length)`

The dimensions of each of these variables are:

`sapply(list(airdata,hgtdata,omegadata,shumdata,tkedata,uwnddata,vwnddata),dim)`

The percentage of missing data in each of the 7 files is:

`sapply(list(airdata,hgtdata,omegadata,shumdata,tkedata,uwnddata,vwnddata), function(x) {sum(is.na(x))/length(x)} )`

d. Getting the times:

[source,r]
----
gettimes <- function(mytype, mydate) {
  mync <- open.ncdf( paste("/data/public/NARR/pressure/", mytype, ".", mydate, ".nc", sep="") )
  mytimes <- get.var.ncdf(mync,"time")
  close.ncdf(mync)
  mytimes
}
----

[source,r]
----
airtimes <- gettimes("air", "201206")
hgttimes <- gettimes("hgt", "201206")
omegatimes <- gettimes("omega", "201206")
shumtimes <- gettimes("shum", "201206")
tketimes <- gettimes("tke", "201206")
uwndtimes <- gettimes("uwnd", "201206")
vwndtimes <- gettimes("vwnd", "201206")
----

Each of the times vectors contains the exact same data:

`sapply(list(airtimes, hgttimes, omegatimes, shumtimes, tketimes, uwndtimes, vwndtimes), function(x) { sum(x != airtimes)})`

e. There are 3 hours between consecutive times in each of the vectors.

`airformattedtimes <- format(as.POSIXct(airtimes*3600, origin="1800-01-01"), tz="UTC+0:00")`

`head(airformattedtimes)`



== Project 6

The code found in the `week8.html` Week 8 examples should be helpful in this problem set.

Question 1.

Compare the 3 variables found in the first `SATURN03` data set we studied, namely, the `saturn03.240.A.CT_2012_06_PD0.csv` data set, from depth `2.4m`.  Compare them in pairs, to see if any pair of them yields a very good linear model. In all of these cases, be sure to remove any outliers, if necessary.

a. Make a simple linear regression to try to predict the `electrical conductivity` from the `temperature`.

b. Make a simple linear regression to try to predict the 'salinity' from the 'temperature'.

c. Make a simple linear regression model to try to predict the 'electrical conductivity' from the 'salinity'.

d. Which one of these linear models seems most amenable to linear modeling?  Why?

Solution:

`DF <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.240.A.CT_2012_06_PD0.csv")`

`head(DF)`

a.

[source,r]
----
plot(DF$water_temperature[DF$water_temperature<500 & DF$water_electrical_conductivity<20],
     DF$water_electrical_conductivity[DF$water_temperature<500 & DF$water_electrical_conductivity<20])
mylm <- lm( DF$water_electrical_conductivity[DF$water_temperature<500 & DF$water_electrical_conductivity<20]
            ~ DF$water_temperature[DF$water_temperature<500 & DF$water_electrical_conductivity<20])
abline(mylm)
----

b.

[source,r]
----
plot(DF$water_temperature[DF$water_temperature<500], DF$water_salinity[DF$water_temperature<500])
mylm <- lm( DF$water_salinity[DF$water_temperature<500]
            ~ DF$water_temperature[DF$water_temperature<500])
abline(mylm)
----

c.

[source,r]
----
plot(DF$water_salinity[DF$water_electrical_conductivity<20], DF$water_electrical_conductivity[DF$water_electrical_conductivity<20])
mylm <- lm( DF$water_electrical_conductivity[DF$water_electrical_conductivity<20]
            ~ DF$water_salinity[DF$water_electrical_conductivity<20])
abline(mylm)
----

d.  The last of these linear models seems most amenable to linear modeling, because salinity and electrical conductivity seem to be (perhaps) almost linearly related.

We can also read the summaries of the 3 linear models in 1a, 1b, and 1c, to make a more precise comparison.


Question 2.

a. Make a simple linear regression model to predict the `mpg` from the `mtcars` data, based on the `hp`.  Plot the two variables, along with the line suggested by a simple linear regression model.

b. Make a multiple regression model to predict the `mpg` from the `mtcars` data, based on the `hp` and the `disp`.

c. Using the multiple regression model, what kind of `mpg` might we guess that a car has, if it has 147 `hp` and 230 `disp`?

Solution:

a.

`mylm <- lm( mtcars$mpg ~ mtcars$hp)`

`plot( mtcars$hp, mtcars$mpg)`

`abline(mylm)`

b.

`mylm <- lm( mtcars$mpg ~ mtcars$hp + mtcars$disp)`

c.

`summary(mylm)`

So we could estimate the mpg to be about  20.10484  if the car has 147 hp and 230 disp.

`30.735904 - 0.024840*147 - 0.030346*230`


Question 3.

a. Load the 1990 airline data from the `dataexpo` into a `data.frame`.

b. Use the `subset` command to extract only the flights from June 1990.

c. Build a simple linear regression model that predicts the arrival delays from the departure delays.

d. Plot both the delays, putting the arrival delays on the y-axis and the departure delays on the x-axis.

e. Draw the line from the simple linear regression model on the plot.

f. Repeat steps `3c` through `3e`, removing the outliers, e.g., removing the flights with departure delays that are more than `500` and removing those that are less than `-50`.  I.e., restrict attention to flights with departure delays between `-50` and `500`.

Solution:

a.  First we get the 1990 data.

`DF <- read.csv("/data/public/dataexpo2009/1990.csv")`

b.  Next, we take a subset with just the flights from June 1990.

`juneDF <- subset(DF, Month==6)`

c.  To predict the arrival delays from the departure delays, we build a linear model:

`mylm <- lm( juneDF$ArrDelay ~ juneDF$DepDelay)`

d.  Here is a plot of both types of delays:

`plot( juneDF$DepDelay, juneDF$ArrDelay )`

e.  Now we add the regression line

`abline(mylm)`

f.  Now we do these steps again, removing some of the outliers.

[source,r]
----
mylm <- lm( juneDF$ArrDelay[juneDF$DepDelay < 500 & juneDF$DepDelay > -50]
              ~ juneDF$DepDelay[juneDF$DepDelay < 500 & juneDF$DepDelay > -50])
plot( juneDF$DepDelay[juneDF$DepDelay < 500 & juneDF$DepDelay > -50],
              juneDF$ArrDelay[juneDF$DepDelay < 500 & juneDF$DepDelay > -50] )
abline(mylm)
----


Question 4.

a.  Generate 100 (continuous) uniform random numbers, uniformly distributed between 0 and 1.

b.  For each uniform random number `U` in part `a`, define `V = -log(U)/3`.  Make this transform for all 100 numbers from `4a`.

c.  Generate 100 exponential random numbers with rate `3`.

d.  Use a `qqplot` to convince yourself that the numbers from `4b` have the same kind of distribution as the numbers in `4c`.  I.e., if `U` is a continuous uniform random variable, then `-log(U)/3` is an exponential random variable with rate `3`, i.e., with mean `1/3`.

e.  Re-do parts `4a` through `4d` with millions of numbers instead of just 100 numbers, to reinforce this notion in your mind.

Solution:

a. Here are 100 (continuous) Uniform random numbers, uniformly distributed between 0 and 1:

`U <- runif(100)`

b. Now we make the required transformation:

`V <- -log(U)/3`

c. Here are 100 Exponential random numbers, with rate 3

`X <- rexp(100, rate=3)`

d. Here is the desired qqplot; notice that it almost looks like a straight line.

`qqplot( V, X)`

e. Now we re-do parts 4a through 4d with a million (instead of a hundred) numbers per variable:

`U <- runif(1000000)`

`V <- -log(U)/3`

`X <- rexp(1000000, rate=3)`

`qqplot( V, X)`


Question 5.

a.  Generate 1,000,000 (continuous) uniform random numbers (each between 0 and 1) and store them in a matrix `M` with 1000 rows and 1000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 1000 numbers, each of which is equal to the sum of the 1000 uniforms.  Store the result in a vector `v`.

c.  Subtract 500 from each entry of `v` and then (afterwards) divide each number by `sqrt(1000/12)`, i.e., by `9.1287`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Solution:

a. Here is the desired matrix:

`M <- matrix( runif(1000000), nrow=1000)`

b. Here are the row sums:

`v <- apply(M, 1, sum)`

c. Here are the scaled-and-shifted values:

`w <- (v-500)/sqrt(1000/12)`

d. Here is the desired qqplot

`x <- rnorm(1000)`

`qqplot( w,x )`


Question 6.

a.  Generate 100,000,000 exponential random numbers, each with `rate = 5`, and store them in a matrix `M` with 10000 rows and 10000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 10000 numbers, each of which is equal to the sum of the 10000 exponential random numbers.  Store the result in a vector `v`.

c.  Subtract 2000 from each entry of `v` and then (afterwards) divide each number by `sqrt(10000/5^2) = 100/5 = 20`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Solution:

a. Here is the desired matrix:

`M <- matrix( rexp(100000000, rate=5), nrow=10000)`

b. Here are the row sums:

`v <- apply(M, 1, sum)`

c. Here are the scaled-and-shifted values:

`w <- (v-2000)/20`

d. Here is the desired qqplot

`x <- rnorm(10000)`

`qqplot( w,x )`


Question 7.

a.  Use the built-in `R` data set for "Pharmacokinetics of Theophylline" (stored in `Theoph`) to build a multiple linear regression model of the concentration, based on the weight, dose, and time.

b.  If a person weighed `66 kg`, and received a dose of `4 mg/kg`, and it has been 6 hours since the dose was administered, what is the predicted level of concentration?

Solution:

a. Here is a multiple linear regression model for concentration, based on weight, dose, and time, in the Theoph data set

`mylm <- lm( Theoph$conc ~ Theoph$Wt + Theoph$Dose + Theoph$Time)`

b.

`summary(mylm)`

We could estimate the concentration to be about  4.22351 mg/L if the person weighs 66kg, receives a dose of 4 mg/kg, and it has been 6 hours since the dose was administered.

`-4.45137 + 0.06832*66 + 1.16716*4 - 0.12572*4`


Question 8.

Look at the departure delays from the June 1990 flights.  If we restrict attention to departure delays of 30 minutes or more, what kind of distribution do you think the data has?  Normal?  Uniform?  Exponential?  Justify your answer with a `qqplot`.  How closely can you estimate the parameter(s) of the distribution you think that this data has?

Solution:

The vector of departure delays that are longer than 30 has length 28022:

`length(juneDF$DepDelay[juneDF$DepDelay > 30])`

but does not look Normal:

`qqplot(juneDF$DepDelay[juneDF$DepDelay > 30], rnorm(28022))`

but does not look Uniform:

`qqplot(juneDF$DepDelay[juneDF$DepDelay > 30], runif(28022))`

but it DOES look Exponential:

`qqplot(juneDF$DepDelay[juneDF$DepDelay > 30], rexp(28022))`

We can estimate the rate by first taking the mean of these delays, which turns out to be: 70.65488

`mean(juneDF$DepDelay[juneDF$DepDelay > 30], na.rm=T)`

and then we can take the rate to be the inverse of this mean:

`qqplot(juneDF$DepDelay[juneDF$DepDelay > 30], rexp(28022, rate=1/70.65488))`

Indeed, if we explore a little more, we might find that the delays are roughly  distributed like 30 + an Exponential, so we could do something more precise like this:

We see that, on average, the delay is 40.65488 minutes beyond 30:

`mean(juneDF$DepDelay[juneDF$DepDelay > 30] - 30, na.rm=T)`

So it makes more sense to compare the DepDelay - 30 to an Exponential:

`qqplot(juneDF$DepDelay[juneDF$DepDelay > 30] - 30, rexp(28022, rate=1/40.65488))`

This fit is not exact, but it is reasonable for a very rough approximation.




