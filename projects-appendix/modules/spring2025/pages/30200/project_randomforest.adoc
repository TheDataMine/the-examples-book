= Random Forest - Predicting Prescribers 

== Project Objectives

In this project, you will build a **decistion tree** and **random forest** model to predict whether prescribers will prescribe Semaglutide. 

.Learning Objectives
****
- Students will explore subsets of the data to develop a better understanding of the dataset used in this project.
- Review how to build a decision tree model, see how it performs, and make sense of the results.
- Understand the role that bagging plays in building a random forest model. 
- Develop a random forest model and see how it compares to the decision tree in terms of performance.
- Review the concept of bagging, bootstrapping and out-of-bag error and how they relate to random forest models. 
- Look at how confident the model is in its predictions by checking the predicted probabilities for a few sample cases similar to how this might work in real-world scenarios.
****

== Dataset(s)
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/X_train_rf`
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/X_val_rf`
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/X_test_rf`
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/y_train_rf`
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/y_val_rf`
- `/anvil/projects/tdm/data/CMS/RandomForest_F25/y_test_rf`


[IMPORTANT]
====
This project picks up where you left off with our recent logistic regression seminar project. You already did the hard work which was building a model, interpreting the results, and prepping the data through processing and scaling to help the model perform better. Now that the data is ready to go, we'll shift our focus to building and comparing a decision tree and a random forest model. We've provided the pre-processed datasets (X_train_rf, y_train_rf, etc.) so you can dive right into modeling without worrying about cleaning the data again.
====

**About the Data**

This dataset comes from CMS and includes prescription-level records for diabetes medications. Each row represents a **unique** prescriber writing a prescription for a specific brand-name drug. Among these drugs is Semaglutide, the drug name behind Ozempic, a medication approved for type 2 diabetes. The original 2023 data comes from the link: https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug/data?query=%7B%22filters%22%3A%7B%22rootConjunction%22%3A%7B%22label%22%3A%22And%22%2C%22value%22%3A%22AND%22%7D%2C%22list%22%3A%5B%5D%7D%2C%22keywords%22%3A%22methylphenidate%22%2C%22offset%22%3A0%2C%22limit%22%3A10%2C%22sort%22%3A%7B%22sortBy%22%3Anull%2C%22sortOrder%22%3Anull%7D%2C%22columns%22%3A%5B%5D%7D[Centers for Medicare and Medicaid Services (CMS)] dataset. 


**Variables in the Data**

Below are the columns you will be working with in this project. Rememeber, these were the selected features in the final model the last seminar project, the logistic regression project. 

[cols="1,3", options="header"]
|===
| Column Name | Description

| `Cost_per_claim` | Calculated as `Tot_Drug_Cst` divided by `Tot_Clms`; represents the average cost per claim (scaled).
| `Prscrbr_Type_Grouped_Primary Care` | Indicator variable (0 or 1) for whether the prescriber falls under the Primary Care category.
| `Prscrbr_Type_Grouped_GI/Renal/Rheum` | Indicator variable (0 or 1) for prescribers grouped into Gastroenterology, Renal, or Rheumatology specialties.
| `Tot_Day_Suply` | Total number of days of medication supplied across all claims for the prescriber-drug combination (scaled).
| `Prscrbr_Type_Grouped_Endocrinology` | Indicator variable (0 or 1) for prescribers in the Endocrinology specialty.
| `Prscrbr_Type_Grouped_Neuro/Psych` | Indicator variable (0 or 1) for prescribers grouped under Neurology or Psychiatry.
| `Prscrbr_Type_Grouped_Missing` | Indicator variable (0 or 1) for cases where the prescriber specialty was missing or ungrouped.
| `Prscrbr_Type_Grouped_Surgery` | Indicator variable (0 or 1) for prescribers whose specialty is grouped under Surgery.
| `Prscrbr_Type_Grouped_Other` | Indicator variable (0 or 1) for prescribers that don’t fall into predefined specialty groups and are labeled as “Other.”
| `Prscrbr_Type_Grouped_Women's Health` | Indicator variable (0 or 1) for prescribers in Obstetrics, Gynecology, or other Women's Health fields.
| `Prscrbr_State_Region_South` | Indicator variable (0 or 1) identifying prescribers located in the Southern U.S. region.
|===

In this project, you’ll first train a decision tree to understand the logic behind tree splits and predictions. Then, you’ll train a random forest to improve accuracy and explore how ensemble models work. We wil review decision trees and go over how random forest work under the good. Along the way, you’ll evaluate each model’s performance and think critically about model confidence and real-world use cases.

**Why Use These Models in Healthcare Data?**

In healthcare and pharmaceutical analytics, we often want to:

- Predict outcomes (e.g., Will a provider prescribe a new drug?)

- Understand feature importance (e.g., What specialties are more likely to prescribe it?)

- Balance performance with interpretability

Decision trees and random forests are good tools for this kind of work. You can get insight from a simple tree and then scale up to a more powerful random forest for better predictions.



== Questions

=== Question 1 - Understanding the Data (2 points)

In the previous logistic regression project, you learned how to build an interpretable model to identify important features associated with Semaglutide prescriptions.

[IMPORTANT]
====
Remember, in the recent logistic regression project we split the data into different subsets to help build and evaluate the model. 
====

Let review what each subset is again as we will be utilizing these data splits in this project: 

[cols="1,2,2", options="header"]
|===
| Subset | X (Predictors) | y (Target Labels)

| **Training**
| `X_train`  
Contains only features such as total patients, cost per claim, prescriber type.  
Does *not* include the Semaglutide outcome.
| `y_train`  
Binary values (0 or 1) indicating whether the prescription was for Semaglutide.

| **Validation**
| `X_val`  
Same features as training data.  
Used to evaluate the model during development.
| `y_val`  
Binary outcome labels (0 or 1) used to evaluate model performance on the validation set.

| **Test**
| `X_test`  
Held-out feature data.  
Never seen by the model during training or tuning.
| `y_test`  
Final set of target labels (0 or 1) used for unbiased model evaluation on the test set.
|===

.Deliverables
====
**1a. Read in each dataset then state how many total observations are in your training, validation, and test datasets.**

**1b. Calculate and print the proportion of records where `Semaglutide_drug = 0` and `Semaglutide_drug = 1` in each dataset (`y_train_rf`, `y_val_rf`, and `y_test_rf`).**

Hint: 
You can use `pd.Series(DF).value_counts(normalize=True))`.

**1c. Write 1-2 sentences to comment on class balance. Would you consider this dataset imbalanced in terms of proportions of 0's and 1's? Why or why not?**

====

=== Question 2 - Decision Trees (2 points) 

**Review Decision Trees**

Decision trees are a popular tool for making predictions in machine learning. They can be used for both *regression* (predicting numbers) and *classification* (predicting categories). In this project, we’re focused on classification specifically, predicting whether a prescriber will prescribe **Semaglutide** (yes or no).

**Review: What Is a Decision Tree?**

You can think of a decision tree like a flowchart: 

- At each step (called a *node*), the model asks a yes/no question about one of the features (like `Cost_per_claim` or `Total_Day_Supply`).

- Depending on the answer, the data follows one of two branches.

- This continues until it reaches a *leaf* or *terminal node*, where a prediction is made.

The goal is to keep splitting the data into smaller and smaller groups that are more *pure* which means most observations in each group belong to the same class.

**How Does the Model Make Splits?**

Decision trees are built using a process called *recursive binary splitting*:

1. The model starts with the entire dataset.

2. It looks through all possible features and cut-off values and picks the one that best separates the data into groups with different outcomes.

3. It repeats this within each subgroup, splitting again and again until it reaches a stopping point like a maximum tree depth or a minimum number of observations per group.

Each group that results from a split is called a *region*, and each split is chosen to increase *node purity*.

**What Does the Tree Represent?**

The tree divides the dataset into **boxes** (or *regions*) based on the feature values. For example:

[source]
----
If Cost_per_claim < 200:
    Predict Not Semaglutide
Else:
    If Total_Day_Supply > 180:
        Predict Semaglutide
    Else:
        Predict Not Semaglutide
----

We start with decision trees in this project because they are easy to interpret and help us understand how the model is making predictions. Once we understand and review them, we’ll move on to random forests, which build on decision trees to create more complex and accurate models.



**How to Interpret a Decision Tree**

Each path from the top (the root node) to the bottom (a leaf node) represents a sequence of decisions that leads to a final predicted class.

[Insert imafe of deciion tree here]

**Basic Structure**

- *Nodes*: Each box in the tree is called a node. It includes:
  * A splitting rule (e.g., `feature <= threshold`)
  * The *gini* impurity, which measures how mixed the classes are at that node. A gini of 0 means the node is pure (only one class), while a gini close to 0.5 means it's a 50/50 mix.
  * The number of *samples* that reached this node.
  * The *value*, which shows how many records belong to each class at that node.
  * The predicted *class*, which is the majority class at that node.

- *Branches*: These are the lines that connect the nodes. Each branch represents the outcome of a condition—if the condition is true, the model moves to the left; if false, it moves to the right.

- *Leaf Nodes*: These are the final nodes at the bottom of the tree. They represent the model’s predicted class for any data point that ends up there.

**How to Read It**

To interpret a decision tree:

- Start at the top and follow the branches based on the conditions.
- At each node, decide whether the condition is true or false.
- Continue until you reach a leaf node, which gives the predicted class.

**What to Pay Attention To**

- *Which features are used for splitting?* Features near the top of the tree are usually more influential.
- *What do the gini values tell you?* The lower the gini, the more confident the model is in that node’s classification.
- *What are the class distributions?* Look at the `value` field to see how many examples from each class ended up in the node. This helps you judge the strength of the prediction.

**Why This Matters**

Interpreting a decision tree helps you:

- Understand how the model is making decisions.
- Identify important patterns in the data.
- Gain insight into which features are most useful for predicting the outcome.


.Deliverables
====
**2a. Use the code below to fit a DecisionTreeClassifier on the training data using class_weight='balanced', max_depth = 2, and random_state=42. Then, on your own, print the number of leaves along with the tree depth and explain what the depth and leaves represent in the context of a decision tree in 1–2 sentences.**

_Note:_ Use the code below to answer 2a. Make sure to fill in the blank spaces in dt_model, print the number of leaves along with the tree depth, and then explain depth and leaves in the context of decision trees in 1-2 sentences. 


[source,python]
----
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

dt_model = DecisionTreeClassifier(class_weight='....', max_depth=...., random_state=....) # For YOU to fill in

dt_model.fit(X_train_rf, y_train_rf) 
----

**2b. Using the code below, visualize the decision tree using plot_tree() from sklearn.tree on the training data.**

_Note:_ The code structure below is mostly complete. Your task is to fill in the missing parts: specify the feature names using `X_train_rf`, provide the correct class labels and give a title to the plot. To determine the correct labels, refer back to the order of your results from Question 1b, where you explored the distribution of Semaglutide prescriptions.

[source,python]
----
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 8))
plot_tree(dt_model,
          feature_names=....., # For YOU to fill in 
          class_names=["....", "....."], # For YOU to fill in 
          filled=True,
          rounded=True)
plt.title(".......") # For YOU to fill in 
plt.show()
----

**2c. Run the code below to evaluate the model on the validation set using a confusion matrix and the classification report. Then in 1-2 sentences write about the model's performance.**

_Note:_ Your task is to run the code below and then interpret the results in 1-2 sentences. 

[source,python]
----
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

y_test_pred_dt = dt_model.predict(X_test_rf)
y_test_proba_dt = dt_model.predict_proba(X_test_rf)[:, 1]
auc_dt_test = roc_auc_score(y_test_rf, y_test_proba_dt)

print("Decision Tree Performance on Test Set:")
print("Confusion Matrix:")
print(confusion_matrix(y_test_rf, y_test_pred_dt))
print("\nClassification Report:")
print(classification_report(y_test_rf, y_test_pred_dt))
print(f"AUC: {round(auc_dt_test, 4)}")
----


**2d. Look at the first split in your decision tree and write 1–2 sentences explaining what feature is used, what the condition is and what this tells you about how the model begins making predictions for Semaglutide.**


====

=== Question 3 - Build and Evaluate a Random Forest Model (2 points)

**Why Random Forests?**

"Decision trees have one aspect that prevents them from being the ideal tool for predictive learning — namely *inaccuracy*." — Elements of Statistical Learning

Decision trees work well on training data, but they're not known for generalizing well to new data.

Random forests improve this by combining 
**bootstrapped datasets** and  **random predictor selection**
→ leading to diverse trees and better performance. 


**How it Works**

**Step 1: Create a Bootstrapped Dataset**

We start with a dataset like this:

[cols="1,1,1,1,1", options="header"]
|===
| Tot_Day_Suply | Cost_per_claim | Prscrbr_Type_PrimaryCare | Region_South | Semaglutide
| 20457         | 0.857          | 1                         | 1            | 0
| 23059         | 0.831          | 1                         | 0            | 0
| 11235         | 1.132          | 1                         | 1            | 0
| 2220          | -0.435         | 0                         | 0            | 1
| 20332         | 0.381          | 1                         | 0            | 0
|===

We create a **bootstrapped dataset** by sampling rows *with replacement*:

.Sample Bootstrapped Data:
[cols="1,1,1,1,1", options="header"]
|===
| Tot_Day_Suply | Cost_per_claim | Prscrbr_Type_PrimaryCare | Region_South | Semaglutide
| 11235         | 1.132          | 1                         | 1            | 0
| 2220          | -0.435         | 0                         | 0            | 1
| 23059         | 0.831          | 1                         | 0            | 0
| 11235         | 1.132          | 1                         | 1            | 0
|===

**Step 2: Build a Tree Using Random Predictors**

When splitting the data:

- The tree selects a **random subset of predictors** (e.g. 2 or 3 out of 11)
- Suppose we randomly pick: `Cost_per_claim`, and `Prscrbr_Type_PrimaryCare`
- The tree uses the best of those to split.

Each node repeats this process with a new random subset.

**Step 3: Repeat to Build a Forest**

Repeat:

. Create new bootstrapped data  
. Build new tree using random predictors  
. Repeat **hundreds of times**

The result: a **forest** of trees, each slightly different! This diversity helps reduce **variance** and avoid overfitting.

After running the data down all thre tees in the random forest, we see which option recieved more votes. In this case "1" recieved more votes, so we will conclude the prescriber did prescribe semaglutide. 


**Step 4: Estimate Accuracy Using Out-of-Bag (OOB) Error**

How do we know if the random forest is any good? Some rows are not included in each bootstrapped sample, these are called **Out-of-Bag samples**.

We use them like test data:

. For each row, run it through all trees that didn’t train on it.
. Each tree votes: `Semaglutide = 0` or `1`
. Take the majority vote and compare to the true label

.Example:
Row = `2220`
True label = `1`  

OOB predictions: 1, 1, 0, 1 

→ majority vote = `1`  

→ Correct!

Repeat for every row. 

Then we run the **out-of-bag** sample through all the other trees that were built without it. Since the label with the most votes wins, it is the label that we assign the out of bag sample. We then to the same thing for all the other out of bag samples for all the other trees. 

Ultimately, we can measure how accurate our random forest model is by the proportion of out of bag samples that were correcrly classifified by the random forest. The proportion of the Out-of-bag samples that were incorrectly classified is the **"Out-of-Bag"** error. 

**Why This Works**

By making each tree slightly different through both bootstrapping and random predictor selection, random forests produce more reliable predictions.

**How Predictions Are Made**

- Classification: Each tree votes for a class label. The final prediction is the majority vote.

- Regression: Each tree gives a numeric prediction. The final prediction is the average.


**Important Terminology**

[cols="1,2", options="header"]
|===
| Term | Definition

| **Random Forest** | An ensemble method that builds many decision trees on random subsets of the data and predictors, then combines them by averaging (regression) or voting (classification).
| **Decision Tree** | A model that makes decisions by splitting data into branches based on conditions on predictor variables.
| **Bootstrapping** | Sampling from the original dataset *with replacement* to create a new dataset the same size.
| **Bagging** | Short for *bootstrap aggregating*: training multiple models on bootstrapped data and averaging the results.
| **Out-of-Bag (OOB) Sample** | Data points that were *not* selected in a given bootstrap sample, used like a built-in validation set.
| **Out of Bag (OOB) Error** | An estimate of the model’s prediction error, calculated using only the OOB predictions for each observation.
| **m** | Number of predictor variables randomly selected at each split in a tree.
| **Ensemble** | A group of models combined to produce a stronger overall prediction.
| **Majority Vote** | In classification, the final predicted class is the one that most trees predict.
|===


**Summary**

We built a tree ..... 

1. Using a bootstrapped dataset. 
2. Only considered a random subset of variables at eash step. 
3. Keep repeating! Make a new bootstrapped dataset and build a tree considering a subset of variables at each step. 
4. Ideally, you do this hundreds of times
5. Use a bootstrapped sample and consider only a subset of variables at each step results in a wide variety of trees. 
6. This variety is what makes random forest more effective than individual decision trees!



.Deliverables
====
**3a. Use the code below to fit a RandomForestClassifier on the training data using class_weight='balanced', random_state=42, and set max_depth=5 to limit how deep each tree can grow. After fitting the model, write 1–2 sentences explaining how Random Forest models work and how they differ from a single decision tree.**

_Note:_ Most of the code has been provided for you below. Make sure to fill in the missing pieces. 

[source,python]
----
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

rf_model = RandomForestClassifier(class_weight='.....', random_state=....) # For YOU to fill in

rf_model.fit(X_train_rf, y_train_rf)
----

**3b. Print the training AUC on X_train_rf, X_val_rf, and X_test_rf by running the code below and then write 1-2 sentences evaluating the performance.**

_Note:) The code to print the AUC and confusion matrix has been provided. We ask that you try to understand and interpet the results in 1-2 sentences. 

[source,python]
----
y_test_pred_rf = rf_model.predict(X_test_rf)
y_test_proba_rf = rf_model.predict_proba(X_test_rf)[:, 1]
auc_rf_test = roc_auc_score(y_test_rf, y_test_proba_rf)

print("\nRandom Forest Performance on Test Set:")
print("Confusion Matrix:")
print(confusion_matrix(y_test_rf, y_test_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test_rf, y_test_pred_rf))
print(f"AUC: {round(auc_rf_test, 4)}")
----

**3c. Compare the Random Forest to your Decision Tree. Write 1–2 sentences describing how the performance metrics (e.g., AUC, precision) changed and why that might be expected.**


**3d. Write 2-3 sentences, in your own words, on the role bagging and boostrapping plays in building a random forest model.**

====

=== Question 4 - Feature Importance (2 points)

**Interpreting the Model**

Building a model that makes good predictions is only part of the story we also want to **understand how the model makes decisions**.

In real-world applications like healthcare, finance, and criminal justice, being able to **explain your model** is critical. Doctors, patients, regulators, and other stakeholders need to know:

- What factors most influence the prediction?
- Are those factors reasonable and ethical?
- Can we justify the model’s recommendations?

Random Forests are more complex than individual decision trees, but we can still interpret them using tools like:

- **Feature Importance**: Which variables had the biggest impact on the model’s predictions?
- **Class Proportions for Key Features**: What patterns do we see between important features and the target outcome?

These tools allow us to move from "black box" to "glass box" where we not only trust the model, but also understand *why* it performs well.

In the questions that follow, you’ll practice:

- Visualizing which features were most important to your model
- Exploring how one of those features differs across your target classes
- Reflecting on whether those patterns make sense in the context of your data and objective.

.Deliverables
====
**4a. Identify and visualize the top 10 most important predictors from `X_train_rf` in your random forest model using the code below.**

_Note:_ Make sure to fill in the missing parts to:

- Extract top 10 featurefeatures
- Title and label the plot

[source,python]
----
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Extract feature importances from the model
feature_importances = pd.Series(rf_model.feature_importances_, index=______)  #  For YOU to fill in

# Step 2: Get the top 10 most important features
top10_features = feature_importances.sort_values(ascending=False).head(10)  

# Step 3: Plot the top 10 features
plt.figure(figsize=(10, 6))
top10_features.plot(kind='barh')
plt.gca().invert_yaxis()  
plt.xlabel("....") # For YOU to fill in
plt.title("....") # For YOU to fill in
plt.tight_layout()
plt.show()
----

**4b. Calculate the proportion of primary care prescribers within each class (`Semaglutide = 0` and `Semaglutide = 1`), and then write 1-2 sentences on your interpreation of the results.**

_Note:_ Complete the missing parts of the code below (marked with `# For YOU to fill in`):

[source,python]
----
# Step 1: Choose the binary feature to examine
binary_feature = "Prscrbr_Type_Grouped_Primary Care"  

# Step 2: Combine the training features and the target
df = X_train_rf.copy()
df["_____"] = _____  # For YOU to fill in: assign the target column name and variable

# Step 3: Group by class and calculate the proportion of prescribers with value 1
proportions = df.groupby("target")[binary_feature].mean()

print(proportions)
----

**4c. Write 1-2 sentences to explain what the top 3 most important features are capturing in terms of prescribers of Semaglutide and whether the findings seem expected or unexpected.**


====

=== Question 5 - Confidence In The Model (2 points)

Machine learning models like random forests don’t just give you a predicted class they also estimate how confident they are by outputting a predicted probability. By default, we often classify cases as 1 (e.g., "will prescribe Semaglutide") if the probability is greater than or equal to 0.5. But in practice, you might want to adjust this threshold depending on the business or healthcare context.

For example:

A higher threshold (e.g., 0.9) may reduce false positives but miss potential opportunities.
A lower threshold (e.g., 0.3) may catch more positives, but with less certainty.

This question helps you explore how confident your model is and why thresholds matter.

.Deliverables
====
**5a. Use the predict_proba() method to check how confident the model is in its predictions. Randomly select five prescribers from the test set. For each one, print the predicted probability that they will prescribe Semaglutide, the actual class label (0 or 1), and the predicted class based on a 0.5 threshold.**


_Note:_ Use the code provided code below and make sure to fill in the test data frame in which you will be selecting the random 5 samples from. 


[source,python]
----
import numpy as np

# Pick 5 random test examples
random_indices = np.random.choice(len(________), 5, replace=False) # For YOU to fill in

for i in random_indices:
    prob = rf_model.predict_proba(X_test_rf.iloc[[i]])[:, 1][0]
    pred_class = int(prob >= 0.5)
    actual_class = y_test_rf[i]
    
    print(f"Prescriber {i}:")
    print(f"  Predicted probability of Semaglutide: {round(prob, 3)}")
    print(f"  Final prediction: {pred_class}")
    print(f"  Actual label: {actual_class}\n")
----

**5b. Write 1–2 sentences describing how knowing the probability (not just the predicted label) of a prescriber prescribing semaglutide might be useful in a real-world healthcare or pharmaceutical setting.**

**5c. Use the code below to find out how many prescribers in the test set were predicted with high confidence (model gave them a probability greater than 0.9 of prescribing Semaglutide).** 


_Note:_ Use the code provided below. Make sure to fill in the line that identifies the threshold for high-confidence predictions.

[source,python]
----
# Get predicted probabilities for class 1 (Semaglutide)
probs = rf_model.predict_proba(X_test_rf)[:, 1]

preds = (probs >= ____).astype(int) # For YOU to fill in 
high_confidence = probs >= _____  # For YOU to fill in 

# Check how many were correct
correct = (preds == y_test_rf) & high_confidence
incorrect = (preds != y_test_rf) & high_confidence

print(f"Total high-confidence predictions: {high_confidence.sum()}")
print(f"Correct high-confidence predictions: {correct.sum()}")
print(f"Incorrect high-confidence predictions: {incorrect.sum()}")
----


**5d. Write 1–2 sentences interpreting what this tells you about your model’s confidence and how that could be useful in a real-world healthcare or business setting.**


====


=== Question 6 - Threshold and Visuals (2 points)
**Understanding Classification Thresholds**

In binary classification, most models like random forests output a predicted probability for the positive class (in our case, whether a prescriber will prescribe Semaglutide).

By default, we use a threshold of 0.5, meaning:

- If the predicted probability ≥ 0.5 → classify as Semaglutide prescriber (class 1)

- If the predicted probability < 0.5 → classify as Not Semaglutide prescriber (class 0)

However, in real-world applications, we often adjust the threshold to better match the business or clinical context.


For example:

- In healthcare, you may only want to flag prescribers when the model is very confident (e.g., probability > 0.9) to reduce false positives.

- In marketing or outreach, you might lower the threshold to capture more potential targets, even if that means some will be wrong.

Changing the threshold affects:

- Precision 
- Number of correct predictions


.Deliverables
====
**6a. In real-world settings, you may want to adjust the threshold for classifying a prescriber as someone likely to prescribe Semaglutide (e.g., not just 0.5 default). Use the code below to plot the distribution of predicted probabilities for the test set, broken down by the actual class (y_test_rf).**

_Note:_ Your task is to label the plot appropriately in the missing components and run the code below. 

[source,python]
----
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Get predicted probabilities
probs = rf_model.predict_proba(X_test_rf)[:, 1]

# Create dataframe for plotting
df_probs = pd.DataFrame({
    'Predicted_Probability': probs,
    'Actual_Class': y_test_rf})

# Plot probability distributions by actual class
plt.figure(figsize=(10, 6))
sns.histplot(data=df_probs, x='Predicted_Probability', hue='Actual_Class', bins=30, kde=True, stat="density", common_norm=False)
plt.title('______') # For YOU to fill in 
plt.xlabel('_________')  # For YOU to fill in 
plt.ylabel('Density')
plt.legend(title="Actual Class", labels=["Not Semaglutide", "Semaglutide"])
plt.tight_layout()
plt.show()
----

**6b. Let's choose the new threshold to be 0.9 for flagging high-likelihood prescribers. Using this new threshold, calculate new precision of the model.**

_Note:_  Your task is to fill in the appropriate threshhold and run the code below.

[source,python]
----
from sklearn.metrics import precision_score, recall_score

# Choose new threshold
new_thresh = ____ #For YOU to fill in

# Apply threshold
probs = rf_model.predict_proba(X_test_rf)[:, 1]
new_preds = (probs >= new_thresh).astype(int)

# Calculate stats
predicted_positive = (new_preds == 1)
true_positive = (new_preds == 1) & (y_test_rf == 1)

precision = precision_score(y_test_rf, new_preds)
recall = recall_score(y_test_rf, new_preds)

print(f"Number predicted as Semaglutide: {predicted_positive.sum()}")
print(f"Correct predictions: {true_positive.sum()}")
print(f"Precision: {round(precision, 3)}")

----

**6c. Write 1–2 sentences on whether this new threshold is more practical for targeted outreach or risk flagging in a healthcare or pharma setting (compare to your results from 3b).**



====


== References

Some explanations, examples, and terminology presented in this section were adapted from the following sources for educational purposes:

* James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). _An Introduction to Statistical Learning: with Applications in Python_. Springer Texts in Statistics. Springer.

* Starmer, J. (2020). _Random Forests Part 1 – Building, Using and Evaluating_. StatQuest with Josh Starmer. https://www.youtube.com/watch?v=J4Wdy0Wc_xQ


== Submitting your Work

Once you have completed the questions, save your Jupyter notebook. You can then download the notebook and submit it to Gradescope.

.Items to submit
====
- firstname_lastname_project1.ipynb
====

[WARNING]
====
You _must_ double check your `.ipynb` after submitting it in gradescope. A _very_ common mistake is to assume that your `.ipynb` file has been rendered properly and contains your code, markdown, and code output even though it may not. **Please** take the time to double check your work. See https://the-examples-book.com/projects/submissions[here] for instructions on how to double check this.

You **will not** receive full credit if your `.ipynb` file does not contain all of the information you expect it to, or if it does not render properly in Gradescope. Please ask a TA if you need help with this.
====