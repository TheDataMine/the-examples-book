= STAT-LLC Fall 2014 STAT 29000 Projects

== Project 1

Question 1.

During which pair of years did the level of Lake Huron rise the most?
The data to use is from the built-in `LakeHuron` data set.
(E.g., during 1875 to 1876, Lake Huron rose 1.48 feet.)  It might help to use the `diff` command.

Solution:

`diff(LakeHuron)` gives the year-to-year rises and falls of the data

`diff(LakeHuron) == max(diff(LakeHuron))` gives a vector of `FALSE` values and one `TRUE` showing which of the positions are the max

`which(diff(LakeHuron) == max(diff(LakeHuron)))`  shows that the maximum occurs in the 85th year of the data set.

So the biggest rise of the LakeHuron data set is between the year 1959:

`1874 + which(diff(LakeHuron) == max(diff(LakeHuron)))`

and the year 1960:

`1874 + 1 + which(diff(LakeHuron) == max(diff(LakeHuron)))`

Question 2.

a. What is the average duration of an eruption in the `geyser` dataset in the `MASS` library?

b. What were the 10 longest durations?

c. How many durations were 3 minutes or longer?
(You do not need to install the `MASS` library; it is installed already.  You do, however, need to load the `MASS` library.)

Solution:

We first load the `MASS` library

`library(MASS)`

a. The average duration is 3.460814 minutes:

`mean(geyser$duration)`

b. We can sort the geyser durations, from largest to smallest, as follows:

`sort(geyser$duration, decreasing=TRUE)`

and then we can just extract the largest 10 entries of this vector:

`sort(geyser$duration, decreasing=TRUE)[1:10]`

c. There are 194 durations that last 3 minutes or longer:

`length(geyser$duration[geyser$duration >= 3])`


Question 3.

a.  Which car(s) in the `mtcars` data set had the highest gas mileage?

b.  Which car(s) had the highest horsepower?

c.  Which car(s) had the shortest (i.e., fastest) 1/4 mile time?

d.  How many cars had manual transmission?

e.  How many cars had manual transmission and also six cylinders?

Solution:

a. We see that the 20th car has the highest gas mileage

`which(mtcars$mpg == max(mtcars$mpg))`

so we can get the name of this car this way:

`row.names(mtcars[20,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$mpg == max(mtcars$mpg)),])`

b. We see that the 31st car has the highest horsepower

`which(mtcars$hp == max(mtcars$hp))`

so we can get the name of this car this way:

`row.names(mtcars[31,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$hp == max(mtcars$hp)),])`

c. We see that the 29th car has the highest horsepower

`which(mtcars$qsec == min(mtcars$qsec))`

so we can get the name of this car this way:

`row.names(mtcars[29,])`

or rolling everything into one line of code:

`row.names(mtcars[which(mtcars$qsec == min(mtcars$qsec)),])`

d.

`mtcars$am == 1`

gives a vector of `TRUE` and `FALSE` values, for whether the cars do or do not have manual transmission

then we can sum to get the total number of `TRUE` values, because when we sum, each `TRUE` becomes 1 and each `FALSE` becomes 0.

`sum(mtcars$am == 1)`

e. Similar to 3d, but we add the condition that there are 6 cylinders:

`sum(mtcars$am == 1 & mtcars$cyl == 6)`


Question 4.

a.   Which states are (strictly) larger in population than Indiana but (strictly) smaller in population than Pennsylvania, according to the data in the `state` data set?
Hint: You can get the state populations using `state.x77[,"Population"]`.

b.   Which states are (strictly) larger in land area than Indiana but (strictly) smaller in land area than Pennsylvania, according to the data in the `state` data set, as listed in `state.x77[,"Area"]`?

Solution:

a. The populations of Indiana and Pennsylvania are 5313 and 11860, respectively:

`state.x77[,"Population"]["Indiana"]`

`state.x77[,"Population"]["Pennsylvania"]`

now we use these conditions to index the vector state.x77[,"Population"] as follows:

`state.x77[,"Population"][state.x77[,"Population"] > 5313 & state.x77[,"Population"] < 11860]`

or we can roll this into one line as follows:

`state.x77[,"Population"][  state.x77[,"Population"] > state.x77[,"Population"]["Indiana"] & state.x77[,"Population"] < state.x77[,"Population"]["Pennsylvania"]  ]`

b. the populations of Indiana and Pennsylvania are 5313 and 11860, respectively:

`state.x77[,"Area"]["Indiana"]`

`state.x77[,"Area"]["Pennsylvania"]`

now we use these conditions to index the vector state.x77[,"Area"] as follows:

`state.x77[,"Area"][state.x77[,"Area"] > 36097 & state.x77[,"Area"] < 44966]`

or we can roll this into one line as follows:

`state.x77[,"Area"][state.x77[,"Area"] > state.x77[,"Area"]["Indiana"] & state.x77[,"Area"] < state.x77[,"Area"]["Pennsylvania"]]`


Question 5.

If `Z` is a standard normal random variable, we know that `Z` has average 0 and variance 1.  Use `R` to simulate:

a. the value of the average of `|Z|`, and

b. the value of the variance of `|Z|`.

Here, `|Z|` is just the absolute value of `Z`.

Solution:

a. The value of the mean of |Z| is approximately

`mean(abs(rnorm(1000000)))`

b. The value of the var of |Z| is approximately

`var(abs(rnorm(1000000)))`


Question 6.

Write a function called `countas` that takes a sequence of words and returns the number of words that have 1 or more `a`s. For instance, `countas(  c("ate", "hello", "duolingo", "pat", "aa")  )` should return the value 3.  Hint:  It might help to use the `grep` function.

Solution:

We define the countas function to be:

[source,r]
----
countas <- function(v) {
	length(grep("a", v))
}
----

Question 7.

a.  Write a function called:  `firstthree` that returns the location of the first occurrence of 3 in a vector.  For instance, `firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 2.

b.  Write a function called:  `thirdthree` that returns the location of the third occurrence of 3 in a vector.  Ffor instance, `thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 9.

Solution:

a. We find which elements equal 3, and then take the first such element, so we define the firstthree function to be:

[source,r]
----
firstthree <- function(v) {
	which(v==3)[1]
}
----

and it works as required:

`firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )`

b. We find which elements equal 3, and then take the third such element, so we define the thirdthree function to be:

[source,r]
----
thirdthree <- function(v) {
	which(v==3)[3]
}
----

and it works as required:

`thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )`


Question 8.

Write a function called:  `topfive` that returns the most common five values in a vector, along with the counts for each of the 5 values.

Solution:

We make a table, sort it into decreasing order (i.e., biggest elements first) and then we take the biggest five elements, so we define the topfive function to be:

[source,r]
----
topfive <- function(v) {
	sort(table(v), decreasing=TRUE)[1:5]
}
----


Question 9.

a. Euler's number is 2.718281828459...  Euler's number is defined as `1 + 1/1 + 1/(1*2) + 1/(1*2*3) + 1/(1*2*3*4) + 1/(1*2*3*4*5) + ...` Find a good way to calculate this in `R`, with few keystrokes. If you subtract 2.718281828459 from your estimate, you should get something very small, e.g., roughly `4.5 * 10^{-14}.`

b.  Find a good way to approximate the value of Pi, using only the fact that `Pi^2 / 6 = 1/1^2 + 1/2^2 + 1/3^2 + 1/4^2 + 1/5^2 + 1/6^2 + 1/7^2 + ....`

Solution:

a. Euler's number is approximately

`sum(1/factorial(0:100))`

and indeed, if we subtract 2.718281828459, we get a very small number:

`sum(1/factorial(0:100)) - 2.718281828459`

b. The number Pi^2 / 6 is approximately equal to   sum(1/((1:100000000)^2))

so this means that Pi is approximately:

`sqrt(6*sum(1/((1:10000000)^2)))`


Question 10.

a. The triangular numbers are: `1, 3, 6, 10, 15, 21, 28, 36, 45, 55, ...` See http://oeis.org/A000217 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

b. The tetrahedral numbers are: `1, 4, 10, 20, 35, 56, 84, 120, 165, 220, ...` See: http://oeis.org/A000292 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

Solution:

a. The first 1000 triangular numbers are:

`n <- 1000`

`(1:n)*(2:(n+1))/2`

b. The first 1000 tetrahedral numbers are:

`n <- 1000`

`(1:n)*(2:(n+1))*(3:(n+2))/6`


== Project 2

Question 1.

Consider the Columbia River Estuary dataset discussed in the `week 2 notes`

a.  Download the data set (we no longer need to do this).

b.  Import the `saturn03.240.A.CT_2012_06_PD0.csv` data set into `R`, using the `read.csv` function.

c.  Use the `strptime` function to convert the first column of the data into numerical times that `R` can easily handle.

Solution:

ab. Download the data set and import it to R:
`DF <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.240.A.CT_2012_06_PD0.csv")`

c. Use strptime to convert the times:
`timevec <- strptime(DF[ ,1],  "%Y/%m/%d %H:%M:%S")`

Question 2.

a.  What is the most common time (in seconds) between consecutive measurements, in the data set?  How often is the data sampled with this exact difference in time, between consecutive measurements?

b.  What is the mean time between consecutive measurements?  Why is this significantly different from the most common time, found in part `2a` above?

Solution:

a. The most common time (in seconds) between measurements is 3 seconds.  A 3 second difference occurs 188681 times.

`sort(table(diff(timevec)),decreasing=TRUE)[1]`

b. The mean time between consecutive measurements is 12.7873 seconds.  This is so skewed (to be large, in particular, much larger than 3 seconds!) because there are several large differences in times, e.g., when the machine collecting the data is broken for long periods.

`mean(diff(timevec))`


Question 3.

a.  Suppose that we treat "15 seconds" as a threshold in consecutive time measurements, i.e., if the machine goes more than 15 seconds without taking a measurement, we consider that the machine is temporarily broken/clogged/stuck/etc.  With this level of threshold, how many times did this particular machine (at this particular location) get stuck during June 2012?

b.  How long is longest duration when the machine was broken?  When did this occur? Specifically: when did it break, and when did it start working properly again?

c.  Find the ten longest durations for when the machine was broken; just give each such measurement in seconds.

Solution:

a. The machine gets stuck a total of 10688 times.

`sum(diff(timevec) > 15)`

b. The longest duration when the machine is broken can be achieved in either of these two equivalent ways:

`t <- which(diff(timevec) == max(diff(timevec)))`

`t <- which.max(diff(timevec))`

The longest duration when the machine was broken was 128661 seconds:

`diff(timevec)[t]`

or (equivalently) 1.489132 days:

`timevec[t+1] - timevec[t]`

The longest duration when the machine is broken is from June 17, 2012, 5:24:00 PM, to June 19, 2012, 5:08:21 AM.

`timevec[t]`

`timevec[t+1]`

c. The ten longest durations when the machine was broken (in seconds) are:

`128661 106344  62985  42282  33441  30930  26958  19416  12942   8613`

`sort(diff(timevec),decreasing=TRUE)[1:10]`


Question 4.

a. Does the device which measures the electrical conductivity ever give a false reading?  If so, when?  Give the specific times (e.g., the day(s), hours, minutes, seconds), when this occurs in June, for each such occurrence.

b. Are any of these times in `4a` the same as the one (unique) time when the temperature device gave a false reading?  (We saw, in the notes, that the temperature device had one false reading.)

c.  Does the device which measures the salinity ever give a false reading?  What evidence to you have to support this claim?

Solution:

a. We can see that there are one (or more) outliers, with **electrical conductivity** falsely reported to be about 25 or so

`plot(DF$water_electrical_conductivity)`

but the rest of the points are between 11.051 and 17.845

`range(DF$water_electrical_conductivity[DF$water_electrical_conductivity < 25])`

and there do not appear to be any outliers on the lower side:

`plot(DF$water_electrical_conductivity[DF$water_electrical_conductivity < 25])`

There are actually TWO outliers:

`which(DF$water_electrical_conductivity > 25)`

These two times both occur on June 26, 2012, at 2:36 PM and 3:33 PM:

`timevec[168443]`

`timevec[168742]`

b. The first outlier occurs at the same time as the outlier for the temperature data, i.e., at time index 168443.

c. We can see (visually) that there do not appear to be any outliers for the salinity data:

`plot(DF$water_salinity)`


Question 5.

a.  Repeat the questions from `2a`/`2b`/`3a`/`3b`/`3c`, but now use the data set from the same point on the Columbia River Estuary but at the depth of `8.2m` (the data from the questions above was measured at `2.4m` below the surface).  The data set from `8.2m` below the surface is available at `saturn03.820.A.CT_2012_06_PD0.csv`.

b.  Does the longest time in which the machine was broken in `3b` (at depth `2.4m`) correspond roughly to the same longest time in which the machine was broken in this current data set, at depth `8.2m`?  For this longest time interval, what are the times (at depth `8.2m`), when the machine did break, and when did it start working properly again?

c.  Make a plot of the temperature data at depth `8.2m`.  There is exactly one false reading in which the temperature is too high, and exactly one false reading in which the temperature is too low.  Be sure to remove these points before plotting.

Solution:

a. Download the data set from 8.2m and import it to R:

`DF820 <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.820.A.CT_2012_06_PD0.csv")`

Use strptime to convert the times:

`timevec820 <- strptime(DF820[ ,1],  "%Y/%m/%d %H:%M:%S")`

The most common time (in seconds) between measurements is 3 seconds.  A 3 second difference occurs 119805 times at depth 8.2m.

`sort(table(diff(timevec820)),decreasing=TRUE)[1]`

The mean time between consecutive measurements is 19.5147 seconds at depth 8.2m.

`mean(diff(timevec820))`

The machine gets stuck a total of 10447 times at depth 8.2m.

`sum(diff(timevec820) > 15)`

The longest duration when the machine is broken (at depth 8.2m) can be achieved in either of these two equivalent ways:

`t820 <- which(diff(timevec820) == max(diff(timevec820)))`

`t820 <- which.max(diff(timevec820))`

The longest duration when the machine was broken (at depth 8.2m) was 128853 seconds:

`diff(timevec820)[t820]`

or (equivalently) 1.491354 days:

`timevec820[t820+1] - timevec820[t820]`

The longest duration when the machine is broken (when working at depth 8.2m) is from June 17, 2012, 5:23:00 PM, to June 19, 2012, 5:10:33 AM.

`timevec820[t820]`

`timevec820[t820+1]`

The ten longest durations when the machine was broken (in seconds, at depth 8.2m) are:

`128853 100416  84018  65861  34356  26799  26070  24663  14208   8262`

`sort(diff(timevec820),decreasing=TRUE)[1:10]`

b. Yes, these are roughly the same times; again, we point out:

The longest duration when the machine is broken is from June 17, 2012, 5:24:00 PM, to June 19, 2012, 5:08:21 AM.

`timevec[t]`

`timevec[t+1]`

The longest duration when the machine is broken (when working at depth 8.2m) is from June 17, 2012, 5:23:00 PM, to June 19, 2012, 5:10:33 AM.

`timevec820[t820]`

`timevec820[t820+1]`

c. The temperature data at depth 8.2m looks like:

`plot(DF820$water_temperature)`

We remove the outlier that has temperature above 3000:

`plot(DF820$water_temperature[DF820$water_temperature < 3000])`

and then we also remove the outlier that has temperature below 7:

`plot(DF820$water_temperature[DF820$water_temperature < 3000 & DF820$water_temperature > 7])`


Question 6.

a.  We also have data from depth `13m` below the surface in the file `saturn03.1300.R.CT_2012_06_PD0.csv`.  Import this data into `R`.

b.  Is the water temperature generally highest, on average, at depth `2.4m`, `8.2m`, or `13m` below the surface?  Does your answer make intuitive sense?

Solution:

a. Download the data set from 13m and import it to R:

`DF1300 <- read.csv("http://llc.stat.purdue.edu/2014/29000/projects/saturn03.1300.R.CT_2012_06_PD0.csv")`

b. The average water temperatures at depths 2.4m, 8.2m, and 13m are (respectively):

`mean(DF$water_temperature[DF$water_temperature < 500])`

`mean(DF820$water_temperature[DF820$water_temperature<3000])`

`mean(DF1300$water_temperature)`

So the average temperature is highest at depth 2.4m. This makes intuitive sense, since the water is warmer at depths that are more shallow (and gets colder and deeper depths)


Question 7.

a.  What is the average salinity of the water at depth `2.4m`?  At depth `8.2m`?  At depth `13m`?  What about the variance of the salinity at all 3 depths?  Be sure to remove any outliers, when appropriate.

b.  At depth `13m`, make a plot of time versus salinity.

c.  As we saw in `7b`, much more data is available during the first two weeks of June, as opposed to the second two weeks of June.  Make a revised plot, showing only the time versus salinity from the start of the day on June 6, through the end of the day on June 12 (i.e., for a full 7-day period).  How many cycles of the salinity do you think you see on this plot?  Is there a natural reason for this number of cycles?

Solution:

a. The average salinity at the 3 depths 2.4m, 8.2m, 13m are (respectively) 2.894666, 6.327593, 12.18957

`mean(DF$water_salinity)`

`mean(DF820$water_salinity[DF820$water_salinity<1000])`

`mean(DF1300$water_salinity)`

The variance of the salinity at the 3 depths 2.4m, 8.2m, 13m are (respectively) 10.58944, 44.93308, 93.84552

`var(DF$water_salinity)`

`var(DF820$water_salinity[DF820$water_salinity<1000])`

`var(DF1300$water_salinity)`

b. We first extract the times at depth 13m:

`timevec1300 <- strptime(DF1300[ ,1],  "%Y/%m/%d %H:%M:%S")`

and now we plot the salinity at depth 13m:

`plot(timevec1300, DF1300$water_salinity)`

c. We first convert the starttime and stoptime to seconds as needed by R (i.e., to seconds after Jan 1, 1970):

`starttime <- strptime("2012/06/06 00:00:00",  "%Y/%m/%d %H:%M:%S")`

`stoptime <- strptime("2012/06/12 23:59:59",  "%Y/%m/%d %H:%M:%S")`

Now we plot the salinity data:

`plot( timevec1300[timevec1300 >= starttime & timevec1300 <= stoptime], DF1300$water_salinity[timevec1300 >= starttime & timevec1300 <= stoptime])`

It appears that there are roughly 14 cycles of the salinity data, i.e., roughly two per day.  This makes sense, since it seems to fit with the fact that the tide comes twice per day, which has a significant effect on the salinity.


Question 8.

At depth `2.4m`, what fraction of the temperature data points are between 10 and 12?  Between 12 and 14?  Between 14 and 16?  Between 16 and 18?  Use the `tapply` function to answer all four of these questions with one line of code.

Solution:

The counts of the temperature data are as follows:

`tapply( DF$water_temperature[DF$water_temperature<500], cut(DF$water_temperature[DF$water_temperature<500], breaks=c(10,12,14,16,18)), length)`

and the percentages of the temperature in each category are:

`tapply( DF$water_temperature[DF$water_temperature<500], cut(DF$water_temperature[DF$water_temperature<500], breaks=c(10,12,14,16,18)), length)/length((DF$water_temperature[DF$water_temperature<500]))`

Question 9.

At depth `2.4m`, what is the average temperature between the start of the day on June 1 and the end of the day on June 7?

What is the average temperature between the start of the day on June 8 and the end of the day on June 14?

What is the average temperature between the start of the day on June 15 and the end of the day on June 21?

What is the average temperature between the start of the day on June 22 and the end of the day on June 28?

Use the `tapply` function to answer all four of these questions with one line of code.

[Note: The original problem statement had an off-by-one typographical error on some of the dates.]

Solution:

The average water temperatures, week by week, were: 13.80297, 14.80604, 15.38551, 16.06782

`time1 <-- strptime("2012/06/01 00:00:00",  "%Y/%m/%d %H:%M:%S")`

`time2 <- strptime("2012/06/07 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time3 <- strptime("2012/06/14 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time4 <- strptime("2012/06/21 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`time5 <- strptime("2012/06/28 23:59:59",  "%Y/%m/%d %H:%M:%S")`

`tapply( DF$water_temperature[DF$water_temperature<500], cut(timevec[DF$water_temperature<500], breaks=c(time1,time2,time3,time4,time5)), mean, na.rm=TRUE)`

Note: The original problem statement had a small typographical error on some of the dates, which is corrected here.


Question 10.

At depth `13m`, how many data points have salinity greater than 12 and temperature greater than 14?

How many data points have salinity greater than 12 and temperature at most 14?

How many data points have salinity at most 12 and temperature greater than 14?

How many data points have salinity at most 12 and temperature at most 14?

Use the `tapply` function to answer all four of these questions with one line of code.

Hint:  You will need to embed a `list` into your `tapply`, as we did in the notes file `CO2examplecontinued.R` (the second CO2 example).

Solution:

We use a tapply with two kinds of breaks, to see that the counts are as follows:

`salinity > 12 and temperature > 14 --- 387 points`

`salinity > 12 and temperature <= 14 --- 4533 points`

`salinity <= 12 and temperature > 14 --- 190603 points`

`salinity <= 12 and temperature <= 14 --- 7176 points`

[source,r]
----
tapply( DF$water_temperature[DF$water_temperature<500],
  list(
    cut( DF$water_salinity[DF$water_temperature<500], breaks=c(0,12,24) ),
    cut( DF$water_temperature[DF$water_temperature<500], breaks=c(0,14,18) )
  ), length)
----


== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]

There is also some `supplemental-data.html` provided by the ASA.

You can see http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site] too.  In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you.  Since the data itself is so large, I saved it into a common data directory:
`/data/public/dataexpo2009/`

Notes:  If you want to read ALL of the data into `R` at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data.  You are not expected to import all of the data while you are solving the questions.  You can wait until you have solved the questions, and then come back and try to get the answers with all of the data.  So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv("/data/public/dataexpo2009/2006.csv"), read.csv("/data/public/dataexpo2009/2007.csv"), read.csv("/data/public/dataexpo2009/2008.csv") )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years.

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Just loading the data itself (if you choose all of the years) might take roughly 15 or 20 minutes to accomplish.  It would be done with some code like this:  (WARNING! This will take quite a long time to load, if you load all years at once.)

`bigDF <- rbind(
read.csv("/data/public/dataexpo2009/1987.csv"),
read.csv("/data/public/dataexpo2009/1988.csv"),
read.csv("/data/public/dataexpo2009/1989.csv"),
read.csv("/data/public/dataexpo2009/1990.csv"),
read.csv("/data/public/dataexpo2009/1991.csv"),
read.csv("/data/public/dataexpo2009/1992.csv"),
read.csv("/data/public/dataexpo2009/1993.csv"),
read.csv("/data/public/dataexpo2009/1994.csv"),
read.csv("/data/public/dataexpo2009/1995.csv"),
read.csv("/data/public/dataexpo2009/1996.csv"),
read.csv("/data/public/dataexpo2009/1997.csv"),
read.csv("/data/public/dataexpo2009/1998.csv"),
read.csv("/data/public/dataexpo2009/1999.csv"),
read.csv("/data/public/dataexpo2009/2000.csv"),
read.csv("/data/public/dataexpo2009/2001.csv"),
read.csv("/data/public/dataexpo2009/2002.csv"),
read.csv("/data/public/dataexpo2009/2003.csv"),
read.csv("/data/public/dataexpo2009/2004.csv"),
read.csv("/data/public/dataexpo2009/2005.csv"),
read.csv("/data/public/dataexpo2009/2006.csv"),
read.csv("/data/public/dataexpo2009/2007.csv"),
read.csv("/data/public/dataexpo2009/2008.csv"))`

Therefore, it is probably better (instead) to test your code on (say) three years of data, e.g., 2006-2008, before working on the full data set.

Question 1.

a. Consider the departure times (`DepTime`).  What fraction of the data are missing, i.e., are stored as `NA` values?

b. Within the departure times that are recorded (i.e., that are not `NA` values), the times are stored in `hhmm` format.  So there should be at most `24*60 = 1440` such possible times.  Are there other `DepTime` values?  Are they correct or perhaps erroneous?  How many such `DepTime` values (overall) seem to be erroneous?

Solution:

a. We can find the NA values using is.na.  If we sum the result, the TRUE's (from the is.na) become 1's and the FALSE's become 0's, so we get the number of NA's.  Then we can divide by the total number of DepTime's to get the desired fraction, which is 0.01939045.

`sum(is.na(bigDF$DepTime))/length(bigDF$DepTime)`

b. There are several values that are not valid times, as we can see:

`levels(as.factor(bigDF$DepTime[!is.na(bigDF$DepTime)]))`

It is up to whether you included (or not) the times 0000 and/or 2400, i.e., how you handled the midnight time. For example, here I am allowing 0000 but not 2400, but other methods are possible. Once we have the times less than 2400, we can check to make sure that the number of minutes is less than 60, by taking a modulus by 100, i.e., by dividing by 100 and getting the remainder.

We see that, once we restrict to times less than 2400, all of the times have valid minutes:

`sum((bigDF$DepTime[!is.na(bigDF$DepTime) & (bigDF$DepTime < 2400)] %% 100) >= 60)`

So the only invalid minutes are those which are 2400 or larger:                 

`length(bigDF$DepTime[!is.na(bigDF$DepTime) & (bigDF$DepTime >= 2400)])`

So there are 2695 such values that are >= 2400.


Question 2.

a.  Which departure times are the best, for minimizing the arrival delay (`ArrDelay`)?  More specifically, if our goal is to minimize the arrival delay, which of these 4 time categories is best time of day for our departure?  Between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Or between 6 PM and 12 midnight?

b.  Which of the 4 time categories for the departure will have the highest variance for arrival delay?

c.  Now please solve `2a` and `2b` again, splitting the data not only by the best time of day but also by the airline too.  That way, we can know what time of day and which airline we might prefer to use.

Solution:

a. We use tapply to split the ArrDelay, according to the four suggested groupings of the DepTime's.

`tapply(bigDF$ArrDelay, cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), mean, na.rm=TRUE)`

The best times to depart (by this measure) are between 6 AM and 12 noon.

b. The highest variance in ArrDelay's occur for flights departing between 12 midnight and 6 AM.

`tapply(bigDF$ArrDelay, cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), var, na.rm=TRUE)`

c. Now we analyze by airline too.

`tapply(bigDF$ArrDelay, list(cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), bigDF$UniqueCarrier), mean, na.rm=TRUE)`

`tapply(bigDF$ArrDelay, list(cut(bigDF$DepTime, breaks=c(0,600,1200,1800,2400),include.lowest=TRUE), bigDF$UniqueCarrier), var, na.rm=TRUE)`


Question 3.

a.  Which 10 airports have the most departures?

b.  Which 10 airports have the most arrivals?

c.  If we reconsider `3a` and `3b`, by splitting the data year by year, are the answers to `3a` and `3b` relatively consistent from year to year?

d.  Which are the most 10 popular pairs of departure/arrival city pairs?  (For instance, `IND-to-ORD` might be one such popular pair.)

Solution:

a. The cities with the most departures are:

`ATL     ORD     DFW     DEN     LAX     PHX     IAH     LAS     DTW     EWR `

`sort(tapply(bigDF$Origin, bigDF$Origin, length),decreasing=TRUE)[1:10]`

b. The cities with the most arrivals are:

`ATL     ORD     DFW     DEN     LAX     PHX     IAH     LAS     DTW     EWR `

`sort(tapply(bigDF$Dest, bigDF$Dest, length),decreasing=TRUE)[1:10]`

c. Wrapping everything up into one line, we can do this as follows, with the columns in order from 2006 to 2008, reading left-to-right. Indeed, the answers are pretty consistent, from year to year. For most departures:

`sapply(1:3, function(j) names(sort(tapply(bigDF$Origin, list(bigDF$Origin, bigDF$Year), length)[,j], decreasing=TRUE))[1:10])`

For most arrivals:

`sapply(1:3, function(j) names(sort(tapply(bigDF$Dest, list(bigDF$Dest, bigDF$Year), length)[,j], decreasing=TRUE))[1:10])`

d. The 10 most popular departure/arrival pairs are:

`from OGG to HNL from HNL to OGG from LAX to LAS from LAS to LAX from SAN to LAX from LAX to SAN from BOS to LGA from LGA to BOS from SFO to LAX from HNL to LIH `

`sort(table(paste("from", bigDF$Origin, "to", bigDF$Dest)), decreasing=TRUE)[1:10]`


Question 4.

a.  Which 5 airports are most likely to be on time for arrivals (on average)?

b.  Which 5 airports are most likely to be on time for departures (on average)?

c.  Which 5 airports are most likely to be delayed for arrivals (on average)?

d.  Which 5 airports are most likely to be delayed for departures (on average)?

Solution:

a. these 5 airports are most likely to be on time for arrivals (on average)

`HVN        ITH        LIH        EAU        ITO        HTS        OGG        KOA        PIH        CDC`

`sort(tapply(bigDF$ArrDelay, bigDF$Dest, mean, na.rm=TRUE))[1:10]`

b. these 5 airports are most likely to be on time for departures (on average)

`GLH       WYS       PIH       HVN       ITO       COD       EKO       CDC       LIH       IYK`

`sort(tapply(bigDF$DepDelay, bigDF$Origin, mean, na.rm=TRUE))[1:10]`

c. these 5 airports are most likely to be delayed for arrivals (on average)

`MQT      OTH      ACK      SOP      HHH      ISO      MCN      EWR      TTN      CIC`

`sort(tapply(bigDF$ArrDelay, bigDF$Dest, mean, na.rm=TRUE), decreasing=TRUE)[1:10]`

d. these 5 airports are most likely to be delayed for departures (on average)

`ACK      PIR      PUB      SOP      OTH      CEC      ADK      LMT      CKB      AKN`

`sort(tapply(bigDF$DepDelay, bigDF$Origin, mean, na.rm=TRUE), decreasing=TRUE)[1:10]`


Question 5.

a.  Which is the best day of the week to fly, if you want to minimize delayed arrivals?

b.  Which portion of the flights depart on which days?

c.  What percent of flights depart between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Between 6 PM and 12 midnight?

d.  Can you study 5b and 5c simultaneously, e.g., can you give an analysis by day of the week and time of day (in tandem), so that we know precisely which days of the week and which portions of the days are busiest for departures, i.e., so that we have a finer breakdown of the departure data?

Solution:

a. The best day of the week to fly, to minimize delayed arrivals, is Saturday (DayOfWeek=6)

`tapply(bigDF$ArrDelay, bigDF$DayOfWeek, mean, na.rm=TRUE)`

b. The portions of flights, by day, is (Monday is DayOfWeek=1; Sunday is DayOfWeek=7) the following:

`0.1479634 0.1453808 0.1467945 0.1473238 0.1478121 0.1245277 0.1401976`

`tapply(bigDF$DayOfWeek, bigDF$DayOfWeek, length)/length(bigDF$DayOfWeek)`

c. The portions of flights, by time of day, is:

`12 midnight to 6 AM:  0.02610195`

`6 AM to 12 noon:      0.38311298`

`12 noon to 6 PM:      0.37239489`

`6 PM to 12 midnight:  0.21839018`

`tapply(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], cut(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], breaks=c(0,600,1200,1800,2400), include.lowest=TRUE), length)/length(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)])`

d. Also with a breakdown by day of the week, we have:

`tapply(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], list( cut(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)], breaks=c(0,600,1200,1800,2400), include.lowest=TRUE),  bigDF$DayOfWeek[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)]), length)/length(bigDF$DepTime[bigDF$DepTime<2400 & !is.na(bigDF$DepTime)])`


Question 6.

a.  Which 5 carriers are the most likely to be delayed?

b.  Which 5 carriers are the most likely to be on time?

Solution:

a. The most likely carriers to be delayed are:

`OH        F9        EV        NW        TZ`

`sort(tapply(bigDF$ArrDelay[bigDF$ArrDelay > 0], bigDF$UniqueCarrier[bigDF$ArrDelay > 0], length) / tapply(bigDF$ArrDelay, bigDF$UniqueCarrier, length))`

b. The most likely carriers to be on time are:

`FL        WN        9E        AQ        HA`

`sort(tapply(bigDF$ArrDelay[bigDF$ArrDelay <= 0], bigDF$UniqueCarrier[bigDF$ArrDelay <= 0], length) / tapply(bigDF$ArrDelay, bigDF$UniqueCarrier, length))`

Question 7.

a.  Give a month-by-month breakdown of the percentage of cancelled flights.

b.  What are the worst 3 months of the year for cancelled flights?  I.e., during which 3 months are the most flights cancelled?  (Since 1987 is an incomplete year, please avoid the data from 1987 for `7a` and `7b`, because we do not want to unfairly balance the months.)

Solution:

a. Here is a month-by-month breakdown of cancelled flights:

`tapply(bigDF$Cancelled[bigDF$Cancelled==1], list(bigDF$Year[bigDF$Cancelled==1], bigDF$Month[bigDF$Cancelled==1]), length)/tapply(bigDF$Cancelled, list(bigDF$Year, bigDF$Month), length)`

b. The worst months of the year for cancelled flights are, respectively, Feb (worst of all!), Dec (2nd worst), Jan (3rd worst)

`sort(tapply(bigDF$Cancelled[bigDF$Cancelled==1], list(bigDF$Month[bigDF$Cancelled==1]), length)/tapply(bigDF$Cancelled, list(bigDF$Month), length),decreasing=TRUE)[1:3]`


Question 8.

Make a plot that shows how the number of flights departing `ORD` has changed, year by year.  Then add similar data to the same plot, for the number of flights departing `IND`, year by year.

Solution:

First we get the counts:

[source,r]
----
ChiDF <- subset(bigDF, subset=bigDF$Origin == "ORD")
ChiCounts <- tapply(ChiDF$Year, ChiDF$Year, length)
IndDF <- subset(bigDF, subset=bigDF$Origin == "IND")
IndCounts <- tapply(IndDF$Year, IndDF$Year, length)
----

Then we build the dotchart:
`dotchart(rbind(ChiCounts, IndCounts))`


Question 9.

Read the documentation for the `dotchart` function.  Make a `dotchart` as follows:  The x-axis should be the percentage of the time that flights are delayed more than 30 minutes.  On the y-axis, the main groupings should be according to month, and within each month, please show O'Hare and Indianapolis as cities of departure for flights.  The data to be displayed are the `DepDelay` data for 2007 only.  So the overall plot will show, month-by-month, a comparison of the `DepDelay` data for O'Hare and Indianapolis.

Solution:

First we get the 2007 data only:

`Chi2007DF <- subset(bigDF, subset=bigDF$Origin == "ORD" & Year==2007)`

`Ind2007DF <- subset(bigDF, subset=bigDF$Origin == "IND" & Year==2007)`

Then we get the delay counts and the total counts:

`Chi2007DelayCounts <- tapply(Chi2007DF$DepDelay[Chi2007DF$DepDelay>30], Chi2007DF$Month[Chi2007DF$DepDelay>30], length)`

`Chi2007AllCounts <- tapply(Chi2007DF$DepDelay, Chi2007DF$Month, length)`

`Ind2007DelayCounts <- tapply(Ind2007DF$DepDelay[Ind2007DF$DepDelay>30], Ind2007DF$Month[Ind2007DF$DepDelay>30], length)`

`Ind2007AllCounts <- tapply(Ind2007DF$DepDelay, Ind2007DF$Month, length)`

Finally we make a matrix and then a dotchart:

`M <- rbind(Chi2007DelayCounts/Chi2007AllCounts, Ind2007DelayCounts/Ind2007AllCounts)`

`row.names(M) <- c("ORD","IND")`

`dotchart(M)`


Question 10.

Make another `dotchart`, similar to the one in question `9`, where the main groupings on the y-axis are O'Hare and Indianapolis, and within each city, display all 12 months.  Again, the data to be displayed are the `DepDelay` data for 2007.  The x-axis should again be the percentage of the time that flights are delayed more than 30 minutes.  So the overall plot will show, for each of the two cities, a month-by-month comparison of the `DepDelay` data.  If you are able, you can organize the months according to their percentage of time delayed more than 30 minutes, rather than according to alphabetic order.

Solution:

Now we rearrange the matrix from #9, switching the role of rows and columns, by taking a transpose:

`newM <- t(M)`

Then we plot the data:

`dotchart(newM)`



== Project 4

This project is about visualizing data.  It will give you some time to write ababout data visualization and to take a little break from coding.

Question 1.

Check out the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots on `Many Eyes` (please give links to each of these plots) that violate the concepts of effective data visualization that are discussed in the handouts from class (e.g., in Cleveland's book and Robbins's book, and in the paper "How to display data badly").  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine, for instance, that you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  (Your discussion of these 4 plots should be at least one single-spaced page in (say) 12 point Times font, for example... but more than 1 page is certainly allowed.)  Each student should write about at least 1 plot.

Question 2.

Revisit the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots (again, with links to the plots) on `Many Eyes` that do an overall good job of effective data visualization.  Justify the reasons why you think that the plots are effective.  (Again, please write at least one page total, justifying the reasons that you think the plot is effective.)  Each student should write about at least 1 plot.

Question 3.

Check out the website http://www.informationisbeautiful.net[Information Is Beautiful].  Find 4 (or more) separate plots on `Information Is Beautiful` (please give links to each of these plots) that violate the concepts of effective data visualization.  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  Your constructive criticism should be at least 1 page altogether.

Question 4.

The http://www.gapminder.org/world[Wealth and Health of Nations] is a fun depiction of data.  On the other hand, as with many depictions of data, it violates some of the techniques of effective data display.  Please write an explanation of which techniques of effective data display are violated.  If you imagine you are writing a constructive criticism to the authors of this animation, please make suggestions for how the depiction of data (for the health and wealth, over the years displayed) could have been done more effectively.  Please make sure your explanation is at least 1 page long.

Question 5.

Describe (at least!) 4 very significant ways that the poster winner "Congestion in the sky" http://stat-computing.org/dataexpo/2009/posters/ from the `Data Expo 2009` poster competition results could be significantly improved, using the concepts of effective data visualization.  Write a constructive criticism (of at least 1 page) that gives suggestions for improvement on each aspect that you criticize.

Question 6.

For the other posters (do not use the winner, "Congestion in the sky", since it was discussed already in question `5`), find a total of at least 4 significant ways that some of the other posters can be improved.  You can analyze several different posters, that is OK.  Your constructive critique should be at least 1 page.

Question 7.

Which of the posters in the `Data Expo 2009` do you think should be the winner?  Why?  (It is OK if you choose the poster that actually won, or any of the other posters.)  Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long.

Questions 8, 9, 10.

Imagine that you are going to enter the `Data Expo 2009`.  Rather than having to organize your information into a poster, prepare 3 pages of analysis, exploring some aspects of the airline data set that are interesting to you, and which you think might be of broad interest to potential readers too.  Your discussion and plots should be at least 3 pages long.

== Project 5

The code found in the `week6.html` Week 6 examples should be helpful in this problem set.

Question 1.

Practice using the sapply function:

a. Find, with only one line (altogether) of `sapply` code, the 5 lengths of the following 5 vectors:

* the `LakeHuron` vector,
* the `waiting` vector in the `geyser` data (remember to load the `MASS` library first)
* the `duration` vector in the `geyser` data
* the `chickwts$weight` vector
* the `mtcars$mpg` vector

b. Now find the average value stored in each of the 5 vectors, using `sapply`.

c. Check that `R` did the right thing in `1b` by manually taking the mean of each vector, using 5 separate lines of code.

d.  If you accidentally use `c` instead of `list` in `1b`, `R` just takes an average of individual values, but the average of 1 value is just the value itself, so `R` returns the full list of values.  Please give this (incorrect) behavior a try, just to see how it misbehaves!

e. Now find the variance of the values stored in each of the 5 vectors, using `sapply`.

f. Check that `R` did the right thing in `1e` by manually finding the variance of each vector, using 5 separate lines of code.

g.  If you accidentally use `c` instead of `list` in `1e`, `R` just takes the variance of each individual value, but `R` gives an `NA` when taking the variance of an individual value (you can try this, e.g., `var(3.79)` gives `NA`, so `R` returns `NA` for each value).  Please give this (incorrect) behavior a try!

h.  Examine the head of the `Cars93` data.  This data set has a lot of types of columns.  Use `sapply` to find out the kinds of classes for each of the 27 columns in this `data.frame` (using just one call to `sapply`; hint: use `class` for the function).

Solution:

a.

[source,r]
----
98 299 299  71  32
library(MASS)
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), length )
----

b.

[source,r]
----
579.004082  72.314381   3.460814 261.309859  20.090625
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), mean )
----


c.  We get the same answers as above

[source,r]
----
mean(LakeHuron)
mean(geyser$waiting)
mean(geyser$duration)
mean(chickwts$weight)
mean(mtcars$mpg)
----

d.

`sapply( c(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), mean )`

e.

[source,r]
----
1.737911  192.941101    1.317683 6095.502616   36.324103
sapply( list(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), var )
----

f.  We get the same answers as above

[source,r]
----
var(LakeHuron)
var(geyser$waiting)
var(geyser$duration)
var(chickwts$weight)
var(mtcars$mpg)
----

g.

`sapply( c(LakeHuron, geyser$waiting, geyser$duration, chickwts$weight, mtcars$mpg), var )`

h.

[source,r]
----
head(Cars93)
sapply( Cars93, class )
----


Question 2.

a. Use the `mapply` function, with the `paste` function, and the vectors
`c("a","b","c","d","e")`
and
`c("A","B","C","D","E")`
and the parameters
`sep=""` and `USE.NAMES=FALSE`
to print these five sentences:
`[1] "The uppercase version of a is A" "The uppercase version of b is B" "The uppercase version of c is C"`
`[4] "The uppercase version of d is D" "The uppercase version of e is E"`

b. Use the `row.names` function, and the column of `population` data, both with the `state.x77` data set, as well as the `mapply` function, to print a vector of 50 sentences.  (It might be helpful to use the parameters `USE.NAMES=F` and `sep=""`.)  The vector should start with the following six sentences:
`[1] "Alabama has 3615 thousand people."        "Alaska has 365 thousand people."`
`[3] "Arizona has 2212 thousand people."        "Arkansas has 2110 thousand people."`
`[5] "California has 21198 thousand people."    "Colorado has 2541 thousand people."`

c. Revise your answer to `2a`, by actually multiplying the `population` data by 1000, so that the vector should start with the following six sentences:
`[1] "Alabama has 3615000 people."        "Alaska has 365000 people."`
`[3] "Arizona has 2212000 people."        "Arkansas has 2110000 people."`
`[5] "California has 21198000 people."    "Colorado has 2541000 people."`

Question 3.

a. Make a `data.frame` containing all of the cars from `mtcars` with `hp>100` and `8` cylinders.  Create a new `data.frame` with these cars, displaying only the `mpg`, `cyl`, `hp`, and `qsec`.

b. Make a `data.frame` containing all of the rows describing provences from the `swiss` data set with 50% or more `Catholics` and 50% or more of `males` involved in agriculture.  Within this specific `data.frame`, find the mean and standard deviation of the `Fertility` data.

c. Make a `data.frame` containing all of the rows in the `chickwts` data for which the feed is either `horsebean` or `soybean`.  What is the average weight (altogether) across these two kinds of feed?

Question 4.

Step through Dr Ward's `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data.  It takes a little time to understand completely what is happening, but essentially we are able to read data from dozens of files with ease (i.e., without having to download them individually, by hand), and to extract and assemble the data in them.  Note that the time parameter in these files is the same data we had in the earlier project, but is stored differently (and, hence, extracted differently) than in the earlier project.

a.  Use the `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data to extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `2.4m`.

b.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `8.2m`, from:
`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.CT/` (Beware: The starting month is not the same for this data set, compared to the previous one.)

c.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `13.0m`, from: `http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.CT` (Beware: Again, the starting month is not the same for this data set, compared to the previous two.)

Question 5.

Extract the `Phycoerythrin` and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Phycoerythrin/`

Question 6.

Extract the `Oxygen Concentration` (`oxygen`), `Oxygen Saturation` (`oxygensat`), and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Oxygen/`

Question 7.

a. For each of the 3 types of data listed above (in questions `4`, `5`, `6`) at each of the 3 depths, find the number of data points per month.  For instance, starting with the `temperature`/`conductivity`/`salinity` data at depth `2.4m`, find the number of data points per month.  Then do the same for `8.2m` and for `13.0m`.  Then do this again for the `Phycoerythrin` data.  Then do it again for the `Oxygen` data.  To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 of the saturn03.240.A.CT data contains 202,702 data points at depth 2.4m.`

b. Re-calculate your answer to `7a`, so that it is normalized according to the number of days in the month. In other words, get the number of data points divided by the number of days in the month. To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 contains an average of 6756.733 data points per day, during the 30 day period, for a total of 202,702 data points during the month, at depth 2.4m.`

Question 8.

a.  Extract the `temperature` data from the `SATURN03` station for June 2012 at depth `2.4m`.  There are 202702 data points.  Save these in a variable called `tempdata`.  Also get the analogous 202702 `time` data points.  Save these in a variable called `temptimes`.

b.  Extract the `oxygen` saturation data (the 2nd parameter in the data set `saturn03.240.A.Oxygen`) from the `SATURN03` station for June 2012.  There are 15725 data points.  Save these in a variable called `oxydata`.  Also get the analogous 15725 time data points.  Save these in a variable called `oxytimes`.

c.  Notice that we would be hard-pressed to compare the `temperature` and `oxygen saturation` data, because there are vastly different amounts of data in the two vectors and (perhaps more importantly) they were measured at different points in time.  We can, however, build a function that predicts the behavior of the temperature data at ALL points in time, and then use it to figure out how the temperature data would have behaved, if it was measured at the same 15725 `time` points as the `oxygen saturation` data, and then we could compare the `temperature` and `oxygen saturation` data.  This can be done as follows:
`tempfunction <- approxfun(temptimes, tempdata)`
This makes `f` into a function that can predict the `temperature` behavior at any `time` we like.  Then we run the function on the `oxygen saturation` times, to see how `temperature` would have behaved at the 15725 times when `oxygen saturation` was measured:
`tempatoxygentimes <- tempfunction(oxytimes)`
Finally, we can plot the `temperature` versus the `oxygen saturation` data this way:
`plot(tempatoxygentimes,oxydata)`

Question 9.

a.  Make comparisons between some of the other variables, in the style of how we did things in question `8`.

b.  Which pair of variables (in the June 2012 data sets) seem to be the most strongly correlated?  Why do you think so?

c.  What could go wrong with the method discussed in `8` and `9`?  Hint: for instance, in part `8c`, take a look at:
`range(temptimes)`
`plot( tempfunction(seq(1338537601,1341129599,by=100) ) )`
How could we potentially fix the problem that happens when missing data occurs?  [You do not have to actually fix it; but briefly mention some way that you might fix it.]  Can you see this problem in the plot?  [We will discuss this problem more, in a set of future questions, in another project.]

Question 10.

a.  There are 7 data sets inside the directory: `/data/public/NARR/pressure`
How many variables do they each contain?

b.  How many pieces of data does the `lat` variable contain in each file?  How about the `lon` variable?  How about the `Lambert Conformal` variable?  Are all of the `lat` variables identical across all 7 files?  If so, how do you know?  If not, how are they different?  What about the `lon` variable?  What about the `Lambert Conformal` variable?

c.  What are the sizes (i.e., dimensions) of the 4th variable in each of the 7 files?  What percent of the 4th variable is missing in each of the 7 files?

d.  If you store the `time` vector from a file in a vector `t`, then the code:  `format(as.POSIXct(3600*t, origin="1800-01-01"), tz="UTC+0:00")`  will convert the time into a human-readable format.  The `3600` converts the hours into seconds, and the seconds are given in units after January 1, 1800.  (Dr Ward fiddled around with this for awhile to figure this out.)  Question:  Do all 7 files have the same time vector?

e.  What is the `time` unit between consecutive times in each of these vectors?

== Project 6

The code found in the `week8.html` Week 8 examples should be helpful in this problem set.

Question 1.

Compare the 3 variables found in the first `SATURN03` data set we studied, namely, the `saturn03.240.A.CT_2012_06_PD0.csv` data set, from depth `2.4m`.  Compare them in pairs, to see if any pair of them yields a very good linear model. In all of these cases, be sure to remove any outliers, if necessary.

a. Make a simple linear regression to try to predict the `electrical conductivity` from the `temperature`.

b. Make a simple linear regression to try to predict the 'salinity' from the 'temperature'.

c. Make a simple linear regression model to try to predict the 'electrical conductivity' from the 'salinity'.

d. Which one of these linear models seems most amenable to linear modeling?  Why?

Question 2.

a. Make a simple linear regression model to predict the `mpg` from the `mtcars` data, based on the `hp`.  Plot the two variables, along with the line suggested by a simple linear regression model.

b. Make a multiple regression model to predict the `mpg` from the `mtcars` data, based on the `hp` and the `disp`.

c. Using the multiple regression model, what kind of `mpg` might we guess that a car has, if it has 147 `hp` and 230 `disp`?

Question 3.

a. Load the 1990 airline data from the `dataexpo` into a `data.frame`.

b. Use the `subset` command to extract only the flights from June 1990.

c. Build a simple linear regression model that predicts the arrival delays from the departure delays.

d. Plot both the delays, putting the arrival delays on the y-axis and the departure delays on the x-axis.

e. Draw the line from the simple linear regression model on the plot.

f. Repeat steps `3c` through `3e`, removing the outliers, e.g., removing the flights with departure delays that are more than `500` and removing those that are less than `-50`.  I.e., restrict attention to flights with departure delays between `-50` and `500`.

Question 4.

a.  Generate 100 (continuous) uniform random numbers, uniformly distributed between 0 and 1.

b.  For each uniform random number `U` in part `a`, define `V = -log(U)/3`.  Make this transform for all 100 numbers from `4a`.

c.  Generate 100 exponential random numbers with rate `3`.

d.  Use a `qqplot` to convince yourself that the numbers from `4b` have the same kind of distribution as the numbers in `4c`.  I.e., if `U` is a continuous uniform random variable, then `-log(U)/3` is an exponential random variable with rate `3`, i.e., with mean `1/3`.

e.  Re-do parts `4a` through `4d` with millions of numbers instead of just 100 numbers, to reinforce this notion in your mind.

Question 5.

a.  Generate 1,000,000 (continuous) uniform random numbers (each between 0 and 1) and store them in a matrix `M` with 1000 rows and 1000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 1000 numbers, each of which is equal to the sum of the 1000 uniforms.  Store the result in a vector `v`.

c.  Subtract 500 from each entry of `v` and then (afterwards) divide each number by `sqrt(1000/12)`, i.e., by `9.1287`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Question 6.

a.  Generate 100,000,000 exponential random numbers, each with `rate = 5`, and store them in a matrix `M` with 10000 rows and 10000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 10000 numbers, each of which is equal to the sum of the 10000 exponential random numbers.  Store the result in a vector `v`.

c.  Subtract 2000 from each entry of `v` and then (afterwards) divide each number by `sqrt(10000/5^2) = 100/5 = 20`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Question 7.

a.  Use the built-in `R` data set for "Pharmacokinetics of Theophylline" (stored in `Theoph`) to build a multiple linear regression model of the concentration, based on the weight, dose, and time.

b.  If a person weighed `66 kg`, and received a dose of `4 mg/kg`, and it has been 6 hours since the dose was administered, what is the predicted level of concentration?

Question 8.

Look at the departure delays from the June 1990 flights.  If we restrict attention to departure delays of 30 minutes or more, what kind of distribution do you think the data has?  Normal?  Uniform?  Exponential?  Justify your answer with a `qqplot`.  How closely can you estimate the parameter(s) of the distribution you think that this data has?

== Project 7

The following resources might be helpful for you:

* Dr Ward's notes: `week9.txt` on Introduction and immersion into the UNIX Operating System and the bash shell
* http://en.wikipedia.org/wiki/List_of_Unix_programs[List of Unix utilities (Wikipedia)]
* http://linuxcommand.org/tlcl.php[The Linux Command Line] official direct link for http://sourceforge.net/projects/linuxcommand/files/TLCL/13.07/TLCL-13.07.pdf[pdf download] of the book by William Shotts (No Starch Press, 2012)

Question 1.

a.  How many lines are found in the file `/etc/passwd`?

b.  Remember (from the notes) that the command: `cat /etc/passwd | cut -f5 -d:` is used to find each person's name who is a user in the system.  Instead of printing the full names of the users, print their usernames (e.g., `mdw`).

Question 2.

a.  How many users have her/his directory in the `/home` filesystem?  (This is described in the 6th of the 7 fields on each line.)

b.  Now extract the first names of each such user who has her/his directory in the `/home` filesystem.

c.  Save the results into a file in your home directory, named `firstnames.txt`

Question 3.

a.  How many words in the file `/usr/share/dict/words` contain the letter `q`?

b.  Convince yourself that the command:  `awk '{print length}'` will print the length of the words in a file.  [Feb 2023 note: I think this should say "length of the lines in a file".]

c.  Find the length of the words, line by line, in the file `/usr/share/dict/words`

Question 4.

a.  What is the longest word length, among all word lengths in the file `/usr/share/dict/words`? Hint: you might need to use `awk '{print length}'` and `sort`, with a certain flag on the sort command.  Such a flag comes in handy when you are sorting text that is numeric.  It would be helpful to read the manual for the sort command, to see which flag to use.

b.  Instead of looking for the longest word length, after you sort the word lengths numerically, pipe the output to the `uniq` command, and use a flag on the `uniq` command to count the number of words of each length.

Question 5.

In the directory `/data/public/election2008` there is some data related to the 2008 election.  There are 49 files, namely, one for each of the 48 mainland states, and one for `DC`.  Each line has seven pieces of data, namely, the percent and number of people who voted for Obama in 2008, the percent and number who voted for McCain in 2008, the overall percent of registered votes, the state, and the county.

a.  How many counties are represented in these 49 files?

b.  What is the largest number of votes in one county for Obama?

c.  What is the largest number of votes in one county for McCain?

d.  How many counties have one or more of the following words in the title:  `north`, `east`, `south`, `west`?

e.  How many characters are found in the longest county name?

Question 6.

In the directory `/data/public/dataexpo2009` there is the airline flight data, which we are already familiar with.

a.  How many flights were taken in 2006?

b.  How many flights were taken altogether, from 1987 to 2008?

c.  How many flights had `IND` as the `Origin` city in 2006?

d.  How many flights had `IND` as the `Origin` city altogether, from 1987 to 2008?

Question 7.

a.  In the airline data, how many unique carriers are there, in the 2006 data set?

b.  In the airline data, how many unique carriers are there altogether, from 1987 to 2008?

c.  What was the longest flight taken in 2006, in terms of miles?

d.  What was the longest flight taken altogether, from 1987 to 2008, in terms of miles?

e.  How many flights had this longest flight distance (in terms of miles), from 1987 to 2008?

Question 8.

Most of the `UNIX` commands you can access are contained in one of three places, namely:

* `/usr/local/bin`
* `/bin`
* `/usr/bin`

Print a list of the names of the programs (be sure to check programs in all three of these places) that have the word `zip` somewhere in the title of the program.

== Project 8

The code found in the `Week 10 examples` should be helpful in this problem set.

Question 1.

Consider the file `yow.lines`, which is distributed with emacs 21.4. It can be downloaded from the llc server or you can access it directly from `/proj/www/2014/29000/projects/yow.lines` if you prefer. (Some of the lines in this file are very strange, but this is a standard text file, which is widely known and widely distributed too, on every Linux and UNIX system that contains emacs 21 and earlier.)

a. How many lines start with a capital letter I?

b. How many lines end with a question mark?

c. How many lines end with an exclamation point?

d. How many lines contain 3 or more exclamation points in a row (which may or may not be at the end of the phrase)?

Question 2.

Continuing to study `yow.lines`:

a. How many lines from contain 3 or more exclamation points altogether (which may or may not be consecutive)?

b. Print `yow.lines` with all uppercase letters converted to lowercase letters.

c. On how many lines does the word `yow` appear (regardless of capitalization)?

Question 3.

Consider the file `/usr/share/dict/words` on the llc server.

a. How many words have exactly 6 characters?

b. How many words have an occurrence of dog as a subword?

c. How many words have the letters dog, in that order, but not necessary in consecutive order?

Question 4.

Continuing to study `/usr/share/dict/words` on the llc server:

a. How many words start with the 2-letter phrase `de`?

b. How many words end with the 2-letter phrase `ly`?

c. How many words do not start with the 3-letter phrase `con`?

Question 5.

This question is based on the Social Security baby names data set. You can read about the Social Security baby names at: `http://www.ssa.gov/OACT/babynames/namesbystate.html`  The data set itself can be downloaded from the llc server or you can access it directly from `/proj/www/2014/29000/projects/babynames.txt` if you prefer. The data set contains 134 years of data (1880 to 2013), with 1000 boy names and 1000 girl names per year. The rank of each name is given within each year. The number of boys or girls born with each name is given in each year.

a. How many children were named Mary during 1880-2013?

b. What are the ranks of Mary's name during each of these 134 years?

c. How many different girl names (from this data set) start with the letter A? Be sure to remove duplicated names, i.e., count each name just once.

d. How many different boy names (from this data set) have 4 letters? Be sure to remove duplicated names, i.e., count each name just once.

Question 6.

Continuing to study the baby names:

a. What are the names (in alphabetic order, without duplicates) that have a double consecutive vowel, e.g., Aa or aa or Ee or ee or Ii or ii or Oo or oo or Uu or uu? Be sure to remove duplicated names, i.e., display each name just once. [Hint: We saw `&&` is used for `and`; similarly, `||` is used for `or`.]

b. Which names have an occurrence of q that is not followed by a u? Be sure to remove duplicated names, i.e., display each name just once.

c. Which names have two or more z's (regardless of uppercase or lowercase), which are not necessary consecutive? Be sure to remove duplicated names, i.e., display each name just once.

Question 7.

Consider the airline flight files stored in this directory: `/data/public/dataexpo2009` on the llc server. We reconsider a few questions that we solved earlier in R. The advantage of using awk is that the speed is faster, and we do not have to input all of the data at the start (recall we had to pre-load all of the data in R).

a. Which 10 airports have the most departures? [It might help to use `awk` and `sort` and `uniq` and another `sort` in conjunction, with a count flag for `uniq`.]

b. Which 10 airports have the most arrivals?

c. Which are the 10 most popular pairs of departure/arrival city pairs? (For instance, IND-to-ORD might be one such popular pair.)

Question 8.

Continuing to study the airline data:

a. Make a new file called `weekend1995.csv` that contains only the flights that were on a weekend, from the 1995 flights file.

b. Make a new file called `longdelays1995.csv` that contains only the flights that had a departure delay of 1 hour or more, from the 1995 flights file.

c. Make a new file called `JFKtoLAX1995.csv` that contains only the flights that were from JFK to LAX, for the 1995 flights file.

Questions 9 and 10.

I might provide another couple of questions soon, as usual, depending on how students seem to be doing with these questions.... BUT I want to see how things go with the problems outlined above. I like to be flexible, as you know!


== Project 9

The code found in the `Week 11 examples` should be helpful in this problem set.

Question 1.

a. Find the first and last names of all the players who attended Purdue University.

b. Same question, but add the years (starting and ending years) for which each of these people played for Purdue.

c. Same question, but now include 1 row for each year that the player was in the Major Leagues. Add a column for the Major League year and a column for home runs that year.

Question 2.

a. Find the first and last names of all the players who attended college anywhere in Indiana.

b. Same question, but add the years (starting and ending years) for which each of these people played in their universities in Indiana.

c. Same question, but now include 1 row for each year that the player was in the Major Leagues. Add a column for the Major League year and a column for home runs that year.

Question 3.

a. Find all teams that won 105 or more games in a season. List the year, the team's full name, and the number of wins in that season.

b. Same question, but now include the first and last name of the manager for the team.

Question 4.

a. Find all the years in which the Chicago Cubs were the League Champions.

b. Same question, but now include the first and last name of the manager for the team.

Question 5.

a. Find all players who stole 100 or more bases. For each such player, list the player's first name, last name, the number of bases stolen, and the year.

b. Same question, but now include the full name of the team that player played on, during the year of the achievement.

Question 6.

a. Find the first and last names of all players inducted into the Hall of Fame since 2008 (including 2008 itself, and all years since then). Be sure to list the year of induction.

b. Same question, but now sort the results of the query by year.

c. Same question, but now give just the counts of the number of players inducted per year. Your results should have 1 column with the year and 1 column with the counts of how many players were inducted that year.

Question 7.

a. For each pitcher who had 20 or more wins, list the player's first name, last name, number of wins, and the relevant year.

b. Same question, but now give just the counts of the number of players per year. Your results should have 1 column with the year and 1 column with the counts of how many players had 20 of more wins that year.

Question 8.

a. The "40-40" club is a name commonly given to the group of all players who stole 40 or more bases and hit 40 or more home runs in the same year. For each member of the 40-40 club, list the player's first name, last name, the relevant year for the achievement, the number of bases stolen in that year, the number of home runs in that year, and the full name of the player's team.

b. Same question, but now for the 30-30 club.

c. Same question, for the 30-30 club, but now ignore the names of the players and their specific numbers of home runs and stolen bases, and ignore the years too. Instead, give the summary totals, by team, of how many times the team has had a 30-30 player (with possible repetitions). For example, the Chicago Cubs would have count "2", since Sammy Sosa achieved this feat twice in his career, but no other Chicago Cub achieved this feat.

Question 9.

a. Find all players who earned 20 million dollars or more in a season. For each such occurrence, list the player's first name, last name, the year, the salary, the teamID, and the Team's full name.

b. Give a table that has a "Year" column, and a "Total Salary" column, and a "Team" column.

c. Sort the table in the previous question according to the total salary column.

Question 10.

a. Consider the total number of saves by pitchers during their entire careers. A few pitchers had 300 or more saves during their careers. Make a list of all such pitchers. For each such pitcher, give his first name, last name, and the total number of saves that the pitcher had during his career.

b. Same question, but instead of finding pitchers with 300 or more saves, find pitchers with 2000 or more strikeouts during their careers.

== Project 10

The code found in the `Week 13 examples` should be helpful in this problem set.

Please answer questions 1 to 3 in R, by making calls to your MySQL database.

Question 1.

a. Who are the 10 pitchers with the highest tallys of strikeouts throughout their careers?

b. Who are the 10 wildest pitchers, i.e., which pitchers have the highest tallys of wild pitches during their whole careers?

c. Who are the 10 pitchers with the most Outs Pitched (`IPOuts`) during their career?

Question 2.

a. Which team has the most home runs of all time (summed over all years)?

b. Which team has the largest average number of home runs per year, where this is averaged over all years?

Question 3.

a. Rank the 50 states according to the number of baseball players who were born in the state.

b. What percent of players have a left batting hand? Right batting hand? Both?

Please answer questions 4 to 6 in R, by using XML tools.

Question 4.

Consider the 11 pages of college rankings found at the US News and World Report page: `http://colleges.usnews.rankingsandreviews.com/best-colleges/rankings/national-universities/data`

a. Which 10 universities have the highest enrollments?

b. Extract the states where the colleges are located. Then use this information to make a list that shows, for each state, how many colleges (from this list) are contained in that state.

c. Which 10 universities have the highest tuition charges? (For universities with both in-state and out-of-state tuitions listed, use the out-of-state listing.)

Question 5.

Consider Dr. Ward's `iTunesMusicLibrary.xml` file (the listing of the songs), located in `/data/public/iTunes/iTunesMusicLibrary.xml` (this has not been updated for a few years, but it is still relatively large and perhaps is interesting).

a. According to their numbers of songs in the playlist, what are the top 10 artists who appear in the playlist?

b. According to their numbers of songs in the playlist, what are the top 3 genres?

There are several ways that you could work on problem 5. One possible method is to extract all of the keys, as follows:

`iTunesDoc <- xmlParse("/data/public/iTunes/iTunesMusicLibrary.xml")`

`iTunesvec <- xpathSApply(iTunesDoc, "//*/dict/dict/dict/child::*/child::text()", xmlValue)`

Warning: This code takes several minutes to run.

Question 6.

Consider the example with the Presidential votes in Indiana.

a. Build your own county-by-county summaries, in each State. Try to do this as efficiently as possible.

b. Do the summaries agree with Politico's State-by-State summaries? If not, what are the differences?

