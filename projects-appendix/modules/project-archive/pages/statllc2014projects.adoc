= STAT-LLC Fall 2014 STAT 29000 Projects

== Project 1

Question 1.

During which pair of years did the level of Lake Huron rise the most?
The data to use is from the built-in `LakeHuron` data set.
(E.g., during 1875 to 1876, Lake Huron rose 1.48 feet.)  It might help to use the `diff` command.

Question 2.

a. What is the average duration of an eruption in the `geyser` dataset in the `MASS` library?

b. What were the 10 longest durations?

c. How many durations were 3 minutes or longer?
(You do not need to install the `MASS` library; it is installed already.  You do, however, need to load the `MASS` library.)

Question 3.

a.  Which car(s) in the `mtcars` data set had the highest gas mileage?

b.  Which car(s) had the highest horsepower?

c.  Which car(s) had the shortest (i.e., fastest) 1/4 mile time?

d.  How many cars had manual transmission?

e.  How many cars had manual transmission and also six cylinders?

Question 4.

a.   Which states are (strictly) larger in population than Indiana but (strictly) smaller in population than Pennsylvania, according to the data in the `state` data set?
Hint: You can get the state populations using `state.x77[,"Population"]`.

b.   Which states are (strictly) larger in land area than Indiana but (strictly) smaller in land area than Pennsylvania, according to the data in the `state` data set, as listed in `state.x77[,"Area"]`?

Question 5.

If `Z` is a standard normal random variable, we know that `Z` has average 0 and variance 1.  Use `R` to simulate:

a. the value of the average of `|Z|`, and

b. the value of the variance of `|Z|`.

Here, `|Z|` is just the absolute value of `Z`.

Question 6.

Write a function called `countas` that takes a sequence of words and returns the number of words that have 1 or more `a`s. For instance, `countas(  c("ate", "hello", "duolingo", "pat", "aa")  )` should return the value 3.  Hint:  It might help to use the `grep` function.

Question 7.

a.  Write a function called:  `firstthree` that returns the location of the first occurrence of 3 in a vector.  For instance, `firstthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 2.

b.  Write a function called:  `thirdthree` that returns the location of the third occurrence of 3 in a vector.  Ffor instance, `thirdthree( c(-2.5,3,3,0.001,22,5,7,19,3,17) )` should return the value 9.

Question 8.

Write a function called:  `topfive` that returns the most common five values in a vector, along with the counts for each of the 5 values.

Question 9.

a. Euler's number is 2.718281828459...  Euler's number is defined as `1 + 1/1 + 1/(1*2) + 1/(1*2*3) + 1/(1*2*3*4) + 1/(1*2*3*4*5) + ...` Find a good way to calculate this in `R`, with few keystrokes. If you subtract 2.718281828459 from your estimate, you should get something very small, e.g., roughly `4.5 * 10^{-14}.`

b.  Find a good way to approximate the value of Pi, using only the fact that `Pi^2 / 6 = 1/1^2 + 1/2^2 + 1/3^2 + 1/4^2 + 1/5^2 + 1/6^2 + 1/7^2 + ....`

Question 10.

a. The triangular numbers are: `1, 3, 6, 10, 15, 21, 28, 36, 45, 55, ...` See http://oeis.org/A000217 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

b. The tetrahedral numbers are: `1, 4, 10, 20, 35, 56, 84, 120, 165, 220, ...` See: http://oeis.org/A000292 Find an efficient way to compute, in `R`, the first 100 such numbers.  Does your method extend to the first 1000 such numbers too?

== Project 2

Question 1.

Consider the Columbia River Estuary dataset discussed in the `week 2 notes`

a.  Download the data set (we no longer need to do this).

b.  Import the `saturn03.240.A.CT_2012_06_PD0.csv` data set into `R`, using the `read.csv` function.

c.  Use the `strptime` function to convert the first column of the data into numerical times that `R` can easily handle.

Question 2.

a.  What is the most common time (in seconds) between consecutive measurements, in the data set?  How often is the data sampled with this exact difference in time, between consecutive measurements?

b.  What is the mean time between consecutive measurements?  Why is this significantly different from the most common time, found in part `2a` above?

Question 3.

a.  Suppose that we treat "15 seconds" as a threshold in consecutive time measurements, i.e., if the machine goes more than 15 seconds without taking a measurement, we consider that the machine is temporarily broken/clogged/stuck/etc.  With this level of threshold, how many times did this particular machine (at this particular location) get stuck during June 2012?

b.  How long is longest duration when the machine was broken?  When did this occur? Specifically: when did it break, and when did it start working properly again?

c.  Find the ten longest durations for when the machine was broken; just give each such measurement in seconds.

Question 4.

a. Does the device which measures the electrical conductivity ever give a false reading?  If so, when?  Give the specific times (e.g., the day(s), hours, minutes, seconds), when this occurs in June, for each such occurrence.

b. Are any of these times in `4a` the same as the one (unique) time when the temperature device gave a false reading?  (We saw, in the notes, that the temperature device had one false reading.)

c.  Does the device which measures the salinity ever give a false reading?  What evidence to you have to support this claim?

Question 5.

a.  Repeat the questions from `2a`/`2b`/`3a`/`3b`/`3c`, but now use the data set from the same point on the Columbia River Estuary but at the depth of `8.2m` (the data from the questions above was measured at `2.4m` below the surface).  The data set from `8.2m` below the surface is available at `saturn03.820.A.CT_2012_06_PD0.csv`.

b.  Does the longest time in which the machine was broken in `3b` (at depth `2.4m`) correspond roughly to the same longest time in which the machine was broken in this current data set, at depth `8.2m`?  For this longest time interval, what are the times (at depth `8.2m`), when the machine did break, and when did it start working properly again?

c.  Make a plot of the temperature data at depth `8.2m`.  There is exactly one false reading in which the temperature is too high, and exactly one false reading in which the temperature is too low.  Be sure to remove these points before plotting.

Question 6.

a.  We also have data from depth `13m` below the surface in the file `saturn03.1300.R.CT_2012_06_PD0.csv`.  Import this data into `R`.

b.  Is the water temperature generally highest, on average, at depth `2.4m`, `8.2m`, or `13m` below the surface?  Does your answer make intuitive sense?

Question 7.

a.  What is the average salinity of the water at depth `2.4m`?  At depth `8.2m`?  At depth `13m`?  What about the variance of the salinity at all 3 depths?  Be sure to remove any outliers, when appropriate.

b.  At depth `13m`, make a plot of time versus salinity.

c.  As we saw in `7b`, much more data is available during the first two weeks of June, as opposed to the second two weeks of June.  Make a revised plot, showing only the time versus salinity from the start of the day on June 6, through the end of the day on June 12 (i.e., for a full 7-day period).  How many cycles of the salinity do you think you see on this plot?  Is there a natural reason for this number of cycles?

Question 8.

At depth `2.4m`, what fraction of the temperature data points are between 10 and 12?  Between 12 and 14?  Between 14 and 16?  Between 16 and 18?  Use the `tapply` function to answer all four of these questions with one line of code.

Question 9.

At depth `2.4m`, what is the average temperature between the start of the day on June 1 and the end of the day on June 7?

What is the average temperature between the start of the day on June 8 and the end of the day on June 14?

What is the average temperature between the start of the day on June 15 and the end of the day on June 21?

What is the average temperature between the start of the day on June 22 and the end of the day on June 28?

Use the `tapply` function to answer all four of these questions with one line of code.

[Note: The original problem statement had an off-by-one typographical error on some of the dates.]

Question 10.

At depth `13m`, how many data points have salinity greater than 12 and temperature greater than 14?

How many data points have salinity greater than 12 and temperature at most 14?

How many data points have salinity at most 12 and temperature greater than 14?

How many data points have salinity at most 12 and temperature at most 14?

Use the `tapply` function to answer all four of these questions with one line of code.

Hint:  You will need to embed a `list` into your `tapply`, as we did in the notes file `CO2examplecontinued.R` (the second CO2 example).

== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]

There is also some `supplemental-data.html` provided by the ASA.

You can see href="http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site] too.  In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you.  Since the data itself is so large, I saved it into a common data directory:
`/data/public/dataexpo2009/`

Notes:  If you want to read ALL of the data into `R` at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data.  You are not expected to import all of the data while you are solving the questions.  You can wait until you have solved the questions, and then come back and try to get the answers with all of the data.  So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv("/data/public/dataexpo2009/2006.csv"), read.csv("/data/public/dataexpo2009/2007.csv"), read.csv("/data/public/dataexpo2009/2008.csv") )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years.

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Just loading the data itself (if you choose all of the years) might take roughly 15 or 20 minutes to accomplish.  It would be done with some code like this:  (WARNING! This will take quite a long time to load, if you load all years at once.)

`bigDF <- rbind(
read.csv("/data/public/dataexpo2009/1987.csv"),
read.csv("/data/public/dataexpo2009/1988.csv"),
read.csv("/data/public/dataexpo2009/1989.csv"),
read.csv("/data/public/dataexpo2009/1990.csv"),
read.csv("/data/public/dataexpo2009/1991.csv"),
read.csv("/data/public/dataexpo2009/1992.csv"),
read.csv("/data/public/dataexpo2009/1993.csv"),
read.csv("/data/public/dataexpo2009/1994.csv"),
read.csv("/data/public/dataexpo2009/1995.csv"),
read.csv("/data/public/dataexpo2009/1996.csv"),
read.csv("/data/public/dataexpo2009/1997.csv"),
read.csv("/data/public/dataexpo2009/1998.csv"),
read.csv("/data/public/dataexpo2009/1999.csv"),
read.csv("/data/public/dataexpo2009/2000.csv"),
read.csv("/data/public/dataexpo2009/2001.csv"),
read.csv("/data/public/dataexpo2009/2002.csv"),
read.csv("/data/public/dataexpo2009/2003.csv"),
read.csv("/data/public/dataexpo2009/2004.csv"),
read.csv("/data/public/dataexpo2009/2005.csv"),
read.csv("/data/public/dataexpo2009/2006.csv"),
read.csv("/data/public/dataexpo2009/2007.csv"),
read.csv("/data/public/dataexpo2009/2008.csv"))`

Therefore, it is probably better (instead) to test your code on (say) three years of data, e.g., 2006-2008, before working on the full data set.

Question 1.

a. Consider the departure times (`DepTime`).  What fraction of the data are missing, i.e., are stored as `NA` values?

b. Within the departure times that are recorded (i.e., that are not `NA` values), the times are stored in `hhmm` format.  So there should be at most `24*60 = 1440` such possible times.  Are there other `DepTime` values?  Are they correct or perhaps erroneous?  How many such `DepTime` values (overall) seem to be erroneous?

Question 2.

a.  Which departure times are the best, for minimizing the arrival delay (`ArrDelay`)?  More specifically, if our goal is to minimize the arrival delay, which of these 4 time categories is best time of day for our departure?  Between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Or between 6 PM and 12 midnight?

b.  Which of the 4 time categories for the departure will have the highest variance for arrival delay?

c.  Now please solve `2a` and `2b` again, splitting the data not only by the best time of day but also by the airline too.  That way, we can know what time of day and which airline we might prefer to use.

Question 3.

a.  Which 10 airports have the most departures?

b.  Which 10 airports have the most arrivals?

c.  If we reconsider `3a` and `3b`, by splitting the data year by year, are the answers to `3a` and `3b` relatively consistent from year to year?

d.  Which are the most 10 popular pairs of departure/arrival city pairs?  (For instance, `IND-to-ORD` might be one such popular pair.)

Question 4.

a.  Which 5 airports are most likely to be on time for arrivals (on average)?

b.  Which 5 airports are most likely to be on time for departures (on average)?

c.  Which 5 airports are most likely to be delayed for arrivals (on average)?

d.  Which 5 airports are most likely to be delayed for departures (on average)?

Question 5.

a.  Which is the best day of the week to fly, if you want to minimize delayed arrivals?

b.  Which portion of the flights depart on which days?

c.  What percent of flights depart between 12 midnight and 6 AM?  Between 6 AM and 12 noon?  Between 12 noon and 6 PM?  Between 6 PM and 12 midnight?

d.  Can you study 5b and 5c simultaneously, e.g., can you give an analysis by day of the week and time of day (in tandem), so that we know precisely which days of the week and which portions of the days are busiest for departures, i.e., so that we have a finer breakdown of the departure data?

Question 6.

a.  Which 5 carriers are the most likely to be delayed?

b.  Which 5 carriers are the most likely to be on time?

Question 7.

a.  Give a month-by-month breakdown of the percentage of cancelled flights.

b.  What are the worst 3 months of the year for cancelled flights?  I.e., during which 3 months are the most flights cancelled?  (Since 1987 is an incomplete year, please avoid the data from 1987 for `7a` and `7b`, because we do not want to unfairly balance the months.)

Question 8.

Make a plot that shows how the number of flights departing `ORD` has changed, year by year.  Then add similar data to the same plot, for the number of flights departing `IND`, year by year.

Question 9.

Read the documentation for the `dotchart` function.  Make a `dotchart` as follows:  The x-axis should be the percentage of the time that flights are delayed more than 30 minutes.  On the y-axis, the main groupings should be according to month, and within each month, please show O'Hare and Indianapolis as cities of departure for flights.  The data to be displayed are the `DepDelay` data for 2007 only.  So the overall plot will show, month-by-month, a comparison of the `DepDelay` data for O'Hare and Indianapolis.

Question 10.

Make another `dotchart`, similar to the one in question `9`, where the main groupings on the y-axis are O'Hare and Indianapolis, and within each city, display all 12 months.  Again, the data to be displayed are the `DepDelay` data for 2007.  The x-axis should again be the percentage of the time that flights are delayed more than 30 minutes.  So the overall plot will show, for each of the two cities, a month-by-month comparison of the `DepDelay` data.  If you are able, you can organize the months according to their percentage of time delayed more than 30 minutes, rather than according to alphabetic order.

== Project 4

This project is about visualizing data.  It will give you some time to write ababout data visualization and to take a little break from coding.

Question 1.

Check out the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots on `Many Eyes` (please give links to each of these plots) that violate the concepts of effective data visualization that are discussed in the handouts from class (e.g., in Cleveland's book and Robbins's book, and in the paper "How to display data badly").  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine, for instance, that you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  (Your discussion of these 4 plots should be at least one single-spaced page in (say) 12 point Times font, for example... but more than 1 page is certainly allowed.)  Each student should write about at least 1 plot.

Question 2.

Revisit the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM).  Find 4 (or more) separate plots (again, with links to the plots) on `Many Eyes` that do an overall good job of effective data visualization.  Justify the reasons why you think that the plots are effective.  (Again, please write at least one page total, justifying the reasons that you think the plot is effective.)  Each student should write about at least 1 plot.

Question 3.

Check out the website http://www.informationisbeautiful.net[Information Is Beautiful].  Find 4 (or more) separate plots on `Information Is Beautiful` (please give links to each of these plots) that violate the concepts of effective data visualization.  Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved.  Imagine you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data.  Your constructive criticism should be at least 1 page altogether.

Question 4.

The http://www.gapminder.org/world[Wealth and Health of Nations] is a fun depiction of data.  On the other hand, as with many depictions of data, it violates some of the techniques of effective data display.  Please write an explanation of which techniques of effective data display are violated.  If you imagine you are writing a constructive criticism to the authors of this animation, please make suggestions for how the depiction of data (for the health and wealth, over the years displayed) could have been done more effectively.  Please make sure your explanation is at least 1 page long.

Question 5.

Describe (at least!) 4 very significant ways that the poster winner "Congestion in the sky" http://stat-computing.org/dataexpo/2009/posters/ from the `Data Expo 2009` poster competition results could be significantly improved, using the concepts of effective data visualization.  Write a constructive criticism (of at least 1 page) that gives suggestions for improvement on each aspect that you criticize.

Question 6.

For the other posters (do not use the winner, "Congestion in the sky", since it was discussed already in question `5`), find a total of at least 4 significant ways that some of the other posters can be improved.  You can analyze several different posters, that is OK.  Your constructive critique should be at least 1 page.

Question 7.

Which of the posters in the `Data Expo 2009` do you think should be the winner?  Why?  (It is OK if you choose the poster that actually won, or any of the other posters.)  Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long.

Questions 8, 9, 10.

Imagine that you are going to enter the `Data Expo 2009`.  Rather than having to organize your information into a poster, prepare 3 pages of analysis, exploring some aspects of the airline data set that are interesting to you, and which you think might be of broad interest to potential readers too.  Your discussion and plots should be at least 3 pages long.

== Project 5

The code found in the `week6.html` Week 6 examples should be helpful in this problem set.

Question 1.

Practice using the sapply function:

a. Find, with only one line (altogether) of `sapply` code, the 5 lengths of the following 5 vectors:

* the `LakeHuron` vector,
* the `waiting` vector in the `geyser` data (remember to load the `MASS` library first)
* the `duration` vector in the `geyser` data
* the `chickwts$weight` vector
* the `mtcars$mpg` vector

b. Now find the average value stored in each of the 5 vectors, using `sapply`.

c. Check that `R` did the right thing in `1b` by manually taking the mean of each vector, using 5 separate lines of code.

d.  If you accidentally use `c` instead of `list` in `1b`, `R` just takes an average of individual values, but the average of 1 value is just the value itself, so `R` returns the full list of values.  Please give this (incorrect) behavior a try, just to see how it misbehaves!

e. Now find the variance of the values stored in each of the 5 vectors, using `sapply`.

f. Check that `R` did the right thing in `1e` by manually finding the variance of each vector, using 5 separate lines of code.

g.  If you accidentally use `c` instead of `list` in `1e`, `R` just takes the variance of each individual value, but `R` gives an `NA` when taking the variance of an individual value (you can try this, e.g., `var(3.79)` gives `NA`, so `R` returns `NA` for each value).  Please give this (incorrect) behavior a try!

h.  Examine the head of the `Cars93` data.  This data set has a lot of types of columns.  Use `sapply` to find out the kinds of classes for each of the 27 columns in this `data.frame` (using just one call to `sapply`; hint: use `class` for the function).

Question 2.

a. Use the `mapply` function, with the `paste` function, and the vectors
`c("a","b","c","d","e")`
and
`c("A","B","C","D","E")`
and the parameters
`sep=""` and `USE.NAMES=FALSE`
to print these five sentences:
`[1] "The uppercase version of a is A" "The uppercase version of b is B" "The uppercase version of c is C"`
`[4] "The uppercase version of d is D" "The uppercase version of e is E"`

b. Use the `row.names` function, and the column of `population` data, both with the `state.x77` data set, as well as the `mapply` function, to print a vector of 50 sentences.  (It might be helpful to use the parameters `USE.NAMES=F` and `sep=""`.)  The vector should start with the following six sentences:
`[1] "Alabama has 3615 thousand people."        "Alaska has 365 thousand people."`
`[3] "Arizona has 2212 thousand people."        "Arkansas has 2110 thousand people."`
`[5] "California has 21198 thousand people."    "Colorado has 2541 thousand people."`

c. Revise your answer to `2a`, by actually multiplying the `population` data by 1000, so that the vector should start with the following six sentences:
`[1] "Alabama has 3615000 people."        "Alaska has 365000 people."`
`[3] "Arizona has 2212000 people."        "Arkansas has 2110000 people."`
`[5] "California has 21198000 people."    "Colorado has 2541000 people."`

Question 3.

a. Make a `data.frame` containing all of the cars from `mtcars` with `hp>100` and `8` cylinders.  Create a new `data.frame` with these cars, displaying only the `mpg`, `cyl`, `hp`, and `qsec`.

b. Make a `data.frame` containing all of the rows describing provences from the `swiss` data set with 50% or more `Catholics` and 50% or more of `males` involved in agriculture.  Within this specific `data.frame`, find the mean and standard deviation of the `Fertility` data.

c. Make a `data.frame` containing all of the rows in the `chickwts` data for which the feed is either `horsebean` or `soybean`.  What is the average weight (altogether) across these two kinds of feed?

Question 4.

Step through Dr Ward's `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data.  It takes a little time to understand completely what is happening, but essentially we are able to read data from dozens of files with ease (i.e., without having to download them individually, by hand), and to extract and assemble the data in them.  Note that the time parameter in these files is the same data we had in the earlier project, but is stored differently (and, hence, extracted differently) than in the earlier project.

a.  Use the `/notes/SATURNapplyexamples.R` `R` code for `apply` examples with `SATURN` data to extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `2.4m`.

b.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `8.2m`, from:
`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.CT/` (Beware: The starting month is not the same for this data set, compared to the previous one.)

c.  Extract the `temperature`, `electrical conductivity`, `salinity`, and `time` data from the `SATURN03` station at depth `13.0m`, from: `http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.CT` (Beware: Again, the starting month is not the same for this data set, compared to the previous two.)

Question 5.

Extract the `Phycoerythrin` and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Phycoerythrin/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Phycoerythrin/`

Question 6.

Extract the `Oxygen Concentration` (`oxygen`), `Oxygen Saturation` (`oxygensat`), and `time` data from the `SATURN03` station at depths `2.4m`, `8.2m`, and `13.0m` from:

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.240.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.820.A.Oxygen/`

`http://amb6400b.stccmop.org:8080/thredds/dodsC/preliminary_data/saturn03/saturn03.1300.R.Oxygen/`

Question 7.

a. For each of the 3 types of data listed above (in questions `4`, `5`, `6`) at each of the 3 depths, find the number of data points per month.  For instance, starting with the `temperature`/`conductivity`/`salinity` data at depth `2.4m`, find the number of data points per month.  Then do the same for `8.2m` and for `13.0m`.  Then do this again for the `Phycoerythrin` data.  Then do it again for the `Oxygen` data.  To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 of the saturn03.240.A.CT data contains 202,702 data points at depth 2.4m.`

b. Re-calculate your answer to `7a`, so that it is normalized according to the number of days in the month. In other words, get the number of data points divided by the number of days in the month. To express your answers, use the `mapply` function to print sentences that say statements like:
`Month 06 of year 2012 contains an average of 6756.733 data points per day, during the 30 day period, for a total of 202,702 data points during the month, at depth 2.4m.`

Question 8.

a.  Extract the `temperature` data from the `SATURN03` station for June 2012 at depth `2.4m`.  There are 202702 data points.  Save these in a variable called `tempdata`.  Also get the analogous 202702 `time` data points.  Save these in a variable called `temptimes`.

b.  Extract the `oxygen` saturation data (the 2nd parameter in the data set `saturn03.240.A.Oxygen`) from the `SATURN03` station for June 2012.  There are 15725 data points.  Save these in a variable called `oxydata`.  Also get the analogous 15725 time data points.  Save these in a variable called `oxytimes`.

c.  Notice that we would be hard-pressed to compare the `temperature` and `oxygen saturation` data, because there are vastly different amounts of data in the two vectors and (perhaps more importantly) they were measured at different points in time.  We can, however, build a function that predicts the behavior of the temperature data at ALL points in time, and then use it to figure out how the temperature data would have behaved, if it was measured at the same 15725 `time` points as the `oxygen saturation` data, and then we could compare the `temperature` and `oxygen saturation` data.  This can be done as follows:
`tempfunction <- approxfun(temptimes, tempdata)`
This makes `f` into a function that can predict the `temperature` behavior at any `time` we like.  Then we run the function on the `oxygen saturation` times, to see how `temperature` would have behaved at the 15725 times when `oxygen saturation` was measured:
`tempatoxygentimes <- tempfunction(oxytimes)`
Finally, we can plot the `temperature` versus the `oxygen saturation` data this way:
`plot(tempatoxygentimes,oxydata)`

Question 9.

a.  Make comparisons between some of the other variables, in the style of how we did things in question `8`.

b.  Which pair of variables (in the June 2012 data sets) seem to be the most strongly correlated?  Why do you think so?

c.  What could go wrong with the method discussed in `8` and `9`?  Hint: for instance, in part `8c`, take a look at:
`range(temptimes)`
`plot( tempfunction(seq(1338537601,1341129599,by=100) ) )`
How could we potentially fix the problem that happens when missing data occurs?  [You do not have to actually fix it; but briefly mention some way that you might fix it.]  Can you see this problem in the plot?  [We will discuss this problem more, in a set of future questions, in another project.]

Question 10.

a.  There are 7 data sets inside the directory: `/data/public/NARR/pressure`
How many variables do they each contain?

b.  How many pieces of data does the `lat` variable contain in each file?  How about the `lon` variable?  How about the `Lambert Conformal` variable?  Are all of the `lat` variables identical across all 7 files?  If so, how do you know?  If not, how are they different?  What about the `lon` variable?  What about the `Lambert Conformal` variable?

c.  What are the sizes (i.e., dimensions) of the 4th variable in each of the 7 files?  What percent of the 4th variable is missing in each of the 7 files?

d.  If you store the `time` vector from a file in a vector `t`, then the code:  `format(as.POSIXct(3600*t, origin="1800-01-01"), tz="UTC+0:00")`  will convert the time into a human-readable format.  The `3600` converts the hours into seconds, and the seconds are given in units after January 1, 1800.  (Dr Ward fiddled around with this for awhile to figure this out.)  Question:  Do all 7 files have the same time vector?

e.  What is the `time` unit between consecutive times in each of these vectors?

== Project 6

The code found in the `week8.html` Week 8 examples should be helpful in this problem set.

Question 1.

Compare the 3 variables found in the first `SATURN03` data set we studied, namely, the `saturn03.240.A.CT_2012_06_PD0.csv` data set, from depth `2.4m`.  Compare them in pairs, to see if any pair of them yields a very good linear model. In all of these cases, be sure to remove any outliers, if necessary.

a. Make a simple linear regression to try to predict the `electrical conductivity` from the `temperature`.

b. Make a simple linear regression to try to predict the 'salinity' from the 'temperature'.

c. Make a simple linear regression model to try to predict the 'electrical conductivity' from the 'salinity'.

d. Which one of these linear models seems most amenable to linear modeling?  Why?

Question 2.

a. Make a simple linear regression model to predict the `mpg` from the `mtcars` data, based on the `hp`.  Plot the two variables, along with the line suggested by a simple linear regression model.

b. Make a multiple regression model to predict the `mpg` from the `mtcars` data, based on the `hp` and the `disp`.

c. Using the multiple regression model, what kind of `mpg` might we guess that a car has, if it has 147 `hp` and 230 `disp`?

Question 3.

a. Load the 1990 airline data from the `dataexpo` into a `data.frame`.

b. Use the `subset` command to extract only the flights from June 1990.

c. Build a simple linear regression model that predicts the arrival delays from the departure delays.

d. Plot both the delays, putting the arrival delays on the y-axis and the departure delays on the x-axis.

e. Draw the line from the simple linear regression model on the plot.

f. Repeat steps `3c` through `3e`, removing the outliers, e.g., removing the flights with departure delays that are more than `500` and removing those that are less than `-50`.  I.e., restrict attention to flights with departure delays between `-50` and `500`.

Question 4.

a.  Generate 100 (continuous) uniform random numbers, uniformly distributed between 0 and 1.

b.  For each uniform random number `U` in part `a`, define `V = -log(U)/3`.  Make this transform for all 100 numbers from `4a`.

c.  Generate 100 exponential random numbers with rate `3`.

d.  Use a `qqplot` to convince yourself that the numbers from `4b` have the same kind of distribution as the numbers in `4c`.  I.e., if `U` is a continuous uniform random variable, then `-log(U)/3` is an exponential random variable with rate `3`, i.e., with mean `1/3`.

e.  Re-do parts `4a` through `4d` with millions of numbers instead of just 100 numbers, to reinforce this notion in your mind.

Question 5.

a.  Generate 1,000,000 (continuous) uniform random numbers (each between 0 and 1) and store them in a matrix `M` with 1000 rows and 1000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 1000 numbers, each of which is equal to the sum of the 1000 uniforms.  Store the result in a vector `v`.

c.  Subtract 500 from each entry of `v` and then (afterwards) divide each number by `sqrt(1000/12)`, i.e., by `9.1287`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Question 6.

a.  Generate 100,000,000 exponential random numbers, each with `rate = 5`, and store them in a matrix `M` with 10000 rows and 10000 columns.

b.  Use the `apply` function to sum each row of `M`.  So we get 10000 numbers, each of which is equal to the sum of the 10000 exponential random numbers.  Store the result in a vector `v`.

c.  Subtract 2000 from each entry of `v` and then (afterwards) divide each number by `sqrt(10000/5^2) = 100/5 = 20`.  Store the result in a new vector `w`.

d. Use a `qqplot` to convince yourself that the entries of `w` are approximately standard normal random numbers, i.e., normal random numbers with mean 0 and standard deviation 1.

Question 7.

a.  Use the built-in `R` data set for "Pharmacokinetics of Theophylline" (stored in `Theoph`) to build a multiple linear regression model of the concentration, based on the weight, dose, and time.

b.  If a person weighed `66 kg`, and received a dose of `4 mg/kg`, and it has been 6 hours since the dose was administered, what is the predicted level of concentration?

Question 8.

Look at the departure delays from the June 1990 flights.  If we restrict attention to departure delays of 30 minutes or more, what kind of distribution do you think the data has?  Normal?  Uniform?  Exponential?  Justify your answer with a `qqplot`.  How closely can you estimate the parameter(s) of the distribution you think that this data has?

== Project 7

The following resources might be helpful for you:

* Dr Ward's notes: `week9.txt` on Introduction and immersion into the UNIX Operating System and the bash shell
* http://en.wikipedia.org/wiki/List_of_Unix_programs[List of Unix utilities (Wikipedia)]
* http://linuxcommand.org/tlcl.php[The Linux Command Line] official direct link for http://sourceforge.net/projects/linuxcommand/files/TLCL/13.07/TLCL-13.07.pdf[pdf download] of the book by William Shotts (No Starch Press, 2012)

Question 1.

a.  How many lines are found in the file `/etc/passwd`?

b.  Remember (from the notes) that the command: `cat /etc/passwd | cut -f5 -d:` is used to find each person's name who is a user in the system.  Instead of printing the full names of the users, print their usernames (e.g., `mdw`).

Question 2.

a.  How many users have her/his directory in the `/home` filesystem?  (This is described in the 6th of the 7 fields on each line.)

b.  Now extract the first names of each such user who has her/his directory in the `/home` filesystem.

c.  Save the results into a file in your home directory, named `firstnames.txt`

Question 3.

a.  How many words in the file `/usr/share/dict/words` contain the letter `q`?

b.  Convince yourself that the command:  `awk '{print length}'` will print the length of the words in a file.  [Feb 2023 note: I think this should say "length of the lines in a file".]

c.  Find the length of the words, line by line, in the file `/usr/share/dict/words`

Question 4.

a.  What is the longest word length, among all word lengths in the file `/usr/share/dict/words`? Hint: you might need to use `awk '{print length}'` and `sort`, with a certain flag on the sort command.  Such a flag comes in handy when you are sorting text that is numeric.  It would be helpful to read the manual for the sort command, to see which flag to use.

b.  Instead of looking for the longest word length, after you sort the word lengths numerically, pipe the output to the `uniq` command, and use a flag on the `uniq` command to count the number of words of each length.

Question 5.

In the directory `/data/public/election2008` there is some data related to the 2008 election.  There are 49 files, namely, one for each of the 48 mainland states, and one for `DC`.  Each line has seven pieces of data, namely, the percent and number of people who voted for Obama in 2008, the percent and number who voted for McCain in 2008, the overall percent of registered votes, the state, and the county.

a.  How many counties are represented in these 49 files?

b.  What is the largest number of votes in one county for Obama?

c.  What is the largest number of votes in one county for McCain?

d.  How many counties have one or more of the following words in the title:  `north`, `east`, `south`, `west`?

e.  How many characters are found in the longest county name?

Question 6.

In the directory `/data/public/dataexpo2009` there is the airline flight data, which we are already familiar with.

a.  How many flights were taken in 2006?

b.  How many flights were taken altogether, from 1987 to 2008?

c.  How many flights had `IND` as the `Origin` city in 2006?

d.  How many flights had `IND` as the `Origin` city altogether, from 1987 to 2008?

Question 7.

a.  In the airline data, how many unique carriers are there, in the 2006 data set?

b.  In the airline data, how many unique carriers are there altogether, from 1987 to 2008?

c.  What was the longest flight taken in 2006, in terms of miles?

d.  What was the longest flight taken altogether, from 1987 to 2008, in terms of miles?

e.  How many flights had this longest flight distance (in terms of miles), from 1987 to 2008?

Question 8.

Most of the `UNIX` commands you can access are contained in one of three places, namely:

* `/usr/local/bin`
* `/bin`
* `/usr/bin`

Print a list of the names of the programs (be sure to check programs in all three of these places) that have the word `zip` somewhere in the title of the program.

== Project 8





