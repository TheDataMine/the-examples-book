= STAT-LLC Fall 2018 STAT 29000 Projects

== Project 1

Question 1.

Use the airline data stored in this directory:

`/depot/statclass/data/dataexpo2009`

In the year 2005, find:

a.  the number of flights that occurred, on every day of the year, and

b.  find the day of the year on which the most flights occur.

Soution:

We switch to the directory for the airline data

`cd /depot/statclass/data/dataexpo2009`

a. The number of flights that occurred, on every day of the year, can be obtained by extracting the 1st, 2nd, and 3rd fields, sorting the data, and then summarizing the data using the uniq command with the `-c` flag

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c`

The first few lines of the output are:

[source,bash]
----
16477 2005,10,1
19885 2005,10,10
19515 2005,10,11
19701 2005,10,12
19883 2005,10,13
----

and the last few lines of the output are:

[source,bash]
----
20051 2005,9,6
19629 2005,9,7
19968 2005,9,8
19938 2005,9,9
    1 Year,Month,DayofMonth
----

b. The day of the year on which the most flights occur can be found by sorting the results above, in numerical order, using `sort -n` and then (if desired, although it is optional) we can extract the last line of the output using `tail -n1`

`sort 2005.csv | cut -d, -f1-3 | sort | uniq -c | sort -n | tail -n1`

and we conclude that the most flights occur on August 5:

`21041 2005,8,5`


Question 2.

Again considering the year 2005, did United or Delta have more flights?

Solution:

We can extract the 9th field, which is the carrier (i.e., the airline company) and then, in the same way as above, we can sort the data, and then we can summarize the data using `uniq -c`

This yields the number of flights for each carrier.

We can either read the number of United or Delta flights with our eyeballs, or we can use the grep command, searching for both the pattern UA and DL to isolate (only) the number of flights for United and Delta, respectively.

`sort 2005.csv | cut -d, -f9 | sort | uniq -c | grep "UA\|DL"`

The output is:

[source,bash]
----
658302 DL
485918 UA
----

so Delta has more flights than United in 2005.


Question 3.

Consider the June 2017 taxi cab data, which is located in this folder:

`/depot/statclass/data/taxi2018`

What is the distribution of the number of passengers in the taxi cab rides?  In other words, make a list of the number of rides that have 1 passenger; that have 2 passengers; etc.

Solution:

Now we change directories to consider the taxi cab data

`cd ../taxi2018`

The `..` in the previous command just indicates that we want to go up one level to `/depot/statclass/data` and then, from that point, we want to go into the taxi cab directory.  If this sounds complicated, then (instead) it is safe to use the longer version:

`cd /depot/statclass/data/taxi2018`

The number of passengers is given in the 4th column, `passenger_count`

We use a method that is similar to the one from the first three questions, we extract the 4th column, sort the data, and then summarizing the data using the uniq command with the -c flag

`sort yellow_tripdata_2017-06.csv | cut -d, -f4 | sort | uniq -c`

and the distribution of the number of passengers is:

[source,bash]
----
      1 
    548 0
6933189 1
1385066 2
 406162 3
 187979 4
 455753 5
 288220 6
     26 7
     30 8
     20 9
      1 passenger_count
----

Notice that we have some extraneous information, i.e., there is one blank line and also one line for the passenger_count (from the header)



Question 4.

Revisit question 1a, but using all of the data about all of the flights, from 1987 to 2008.  (Note: It is not necessary to run 22 separate commands for this purpose.)

Solution:

To revisit question 1a, we first change back to the airline directory using

`cd ../dataexpo2009`

or using

`cd /depot/statclass/data/dataexpo2009`

Then we perform the same operations as above, but working on the file `allyears.csv`

`sort allyears.csv | cut -d, -f1-3 | sort | uniq -c`

An alternative, we could work on the collection of all of the files whose name starts with a 1 or a 2 We do not just use `*.csv` because in this method, we do not want to work on the file `allnames.csv` in this method (that would double our answers)

`sort [1-2]*.csv | cut -d, -f1-3 | sort | uniq -c`

The first few lines of the output, with either method, are:

[source,bash]
----
14766 1987,10,1
13421 1987,10,10
14020 1987,10,11
14795 1987,10,12
14865 1987,10,13
----



Question 5.

a.  Give a distribution of the number of flights in the ASA Data Expo 2009 according to the day of the week.

b.  Which day of the week is the most popular for travel?

c.  Which day of the week is least popular for travel?

Solution:

a. To find the most popular day of the week for travel, we extract the 4th field, which contains the DayOfWeek, and we summarize it, in the same way that we did above. Either of these two methods will work:

`sort allyears.csv | cut -d, -f4 | sort | uniq -c | sort -n`

or we can compute:

`sort [1-2]*.csv | cut -d, -f4 | sort | uniq -c | sort -n`

We get the following distribution of the days of the week:

[source,bash]
----
15915382 6
17143178 7
18061938 2
18083800 4
18091338 5
18103222 3
18136111 1
       1 DayOfWeek
----

b. We conclude that Monday (day 1) is the most popular for travel,

c. and Saturday (day 6) is the least popular for travel.


Question 6.

a.  Create a text file call `INDtoORD.txt` that contains the data about every flight from Indianapolis (IND) to Chicago O'Hare (ORD).

b.  Zip this data into a compressed file called `INDtoORD.zip`

Solution:

a. We can build such a file and store it in our home directory (please note that the tilde ~ refers to the home directory) as follows:

`grep "IND,ORD" allyears.csv >~/INDtoORD.txt`

or, alternatively, like this:

`grep IND,ORD [1-2]*.csv >~/INDtoORD.txt`

The first method yields a file that starts as follows:

[source,bash]
----
1987,10,31,6,1720,1721,1712,1714,UA,334,NA,52,53,NA,-2,-1,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,1,4,816,816,920,909,UA,453,NA,64,53,NA,11,0,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,2,5,816,816,921,909,UA,453,NA,65,53,NA,12,0,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,3,6,833,816,932,909,UA,453,NA,59,53,NA,23,17,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987,10,4,7,814,816,906,909,UA,453,NA,52,53,NA,-3,-2,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
----

The second method yields a file that starts as follows: (the extra characters at the start of each line show the file where the pattern was found)

[source,bash]
----
1987.csv:1987,10,31,6,1720,1721,1712,1714,UA,334,NA,52,53,NA,-2,-1,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987.csv:1987,10,1,4,816,816,920,909,UA,453,NA,64,53,NA,11,0,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987.csv:1987,10,2,5,816,816,921,909,UA,453,NA,65,53,NA,12,0,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987.csv:1987,10,3,6,833,816,932,909,UA,453,NA,59,53,NA,23,17,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
1987.csv:1987,10,4,7,814,816,906,909,UA,453,NA,52,53,NA,-3,-2,IND,ORD,177,NA,NA,0,NA,0,NA,NA,NA,NA,NA
----

b. Now we zip the file (again the tilde ~ just refers to the fact that these files are both in the home directory)

`zip ~/INDtoORD.zip ~/INDtoORD.txt`


Question 7.

a.  Identify the 10 airports in the ASA Data Expo 2009 that are the busiest, according to the number of departures (i.e., according to serving as the origin airport for flights).

b.  Use the grep command (with multiple patterns) to store the complete data (all 29 parameters) about these 10 airports, into a file called `popularairportdata.txt`.

Solution:

a. We identify the airports by extracting the 17th field and then sorting and counting the lines, and then sorting the results, as we did several times in the previous problems.  The tail contains the 10 most popular airports.

`sort allyears.csv | cut -d, -f17 | sort | uniq -c | sort -n | tail`

[source,bash]
----
2733910 SFO
2754997 MSP
2884518 IAH
2979158 DTW
3319905 DEN
3491077 PHX
4089012 LAX
5710980 DFW
6100953 ATL
6597442 ORD
----

b. We can grep for any line that contains data about these 10 airports:

`grep "SFO\|MSP\|IAH\|DTW\|DEN\|PHX\|LAX\|DFW\|ATL\|ORD" allyears.csv >~/popularairportdata.txt`


Question 8.

How many distinct airports are represented in the ASA Data Expo?

Solution:

One method is to put the 17th and 18th fields in a file with two commands. Please note that we used a double right carrot so that the output gets appended to the file without deleting the file

[source,bash]
----
cat allyears.csv | cut -d, -f17 >>~/airportcodes.txt
cat allyears.csv | cut -d, -f18 >>~/airportcodes.txt
----

and then we could sort the file and find the number of unique airport codes.

The `wc` command counts the number of lines, words, and characters but here we use wc -l because we only need the number of lines (one line per airport)

`sort ~/airportcodes.txt | uniq | wc -l`

There are 354 airports altogether.

An alternative, one line method is to enable the cut command to print a newline after each field, so that we can print the 17th field and then a newline followed by the 18th field and then a newline. That way, we still get one airport per line.

`cat allyears.csv | cut -d, -f17-18 --output-delimiter=$'\n' | sort | uniq | wc -l`

Again we see that there are 354 airports altogether.



Question 9.

a.  Revisit question 3, but using all of the data about all of the taxi cab rides, from 2009 to 2017.  (Note: It is not necessary to run dozens of separate commands for this purpose.)

b.  Do you notice anything unusual about this data?

Solution:

a. We change directories to consider the taxi cab data

`cd ../taxi2018`

We extract the 4th field across all files now, using the similar method to the one that we used above. Since this operation takes a long time to run, we store the results in a file. We also use the `nohup` option, which needs an ampersand (the is the "&") at the end of the line, so that this process can run in the background while we are working on other things.

`nohup cat yellow*.csv | cut -d, -f4 | sort | uniq -c | sort -n >~/taxidistribution.txt &`

and the distribution of the number of passengers includes (among many others):

[source,bash]
----
      974 8
     1165 7
     1495 208
  3800950 0
 29462601 4
 32640245 6
 60751390 3
 92684661 5
205576933 2
971920078 1
----

b. Some of the taxi cab rides seem to have a very large number of passengers, but this is a result of some errors in the data set.  For instance, we see taxi cab rides here that have 208 passengers, which is impossible.



Question 10.

a.  Find the number of taxi cab rides per day in June 2017.  (Use the date when the cabs depart, in case the trip lasts past midnight.)  Hint:  You might need to use *two* cut commands, since you will need to extract the data about the day from the timestamp.  The exact time of departure is given, but the hours, minutes, and seconds are not needed for this question, and must be avoided.

b.  Same question, but use all of the data about all of the taxi cab rides, from 2009 to 2017.

Solution:

a. We first extract the 2nd field, which has the date and time of the departure of each trip:

`head yellow_tripdata_2017-06.csv | cut -d, -f2`

Then we need to run this through the cut command again, this time using a space as the delimiter: and (since this intermediate result had only two fields, namely, the date and the time), this time we extract the first field.

`head yellow_tripdata_2017-06.csv | cut -d, -f2 | cut -d' ' -f1`

and (changing head to cat, so that we examine the entire file) now we are prepared to use this strategy on the full month of taxi cab rides:

`cat yellow_tripdata_2017-06.csv | cut -d, -f2 | cut -d' ' -f1 | sort | uniq -c`

The first few lines of output are the following (notice that we had one blank line)

[source,bash]
----
     1 
344507 2017-06-01
347404 2017-06-02
341807 2017-06-03
294236 2017-06-04
----

and the last few lines of output are the following (notice that we have the header in the output too)

[source,bash]
----
321083 2017-06-27
316000 2017-06-28
313277 2017-06-29
302847 2017-06-30
     1 tpep_pickup_datetime
----

b. Now we run this code on all of the taxi cab rides from all of the years, and we again use the "nohup" option, again with an ampersand at the end of the line, so that this process can run in the background while we are working on other things. We store the results in a file:

`nohup cat yellow*.csv | cut -d, -f2 | cut -d' ' -f1 | sort | uniq -c | sort -n >~/taxicountsbyday.txt &`

As with lots of data, there are some strange properties.  For instance, here are the first several lines of the file:

`head -n30 ~/taxicountsbyday.txt`

[source,bash]
----
     1 2001-01-06
     1 2002-12-31
     1 2018-01-20
     1 2018-02-04
     1 2041-11-15
     1 2053-03-21
     2 2001-01-01
     2 2003-01-01
     2 2018-02-07
     2 2018-03-01
     2 2018-03-19
     3 2003-01-14
     3 2018-01-17
     3 2018-02-25
     4 2018-04-30
     4 2018-05-22
     5 2018-04-09
    12 
    12 Trip_Pickup_DateTime
    26 2018-01-01
    35 2008-12-31
    36 tpep_pickup_datetime
    46 pickup_datetime
    82 
 29028 2011-08-28
 72025 2010-12-27
 78133 2016-01-23
100325 2017-03-14
113808 2012-10-29
135500 2015-01-27
----

This is the end of the first project!


== Project 2

Question 1.

a.  What was the average arrival delay (in minutes) for flights in 2005?

b.  What was the average departure delay (in minutes) for flights in 2005?

cd. Now revise your solution to 1ab, to account for the delays (of both types) in the full set of data, across all years.

Solution:

`cd /depot/statclass/data/dataexpo2009`

a.

basic solution:

`cat 2005.csv | awk -F, '{arrdelay += $15; flightcount++} END{print arrdelay/flightcount}'`

advanced solution:

`cat 2005.csv | grep -v ArrDelay | awk -F, '{if ($15!="NA") {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

basic solution: 7.03274; advanced solution: 7.18134

b.

basic solution:

`cat 2005.csv | awk -F, '{depdelay += $16; flightcount++} END{print depdelay/flightcount}'`

advanced solution:

`cat 2005.csv | grep -v DepDelay | awk -F, '{if ($16!="NA") {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

basic solution: 8.51186; advanced solution: 8.67431
	
c.

basic solution:

`cat [1-2]*.csv | awk -F, '{arrdelay += $15; flightcount++} END{print arrdelay/flightcount}'`

advanced solution:

`cat [1-2]*.csv | grep -v ArrDelay | awk -F, '{if ($15!="NA") {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

basic solution: 6.90229; advanced solution: 7.04996

d.

basic solution:

`cat [1-2]*.csv | awk -F, '{depdelay += $16; flightcount++} END{print depdelay/flightcount}'`

advanced solution:

`cat [1-2]*.csv | grep -v DepDelay | awk -F, '{if ($16!="NA") {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

basic solution: 8.01844; advanced solution: 8.17071

Question 2.

Revise your solutions to 1abcd to only include flights that took place on the weekends.

Solution:

a.

basic solution:

`cat 2005.csv | awk -F, '{if (($4 == 6) || ($4 == 7)) {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

advanced solution:

`cat 2005.csv | grep -v ArrDelay | awk -F, '{if ((($4 == 6) || ($4 == 7)) && ($15!="NA")) {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

basic solution: 4.84079; advanced solution: 4.93905

b.

basic solution:

`cat 2005.csv | awk -F, '{if (($4 == 6) || ($4 == 7)) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

advanced solution:

`cat 2005.csv | grep -v DepDelay | awk -F, '{if ((($4 == 6) || ($4 == 7)) && ($16!="NA")) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

basic solution: 7.51425; advanced solution: 7.65198

c.

basic solution:

`cat [1-2]*.csv | awk -F, '{if (($4 == 6) || ($4 == 7)) {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

advanced solution:

`cat [1-2]*.csv | grep -v ArrDelay | awk -F, '{if ((($4 == 6) || ($4 == 7)) && ($15!="NA")) {arrdelay += $15; flightcount++}} END{print arrdelay/flightcount}'`

basic solution: 5.30331; advanced solution: 5.4005

d.

basic solution:

`cat [1-2]*.csv | awk -F, '{if (($4 == 6) || ($4 == 7)) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

advanced solution:

`cat [1-2]*.csv | grep -v DepDelay | awk -F, '{if ((($4 == 6) || ($4 == 7)) && ($16!="NA")) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

basic solution: 7.55609; advanced solution: 7.677

Question 3.

a.   What is the average departure delay on flights from IND to ORD?

b.   Double-check your work, by analyzing the file created in question 6ab in project 1.

Solution:

a.

basic solution:

`cat [1-2]*.csv | awk -F, '{if (($17 == "IND") && ($18 == "ORD")) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

advanced solution:

`cat [1-2]*.csv | grep -v DepDelay | awk -F, '{if ((($17 == "IND") && ($18 == "ORD")) && ($16!="NA")) {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

basic solution: 8.85475; advanced solution: 9.13213

b.

basic solution:

`cat ~/INDtoORD.txt | awk -F, '{depdelay += $16; flightcount++} END{print depdelay/flightcount}'`

advanced solution:

`cat ~/INDtoORD.txt | grep -v DepDelay | awk -F, '{if ($16!="NA") {depdelay += $16; flightcount++}} END{print depdelay/flightcount}'`

`cd /depot/statclass/data/taxi2018`



Question 4.

a.   What is the average distance of a taxi cab ride in New York City in June 2017?

b.   Now revise your solution to 4a, to account for the full set of data, across all years.

Hint:  On problems that will take a very long time to run, like 4b, you can use the following method:

`nohup  allofyourusualcommandstuffgoeshere  >~/myoutputfile.txt &`

The `nohup` causes the program to keep running, even if you log out.
The ampersand lets you keep typing in the meantime.
The file called `myoutputfile.txt` will be saved in your home directory.
(The tilde stands for your home directory.  You can choose another location if you prefer, of course.)
You get a job number when you start a command running like this.  For instance, you job number might be 13788.
You can stop that job running at any point during its execution by typing, for instance, kill 13788

Solution:

a.

basic solution:

`cat yellow_tripdata_2017-06.csv | awk -F, '{distance += $5; taxicount++} END{print distance/taxicount}'`

advanced solution:

`cat yellow_tripdata_2017-06.csv | grep -v "istance" | awk -F, '{if (NF >= 3) {distance += $5; taxicount++}} END{print distance/taxicount}'`

solution: 2.97862

b.

basic solution:

`nohup cat yellow*.csv | awk -F, '{distance += $5; taxicount++} END{print distance/taxicount}' >~/newnewresult4a.txt &`

advanced solution:

`nohup cat yellow*.csv | grep -v "istance" | awk -F, '{if (NF >= 3) {distance += $5; taxicount++}} END{print distance/taxicount}' >~/newnewresult4b.txt &`

solution: 5.12591



Question 5.

For each taxi cab ride, compute the percentage of the ride's fare that is dedicated to the tolls.  What is this percentage, on average?

Solution:

[source,bash]
----
for year in {2009..2014}; do
  for month in {01..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v olls | awk -F, '{if((NF > 3)&&($18!=0)) {percentage += $17/$18; counter++}} END{print percentage/counter, counter}' >~/taxitolls${year}-month${month}result.txt &
  done
done

for year in {2015..2015}; do
  for month in {01..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v olls | awk -F, '{if((NF > 3)&&($19!=0)) {percentage += $17/$19; counter++}} END{print percentage/counter, counter}' >~/taxitolls${year}-month${month}result.txt &
  done
done

for year in {2016..2016}; do
  for month in {01..06}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v olls | awk -F, '{if((NF > 3)&&($19!=0)) {percentage += $17/$19; counter++}} END{print percentage/counter, counter}' >~/taxitolls${year}-month${month}result.txt &
  done
done

for year in {2016..2016}; do
  for month in {07..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v olls | awk -F, '{if((NF > 3)&&($17!=0)) {percentage += $15/$17; counter++}} END{print percentage/counter, counter}' >~/taxitolls${year}-month${month}result.txt &
  done
done

for year in {2017..2017}; do
  for month in {01..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v olls | awk -F, '{if((NF > 3)&&($17!=0)) {percentage += $15/$17; counter++}} END{print percentage/counter, counter}' >~/taxitolls${year}-month${month}result.txt &
  done
done
----

once all of those are done running, then tabulate the data

`cat ~/taxitolls20*.txt | awk '{total += $1*$2; counter += $2} END{print total/counter}'`

solution: 0.00542058



Question 6.

Consider customers who pay for their taxi cabs with credit card versus with cash.  Does this distinction affect the distance traveled?

Solution:

[source,bash]
----
for year in {2009..2015}; do
  for month in {01..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v ayment | awk -F, '{if(NF > 3) {totaldistance[$12] += $5; counter[$12]++;}} END{for (key in totaldistance) {print key, totaldistance[key], counter[key];}}' >~/taxipaymenttypes${year}-month${month}result.txt &
  done
done

for year in {2016..2016}; do
  for month in {01..06}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v ayment | awk -F, '{if(NF > 3) {totaldistance[$12] += $5; counter[$12]++;}} END{for (key in totaldistance) {print key, totaldistance[key], counter[key];}}' >~/taxipaymenttypes${year}-month${month}result.txt &
  done
done

for year in {2016..2016}; do
  for month in {07..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v ayment | awk -F, '{if(NF > 3) {totaldistance[$10] += $5; counter[$10]++;}} END{for (key in totaldistance) {print key, totaldistance[key], counter[key];}}' >~/taxipaymenttypes${year}-month${month}result.txt &
  done
done

for year in {2017..2017}; do
  for month in {01..12}; do
    nohup cat yellow_tripdata_${year}-${month}.csv | grep -v ayment | awk -F, '{if(NF > 3) {totaldistance[$10] += $5; counter[$10]++;}} END{for (key in totaldistance) {print key, totaldistance[key], counter[key];}}' >~/taxipaymenttypes${year}-month${month}result.txt &
  done
done

cat ~/taxipaymenttypes*.txt | awk '{if(!(($1 > 40)&&($1 < 42))) {print $0}}' | awk '{totaldistance[$1] += $2; totalcount[$1] += $3;} END{for (key in totaldistance) {print key, totaldistance[key], totalcount[key], totaldistance[key]/totalcount[key];}}'
----


Question 7.

a.  Use the method from the end of the notes, to add up the total number of miles flow by each airline, in 2005.

b.  Now revise your solution to 7a, to account for the full set of data, across all years.

Solution:

a.

`cd /depot/statclass/data/dataexpo2009`

`cat 2005.csv | grep -v UniqueCarrier | awk -F, '{distance[$9] += $19} END{for (key in distance) {print key, distance[key]}}'`

[source,bash]
----
AA 722852274
DL 564268170
XE 213291464
MQ 211059565
OO 195573943
CO 334136379
B6 150758132
FL 127526045
AS 138031164
TZ 47556475
HA 28642799
UA 532116122
OH 170772547
HP 200373825
NW 365044151
WN 628492880
F9 48562429
DH 60671620
EV 139250724
US 288955635
----

b.

`cat [1-2]*.csv | grep -v UniqueCarrier | awk -F, '{distance[$9] += $19} END{for (key in distance) {print key, distance[key]}}'`

[source,bash]
----
AA 14237240059
XE 1261704518
DL 11782682821
OO 1199143412
MQ 1446828218
PA (1) 213910356
TW 2733374003
B6 970096179
FL 843208347
AQ 52022302
CO 7290881290
ML (1) 47795815
EA 557435834
TZ 239451257
AS 2138434915
YV 339860468
HA 161922365
UA 12185717876
PS 30274790
OH 687290174
9E 235073027
NW 7301968497
HP 2735172637
WN 8085268722
F9 299595575
DH 259805885
PI 331802193
EV 764868753
US 8109732855
----


Question 8.

Repeat question 7ab but using the tail number (which is unique to each airplane) instead of the airline.

Solution:

a. Notice that we choose to print the distances and then the tailnums and we sort by the distances, and we are only printing the tail

`cat 2005.csv | grep -v UniqueCarrier | awk -F, '{distance[$11] += $19} END{for (key in distance) {print distance[key], key}}' | sort -n | tail`

[source,bash]
----
2034981 N211UA
2036892 N588JB
2039066 N590JB
2045220 N589JB
2045625 N593JB
2050927 N213UA
2060375 N598JB
2069280 N550JB
7597848 000000
37522198 0
----

b.

`cat [1-2]*.csv | grep -v UniqueCarrier | awk -F, '{distance[$11] += $19} END{for (key in distance) {print distance[key], key}}' | sort -n | tail`

[source,bash]
----
23882022 N550UA
23886947 N552UA
23893505 N543UA
24073793 N551UA
48624558 000000
81199937 
119055508 ï¿½NKNO
184998252 0
387812613 UNKNOW
23723559710 NA
----


Question 9.

Revisit question 4, breaking the results down according to the number of passengers.  Here is a basic outline of how to do this:

a.  Use the method from the end of the notes, to add up the total distance across all taxi rides, according to the number of passengers.

b.  Now augment the previous awk program to also include the total number of taxi rides, according to the number of passengers.

c.  Now add another feature:  At the end of the awk program, divide the total distance across all taxi rides (according to the number of passengers) by the corresponding total number of taxi rides (again, according to that same number of passengers).  With this method, at the end of the awk program, you can print the average distance per taxi ride, according to the number of passengers.

Solution:

`cd /depot/statclass/data/taxi2018`

abc (June 2017 only).

basic solution:

`cat yellow_tripdata_2017-06.csv | awk -F, '{distance[$4] += $5; taxicount[$4]++} END{ for (key in distance) {print key, distance[key]/taxicount[key]}}' | sort -n`

[source,bash]
----
 0
0 0.417445
passenger_count 0
1 2.92293
2 3.16181
3 3.09691
4 3.19115
5 3.03522
6 3.04735
7 3.54154
8 5.56933
9 5.4615
----

advanced solution:

`cat yellow_tripdata_2017-06.csv | grep -v "istance" | awk -F, '{if (NF >= 3) {distance[$4] += $5; taxicount[$4]++}} END{ for (key in distance) {print key, distance[key]/taxicount[key]}}' | sort -n`

[source,bash]
----
0 0.417445
1 2.92293
2 3.16181
3 3.09691
4 3.19115
5 3.03522
6 3.04735
7 3.54154
8 5.56933
9 5.4615
----

abc (all years).

basic solution:

`nohup cat yellow*.csv | awk -F, '{distance[$4] += $5; taxicount[$4]++} END{ for (key in distance) {print key, distance[key]/taxicount[key]}}' | sort -n >~/newnewresult9a.txt &`

[source,bash]
----
 0
0 2.30545
 passenger_count 0
passenger_count 0
Passenger_Count 0
1 5.52209
2 4.91924
3 4.88274
4 4.25553
5 2.74503
6 2.9605
7 3.30694
8 4.75099
9 5.90511
10 0.318235
13 12.89
15 2.21
17 9.18
19 0.69
25 0.87
33 1.615
34 3.57
36 20.16
37 2.16
38 1.45
47 2.56
49 0
51 2.65
53 1.86
58 5.815
61 8.78
65 7.51333
66 4.5
69 1.28
70 3.06
84 13.98
91 9.44
97 1.87
113 0
125 3.83
129 1.59
133 3.05
134 20.92
137 17.64
141 3.81
155 16.53
158 1.57
160 1.48
163 3.03
164 22.98
165 1.44
169 14.76
177 1.34
192 1.07
193 1.74
208 0.0582838
211 0.97
213 0
223 1.16
225 4.83
229 3.27
232 722862
247 3.31
249 1.69
250 3.64333
254 1.02
255 2.632
----

advanced solution:

`nohup cat yellow*.csv | grep -v "istance" | awk -F, '{if (NF >= 3) {distance[$4] += $5; taxicount[$4]++}} END{ for (key in distance) {print key, distance[key]/taxicount[key]}}' | sort -n >~/newnewresult9b.txt &`

[source,bash]
----
0 2.30545
1 5.52209
2 4.91924
3 4.88274
4 4.25553
5 2.74503
6 2.9605
7 3.30694
8 4.75099
9 5.90511
10 0.318235
13 12.89
15 2.21
17 9.18
19 0.69
25 0.87
33 1.615
34 3.57
36 20.16
37 2.16
38 1.45
47 2.56
49 0
51 2.65
53 1.86
58 5.815
61 8.78
65 7.51333
66 4.5
69 1.28
70 3.06
84 13.98
91 9.44
97 1.87
113 0
125 3.83
129 1.59
133 3.05
134 20.92
137 17.64
141 3.81
155 16.53
158 1.57
160 1.48
163 3.03
164 22.98
165 1.44
169 14.76
177 1.34
192 1.07
193 1.74
208 0.0582838
211 0.97
213 0
223 1.16
225 4.83
229 3.27
232 722862
247 3.31
249 1.69
250 3.64333
254 1.02
255 2.632
----




Question 10.

a. Find the number of taxi cab rides on each day in June 2017.

Hint:  You can use two delimiters like this:

`awk -F[,\ ]`

(the backslash is before the space to ensure that the space is detected)

b.  Does your answer to 10a agree with your answer to 10a from the previous problem set?

Solution:

10.

`cat yellow_tripdata_2017-06.csv | awk -F[,\ ] '{print $2}' | sort -n | uniq -c`

[source,bash]
----
      1 
      1 tpep_pickup_datetime
 344507 2017-06-01
 347404 2017-06-02
 341807 2017-06-03
 294236 2017-06-04
 304042 2017-06-05
 341499 2017-06-06
 339808 2017-06-07
 353452 2017-06-08
 342240 2017-06-09
 337959 2017-06-10
 283088 2017-06-11
 311495 2017-06-12
 333931 2017-06-13
 349305 2017-06-14
 347838 2017-06-15
 341823 2017-06-16
 318478 2017-06-17
 277743 2017-06-18
 306068 2017-06-19
 318727 2017-06-20
 331000 2017-06-21
 338890 2017-06-22
 341160 2017-06-23
 317617 2017-06-24
 248929 2017-06-25
 290740 2017-06-26
 321083 2017-06-27
 316000 2017-06-28
 313277 2017-06-29
 302847 2017-06-30
----


== Project 3

Use R to revisit these questions.  They can each be accomplished with 1 line of code.

Question 1.

As in Project 1, question 2:  In the year 2005, did United or Delta have more flights?

Question 2.

As in Project 2, question 2a:  Restricting attention to weekends (only), what was the average arrival delay (in minutes) for flights in 2005?

Question 3.

As in Project 1, question 3:  In June 2017, what is the distribution of the number of passengers in the taxi cab rides?

Question 4.

As in Project 2, question 4a:   What is the average distance of a taxi cab ride in New York City in June 2017?

Question 5.

Use the tapply function to find the following:

a.  The average number of passengers (on taxi cabs rides) for each day of June 2017.

b.  The average distance (on taxi cabs rides) for each day of June 2017.

c.  The average distance (on airplane flights) for each day of 2005.

d.  The average arrival delay (on airplane flights) for each day of 2005.

In the following questions, we use our UNIX knowledge to help extract some of the data, because we do not want to import all of the data into R.  We want to start combining some of our knowledge about various tools.

Question 6.

Use UNIX to make a new file that contains the departure delays of the flights from `IND` to `ORD`. What is the average departure delay on flights from IND to ORD?  Double-check your work, by analyzing the file created in question 6ab in Project 1, and by comparing to your awk solution in question 3 in Project 2.

Question 7.

a.  Use UNIX to make a new file that contains the distances of every taxi cab ride in the New York City yellow cab files (across all months and years).  It should have 1 distance per line.

b.  What is the mean and standard deviation of the distance of these taxi cab rides?

Question 8.

a.  Use UNIX to extract the information about how many flights (across all years) occur with each airline (i.e., with each `UniqueCarrier`).  You can tabulate these results in UNIX or in R.

b.  Make a dotchart in R that displays this data.

Question 9.

Use the data about the airports available from the http://stat-computing.org/dataexpo/2009/supplemental-data.html[supplemental data] of the ASA DataExpo 2009 to make a map of the contiguous portion of the United States, displaying all of the airports.

Question 10.

Make an analogous map of Indiana with its airports.


== Project 4

Use R to revisit these questions.  They can each be accomplished with 1 line of code.

Question 1.

Find all of the (origin) airports from which you can fly to 100 or more (distinct) destinations.  (Either from 2005 or from all years.)

Hint:   R has a `unique` command.

Hint 2:  `sapply(mydata, myfunction)`

can be used to apply `myfunction` to each element of `mydata`

Hint 3:  ALTERNATIVELY, it is possible to custom build functions, for example, we can get the `mean` of the `DepDelay` for each `Origin` airport this way:

`tapply(myDF$DepDelay, myDF$Origin, mean, na.rm=T)`

but we *cannot* get 3 times the mean this way:

`tapply(myDF$DepDelay, myDF$Origin, 3*mean, na.rm=T)`

we would need to build our own function:

`tapply(myDF$DepDelay, myDF$Origin, function(x) {3*mean(x, na.rm=T)})`

Question 2.

a.  Consider the election donation data:

`https://www.fec.gov/data/advanced/?tab=bulk-data`

from "Contributions by individuals" for 2017-18.  Download this data.

b.  Unzip the file (in the terminal).

c.  Use the `cat` command to concatenate all of the files into one large file (in the terminal).

d.  Read the data dictionary:

`https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/`

Hint:  When working with a file that is not comma separated, you can use the `read.delim` command, and just specify the character that separates the various pieces of data on a row.

Question 3.

a.  Which state's citizens gave the largest number of contributions?

b.  Which state's citizens gave the greatest amount of money?

Question 4.

a.  Now turn attention to the "all candidates" file for 2017-18, which contains summarized data.

b.  Download the data and unzip it.

c.  Read the data dictionary.

d.  Create a new data.frame that contains only these columns: 6-18, 26-27, 29-30

Question 5.

a.  Convert the data.frame to a matrix

b.  Sum these columns (all at once), using the apply function.

Question 6.

a.  Consider the Lahman baseball database available at:

`http://www.seanlahman.com/baseball-archive/statistics/`

Download the comma-delimited version and unzip it.

Inside the "core" folder of the unzipped file, you will find many csv files.

If you want to better understand the contents of the files, there is a helpful readme file available here:
`http://www.seanlahman.com/files/database/readme2017.txt`

b.  read the Teams.csv file into a data.frame called myDF

c.  we can break the data.frame into smaller data frames, according to the `teamID`, using this code:

`by(myDF, myDF$teamID, function(x) {plot(x$W)} )`

For each team, this will draw 1 plot of the number of wins per year.  The number of wins will be on the y-axis of the plots.

d.  For an improved version, we can add the years on the x-axis, as follows:

`by(myDF, myDF$teamID, function(x) {plot(x$year, x$W)} )`

e.  Change your working directory in R to a new folder, using the menu option:

`Session -> Set Working Directory -> Choose Directory`

We are going to make 149 new plots!

f.  After changing the directory, try this code, which makes 149 separate pdf files:

`by(myDF, myDF$teamID, function(x) {pdf(as.character(x$teamID[1])); plot(x$year, x$W); dev.off()} )`

Question 7.

Experiment with this concept yourself!  Make three more series of plots, using the baseball tables.  You are welcome to choose which kinds of series of plots you make.  Enjoy, and be creative!

Question 8.

Put the project into RMarkdown and submit when your code is polished and ready.


== Project 5

Question 1.

Read the selection of The Elements of Graphing Data by William Cleveland, and the selection of Creating More Effective Graphs by Naomi Robbins.

Also read the classic article "How to Display Data Badly" by Howard Wainer:
http://www.jstor.org.ezproxy.lib.purdue.edu/stable/2683253

Question 2.

Find 6 visualizations from the http://www.informationisbeautiful.net/[Information Is Beautiful website] that do a bad job of portraying data, according to the best practices in the selections from question 1.  Write 1/3 of a page (for each such visualization) about what is done poorly.

Question 3.

Identify 3 excellent visualizations of data from the site in question 2.  Write 1/3 of a page (for each such visualization) about what is done well.

Question 4.

The Gapminder comparison of life expectancy and income per person, plotted from 1800 to 2018, is eye-catching but does not necessary follow the best practices for data visualization.  See: https://www.gapminder.org/tools/ Write half a page about things that this visualization does poorly, and half a page about ways that this visualization could be re-created and re-designed, if you were to recreate this website yourself.

Question 5.

5.  Consider the poster winner "Congestion in the Sky", from the 2009 Data Expo:
http://stat-computing.org/dataexpo/2009/posters/

Describe at least 3 significant ways that this poster could be improved.  For each of these 3 ways, write a 1/3 of a page constructive criticism, specifying what could be improved and how that aspect of the visualization could be done better.

Question 6.

Choose a different poster from the 2009 Data Expo, and construct a similar analysis to question 5, i.e., give a constructive criticism of at least 3 significant ways that this poster could be improved, with 1/3 of a page writeup for each such significant need for improvement.

Question 7.

Which of the posters in the Data Expo 2009 do you think should be the winner? Why? (It is OK if you choose the poster that actually won, or any of the other posters.) Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long altogether.

Questions 8-12.

Imagine that you are going to enter the Data Expo 2009.  Your team does not need to assemble a poster, but you should design 5 plots that explore the airline data set.  Use the best practices about visualization, from the Cleveland and Robbins texts.

Be sure to put some text to explain your graphs as well, as if you were working to assemble a poster.

There are many R resources for exploring the graphing tools that are available in R.  For instance, see:

https://www.r-graph-gallery.com/

and many other similar websites.


== Project 6

Read the examples about scraping data from the web:

http://llc.stat.purdue.edu/2018/29000/nps.R

http://llc.stat.purdue.edu/2018/29000/billboard.R

(Dr. Ward will discuss these in class.)

Discuss, as a group, a website whose contents you would like to download. Study the html from the website, and work with Dr. Ward and Tyler to download the contents from the site. Your project should include a download of several hundred webpages within the site.


== Project 7

Question 1.

What percent of batters are lefties?  Switchhitters?

Question 2.

Which team has hit the most home runs altogether?

Question 3.

Find the player who had the largest number of doubles in one season.

Question 4.

During which years was Jackie Robinson active in baseball?  Check your solution by comparing the results from the SQL database with a resource online.

Question 5.

Find the statistics for how many home runs that Ernie Banks hit with a year-by-year breakdown, while he was playing for the Chicago Cubs.  FYI, Ernie Banks was awarded the Presidential Medal of Freedom in 2013, for his contribution to sports.

Question 6.

Make a dotchart of the number of home runs hit by Derek Jeter each year.

Question 7.

Find the first and last names of all the players who attended Purdue University.

Question 8.

Find the first and last names of all the players who attended college anywhere in Indiana.

Question 9.

Find all teams that won 105 or more games in a season. List the year, the team's full name, the number of wins in that season, and the first and last name of the manager for the team.

Question 10.

Find all players who stole 100 or more bases. For each such player, list the player's first name, last name, the number of bases stolen, and the year.

Question 11.

For each pitcher who had 20 or more wins, list the player's first name, last name, number of wins, and the relevant year.

Question 12.

Consider the total number of saves by pitchers during their entire careers. A few pitchers had 300 or more saves during their careers. Make a list of all such pitchers. For each such pitcher, give his first name, last name, and the total number of saves that the pitcher had during his career.

Question 13.

Same question as 12, but instead of finding pitchers with 300 or more saves, find pitchers with 2000 or more strikeouts during their careers.

Question 14.

Plot how many players were born in each state.

Question 15.

a.  Which manager's teams have had the most home runs overall?

b.  Which manager's teams have had the most stolen bases overall?


== Project 8

Question 1.

a.  Extract the total dollar amount of funds per state from the election data.

b.  Use the (built-in) state.abb to extract the results from the 50 US States (only).

Question 2.

a.  Use the master table from the SQL database to find the number of baseball players born in each state.

b.  Use the (built-in) state.abb to extract the results from the 50 US States (only).

Question 3.

a.  Extract a table that contains the total number of flights departing from each airport.

b.  Use the locations of the airports from the auxiliary airports file, available here:

http://stat-computing.org/dataexpo/2009/supplemental-data.html

to turn the results from part a into the total number of flights departing from each state.  (This will require a little thought!)

c.  Use the (built-in) `state.abb` to extract the results from the 50 US States (only).

Question 4.

4.  Make a figure that compares each of these pairs of data, from questions 1b, 2b, and 3c.  For instance, you might use the "pairs" function or another function of your choice.  The resulting plot will compare the total funds donated in each state, and the total number of baseball players born in each state, and the total number of flights departing from each state.

Question 5.

a.  Lookup the top 1000 boy and girl baby names from 2017 on this webpage:
https://www.ssa.gov/oact/babynames/

b.  Save the html of the resulting webpage.  (This is necessary because it is rendered by a CGI script, so we cannot easily scrape the data on the fly from this webpage; instead, we must download the page first.)

c.  From this saved html file, scrape the top 100 boy names and the top 100 girl names.

Question 6.

a.  In the election data, find the total amount of donations contributed by donors whose name contains the phrase Emma (in capital letters).  If you are unable to ensure that this is the "first name" that is OK for the purposes of the project, but you might want to (for instance) try to make your search as intelligent as possible.  There is more than one way to do so, of course.

b.  Repeat part 6a for four more girl names of your choice.

Question 7.

a.  Write a function that takes a girl name as input and extracts the total amount of donations contributed by donors who have this first name.

b.  Use the sapply function to run your function on each of the top 100 girl names that you extracted in 5c.

Question 8.

Repeat questions 6 and 7 for the top 100 boy names.

Question 9.

a.  Which girl name contributed the most, in terms of the total amount of donations?

b.  Which boy name contributed the most, in terms of the total amount of donations?



== Project 9

Mini-pre-Thanksgiving project:

On Thanksgiving Day 2015 (`2015-11-26`) there should be almost no taxi pickups where the parade route takes place: http://www.marching.com/news/2015/2015-macys-parade-lineup/2015_macys_parade_route_map.jpg

Question 1.

Use awk to extract the taxi cab data from Nov 26, 2015. Use three delimiters for the data: comma, space, and colon [That way, you can easily determine the hour in which a taxi cab ride starts.] It suffices to extract the taxi cab rides that started between 9 AM and 12 noon, i.e., those rides in which the hour of departure is 9, 10, or 11.

Question 2.

Save this data into a comma-separated file.

Question 3.

Import the data about the longitudes and latitudes to R.

Question 4.

Make a map of New York City at a zoom level of 14 that shows the entire parade route. (Use the data that you extracted.) Are you able to see that taxi cab rides were unable to pickup passengers along the parade route?

Question 4 Remix.

Extract the data from a region of New York City that you believe would enable you to see the entire parade route.



== Project 10

Here are a handful of examples of regular expressions.

The best way to learn them in earnest is to just read some documentation about regular expressions and then try them!

[source,r]
----
v <- c("me", "you", "mark", "laura","kale","emma","err","eat","queue","kangaroo","kangarooooo","kangarooooooooo")
v[grep("m", v)]
v[grep("me", v)]
v[grep("a", v)]
v[grep("a", v)]
v[grep("k", v)]
v[grep("^k", v)]
v[grep("e", v)]
v[grep("a$", v)]
v[grep("o$", v)]
v[grep("o", v)]
v[grep("o{2}", v)]
v[grep("o{3}", v)]
v[grep("o{2,5}", v)]
v[grep("q(ue){1}", v)]
v[grep("q(ue){2}", v)]
v[grep("q(ue){3}", v)]
v[grep("e(m|r)", v)]
v[grep("e[mr]", v)]
v[grep("e(ma|rr)", v)]
v[grep("([a-z])\\1", v)]
----

When asking about names in the questions below, we assume that you are using the names from the election data, available here:

`/depot/statclass/data/election2018/itcont.txt`

and we are assuming that you are using unique names from column 8, i.e., that you have already removed duplicates of any names of the donors.

Here is a summary of regular expressions:

https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285

You are welcome to use any source or reference for regular expressions that you like.

Please note that we need to use double backslash for back-references, when writing them in R.  In general, in R, when writing a backslash in a regular expression, a double backslash is usually needed.

Question 1.

For each of the members of your team, find the number of donors who have your name, embedded somewhere in the donor's name (not necessarily as the first or last name).

Question 2.

a.  How many donors have a consecutive repeated letter in their name?

b.  How many donors have a consecutive repeated vowel in their name?

c.  How many donors have a consecutive repeated consonant in their name?

Question 3.

Which name(s) has/have the longest sequence of alphanumeric characters?

Question 4.

a.  How many of the names have no commas?  One comma?  Two commas?  Etc.?

b.  Make a plot of the number of names with each of the counts above.

c.  The plot from 4b is a mouse-and-elephants plot, i.e., it shows small and large numbers on the same plot.  So recreate the plot using the logarithms of the counts.  This is a common technique for illustrating very large and very small numbers on the same plot.

Question 5.

abc.  Same question as 4, but using chunks of whitespace, instead of commas.
For instance, the name STERN, MARY  YERBY has two chunks of whitespace,
namely, one chunk of whitespace between STERN, and MARY
and another chunk of whitespace between MARY and YERBY (even though it is a double space, it is still just one chunk of whitespace).

Question 6.

Find the number of people with a name that contains the *word* `SMITH`.
For example, `SMITH` and `ELLIS-SMITH` are ok, but `SMITHY` is not. `\b` is useful! Also note that, in order to use `\b`, you will need to use `\\b`, just as we used another escape character for `\1`, resulting in `\\1`. For example, `\\bWARD\\b` will match all of the full names containing the *word* `WARD`

Question 7.

Question from Project 10, Team 2

Question 8.

Question from Project 10, Team 3

Question 9.

Find the number of names that contain the letter A followed by either BB or RN

Question 10.

How many people have names (first or last) that end in Z?

Question 11.

a.  How many people have Dr Ward's name or part of his name, for instance, `WARD, MARK D` as part of the name?

OR:

b.  How many people have both a Z and an X in their name?

Question 12.

Question from Project 10, Team 7



