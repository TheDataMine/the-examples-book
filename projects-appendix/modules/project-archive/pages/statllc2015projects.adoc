= STAT-LLC Fall 2015 STAT 29000 Projects

== Project 1

Question 1.

a. An `.Rda` file format is used to store data for use with R. Use the `load` function to load `review.Rda` into the R environment. Use the `ls` function to discover the data frame's name. What is the data frame's name?

b. When dealing with an unfamiliar dataset, it is typically best to get a bird's eye view of the data. Use the summary function to find the names of the columns.

The data frame contains 8 components (the votes component has 3 columns):

[source,r]
----
review_frame$votes$funny
review_frame$votes$useful
review_frame$votes$cool
review_frame$user_id
review_frame$review_id
review_frame$stars
review_frame$date
review_frame$text
review_frame$type
review_frame$business_id
}
----

Alternatively, you can use the names function to learn this information:

`names(review_frame)`

You can see that votes has some embedded columns this way:

`names(review_frame$votes)`

You can check the type of each vector as follows:

[source,r]
----
class(review_frame$votes$funny)
class(review_frame$votes$useful)
class(review_frame$votes$cool)
class(review_frame$user_id)
class(review_frame$review_id)
class(review_frame$stars)
class(review_frame$date)
class(review_frame$text)
class(review_frame$type)
class(review_frame$business_id)
}
----

Also you can see that review_frame is a data.frame

`class(review_frame)`

and that the votes are an embedded data.frame with the review_frame:

`class(review_frame$votes)`

c. How long are each of the columns?

d. What is the average number of stars given to a review?

e. What is the user id of the individual who wrote the review with the most "useful” votes?

f. Assuming the funniest reviews got the most funny votes, print the text of the funniest review.

g. What is the distribution of the number of stars?

Solution:

a. First we load the review.Rda file

`load("review.Rda")`

Then we use the `ls` function to see that `review_frame` is the name of the data frame:

`ls()`

`[1] "review_frame"`

b. The names of the columns are:

`summary(review_frame)`

[source,r]
----
     votes.funny         votes.useful          votes.cool     
 Min.   :  0.00000    Min.   :  0.00000    Min.   :  0.00000  
 1st Qu.:  0.00000    1st Qu.:  0.00000    1st Qu.:  0.00000  
 Median :  0.00000    Median :  0.00000    Median :  0.00000  
 Mean   :  0.47889    Mean   :  1.07162    Mean   :  0.59418  
 3rd Qu.:  0.00000    3rd Qu.:  1.00000    3rd Qu.:  1.00000  
 Max.   :141.00000    Max.   :166.00000    Max.   :137.00000  
   user_id           review_id             stars           date          
 Length:1569264     Length:1569264     Min.   :1.000   Length:1569264    
 Class :character   Class :character   1st Qu.:3.000   Class :character  
 Mode  :character   Mode  :character   Median :4.000   Mode  :character  
                                       Mean   :3.743                     
                                       3rd Qu.:5.000                     
                                       Max.   :5.000                     
     text               type           business_id       
 Length:1569264     Length:1569264     Length:1569264    
 Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character  
----

Alternatively, you can use the names function to learn this information:

`names(review_frame)`

[source,r]
----
[1] "votes"       "user_id"     "review_id"   "stars"       "date"       
[6] "text"        "type"        "business_id"
----

You can see that votes has some embedded columns this way:

[source,r]
----
names(review_frame$votes)
[1] "funny"  "useful" "cool"
----

You can check the type of each vector as follows:

[source,r]
----
class(review_frame$votes$funny)
[1] "integer"
class(review_frame$votes$useful)
[1] "integer"
class(review_frame$votes$cool)
[1] "integer"
class(review_frame$user_id)
[1] "character"
class(review_frame$review_id)
[1] "character"
class(review_frame$stars)
[1] "integer"
class(review_frame$date)
[1] "character"
class(review_frame$text)
[1] "character"
class(review_frame$type)
[1] "character"
class(review_frame$business_id)
[1] "character"
----

Also you can see that `review_frame` is a data frame

[source,r]
----
class(review_frame)
[1] "data.frame"
----

and that the votes are an embedded data frame with the review_frame:

[source,r]
----
class(review_frame$votes)
[1] "data.frame"
----

c. The dimension of the data frame is:

[source,r]
----
dim(review_frame)
[1] 1569264       8
----

so the number of rows is:

[source,r]
----
dim(review_frame)[1]
[1] 1569264
----

d. The average number of stars given to a review is

[source,r]
----
mean(review_frame$stars)
[1] 3.742656
----

e. The row of the data frame that has the review with the most "useful” votes is

[source,r]
----
which.max(review_frame$votes$useful)
[1] 1179107
----

So the user id of the individual who wrote that review is:

[source,r]
----
review_frame$user_id[which.max(review_frame$votes$useful)]
[1] "WJSNywtir04BgDDpZVZMpg"
----

f. The row of the data frame that has the review with the most "funny” votes is

[source,r]
----
which.max(review_frame$votes$funny)
[1] 1179107
----

So the user id of the individual who wrote that review is:

[source,r]
----
review_frame$text[which.max(review_frame$votes$funny)]
[1] "I'm the first real person to review this place, let all other fake spammers be gone!  Yelp should really work that shizzle out.  Zack S...um this place closes at 11 so you couldn't have possibly hit the bar...so yeah if your gonna post fake reviews at least check your facts..\n\nThis place has so much potential, yet the ridiculously bad service just overshadowed everything good they did.\n\nThis is probably the worst service I have ever received in my life.  \n\nDo you guys remember in Pretty Woman when Julia Roberts goes into the first store on Rodeo and the snooty lady acts like she's too good for this place?  Well that's what they did to us.\n\nWhere's the hostess?  Oh she's chatting with her friend...*ignoring me*\n\nWe finally get seated and then we sit and wait...\n\nand wait...\n\nand wait...\n\nfinally I get up and ask one of the waiters at the cash register to send over someone...\n\nShe treats us like we're a nuisance to her, she's singing along to the song that's playing...(WTF?)  I think they turned up the music louder as more people left because by the time it was near closing time it was blaring hip hop music...really weird considering it was a nice casual Italian restaurant.\n\nWe order almost 200 bucks worth of food, and then she's like \"is that it?\"\n\nexcuse me bitch...watch the attitude...\n\nWait, wait, watch as all the waiters get together to talk about us...seriously I can see you whispering about us....this is so unprofessional right now....\n\nWe seriously waited for like an hour and a half, it was ridiculous!  The place was practically empty!  The waiters were standing around and chatting with each other.  I took pictures as evidence!\n\nThis place is supposed to be a nice place, for the prices they charge they should have a whole restaurant re-staffed because it was ridiculous!  I have never felt more uncomfortable and treated so rudely in my life!\n\nI also wanted to order take out for later and apparently they don't have take out boxes?  You're a restaurant?  You don't have boxes?  Seriously?\n\nThis is the worst experience in a restaurant I have ever had.  I've gotten better service at Carls Jr in a shady neighborhood than this.  \n\nI won't hesitate to tell everyone to avoid this restaurant when you're in Vegas because this was outrageous!  No one treats me like that and gets away with it!  I wish I had a You've been yelped card so I could give them a piece of my mind.\n\nI'm like those crazy housewives who have nothing better to do...I won't stop calling to speak to management until someone is fired.  I'm serious, no one gets away with treating me like that without suffering repercussions.  We don't play that, I'm sorry I'm not one of those quiet asian people who take your shit...I will not be silenced... You best believe.  You're done..The end.\n\n\n*note I left for the manager*\n\n\"This is probably the most unprofessional and ridiculous restaurant I have ever had the misfortune to experience, not just from the our waitress, but the waitstaff and blatant ignorance from most of the employees I had to encounter.\"\n                                                                   Anthony Nguyen"
----

1g. The distribution of the number of stars is:

[source,r]
----
table(review_frame$stars)

     1      2      3      4      5 
159811 140608 222719 466599 579527
----



Question 2.

a. Create a new factor called totalvotes, which sums the numbers of funny, useful, and cool votes.

b. How many of the reviews received at least 160 votes?

c. Print the user_id’s of the people who wrote the ten reviews that were voted on the most.

Solution:

a. The sum of the numbers of funny, useful, and cool votes is:

`totalvotes <- review_frame$votes$funny + review_frame$votes$useful + review_frame$votes$cool`

(OK, I stored it into a vector, not a factor.)

b. The number of reviews that received at least 160 votes is:

[source,r]
----
sum(totalvotes >= 160)
[1] 28
----

c. The user_id’s of the people who wrote the ten reviews that were voted on the most are:

[source,r]
----
topreviewcounts <- sort(totalvotes, decreasing=T)[1:10]
review_frame$user_id[totalvotes >= min(topreviewcounts)]
[1] "8j5rre5uA2TxjX8Fk9Je3Q" "zfb_dSwWV5mV4f_ZAgkYbg"
[3] "fr3HXiNw5JiIIspADCS5gA" "zfb_dSwWV5mV4f_ZAgkYbg"
[5] "C8ZTiwa7qWoPSMIivTeSfw" "gFTglOy-Skssv7TiuW-D8g"
[7] "WJSNywtir04BgDDpZVZMpg" "YpvGOfegYJ2w8CNITiIv1A"
[9] "ptFwVDjiEKug1qGmYyZ_yw" "WmAyExqSWoiYZ5XEqpk_Uw"
----


Question 3.

a. Now use the "load” function to load business.Rda into the R environment. Once again, use the "ls” function to discover the data frames’s name. What is the data frame’s name?

b. Use the names command on the data frame to find out what variables are stored in the data frame.

c. How many unique states are a part of this data set? Hint: The factor "state” is useful here.

d. The state closest to Purdue that is also in YELP’s dataset is Illinois. How many businesses in Illinois are in the dataset?

e. How many Illinois businesses have strictly more than 50 reviews?

Solution:

a. First we load the `review.Rda` file

`load("business.Rda")`

Then we use the `ls` function to see that `business_frame` is the name of the data frame:

[source,r]
----
ls()
[1] "business_frame"  "review_frame"    "topreviewcounts" "totalvotes"
----

b. The variables stored in the data frame are:

[source,r]
----
names(business_frame)
 [1] "business_id"   "full_address"  "hours"         "open"         
 [5] "categories"    "city"          "review_count"  "name"         
 [9] "neighborhoods" "longitude"     "state"         "stars"        
[13] "latitude"      "attributes"    "type"
----

c. The unique states that are a part of this data set are:

[source,r]
----
table(business_frame$state)

   AZ    BW    CA   EDH   ELN   FIF   HAM    IL   KHL    MA   MLN    MN 
25230   934     3  2971    10     4     1   627     1     1   123     1 
   NC   NTH    NV    NW    ON    OR    PA    QC    RP    SC   SCB    WA 
 4963     1 16485     1   351     1  3041  3921    13   189     3     1 
   WI   XGL 
 2307     1
----

So the number of such states is:

[source,r]
----
length(table(business_frame$state))
[1] 26
----

d. The number of businesses in Illinois that are in the dataset is:

[source,r]
----
sum(business_frame$state == "IL")
[1] 627
----

e. The number of Illinois businesses that have strictly more than 50 reviews is:

[source,r]
----
sum(business_frame$review_count[business_frame$state == "IL"] > 50)
[1] 64
----


Question 4.

a. How many businesses are listed in Illinois?

b. How many businesses are listed in Arizona?

c. The review dataset and the business dataset have a single factor in common–business_id. The business dataset has the state in which the business resides, and the review dataset doesn’t. Let’s say that a state is more popular if it has the most votes per business (regardless of whether the votes are high or low). Which state’s businesses are more popular by this measure (i.e., by most votes per business), Arizona or Illinois? (You will need to use both data sets for this.)

Sketch of one kind of method for solution: Essentially we want to first identify which business_id’s are in Illinois, and then use the %in% command to identify which of the businesses in the review_frame correspond to Illinois, and then tally their number of reviews, and finally divide by the answer in 4a. Then we want to repeat this process for Arizona.

Solution:

a. The number of businesses that are listed in Illinois is:

[source,r]
----
ILcount <- sum(business_frame$state == "IL")
ILcount
[1] 627
----

b. The number of businesses that are listed in Arizona is:

[source,r]
----
AZcount <- sum(business_frame$state == "AZ")
AZcount
[1] 25230
----

c. The business ID’s that are for companies in IL are:

`ILbusinesses <- business_frame$business_id[business_frame$state == "IL"]`

So the total number of votes for IL businesses is:

[source,r]
----
sum(totalvotes[review_frame$business_id %in% ILbusinesses])
[1] 16970
----

So the number of votes per business in IL is:

[source,r]
----
sum(totalvotes[review_frame$business_id %in% ILbusinesses])/ILcount
[1] 27.06539
----

The business ID's that are for companies in AZ are:

`AZbusinesses <- business_frame$business_id[business_frame$state == "AZ"]`

So the total number of votes for AZ businesses is:

[source,r]
----
sum(totalvotes[review_frame$business_id %in% AZbusinesses])
[1] 1333428
----

So the number of votes per business in AZ is:

[source,r]
----
sum(totalvotes[review_frame$business_id %in% AZbusinesses])/AZcount
[1] 52.85089
----

So by this measure, the Arizona businesses are more popular.


Question 5.

a. What does the function tolower do?

b. How many of the review texts contain the word happy? (case-insensitive) Hint: it will be helpful to read about the grepl command.

c. How many of the review texts contain the word good? (case-insensitive)

d. How many of the review texts contain both of these two words? (case-insensitive) Hint: You can use one ampersand for the logical "and”.

Solution:

a. The function tolower changes a string into its lower-case representation.

b. The number of review texts that contain the word happy is:

[source,r]
----
happytexts <- grepl("happy", tolower(review_frame$text))
sum(happytexts)
[1] 108090
----

c. The number of review texts that contain the word good is:

[source,r]
----
goodtexts <- grepl("good", tolower(review_frame$text))
sum(goodtexts)
[1] 613548
----

d. The number of review texts that contain both the word happy and the word good is:

[source,r]
----
sum(happytexts &amp; goodtexts)
[1] 51428
----


== Project 2

Question 1.

a. A `.csv` file format stands for `comma separated value`, and is a very popular format to store data. The file `review.csv` is extracted from `review.Rda`. Even though the file contains the same data, it is twice the size!

Import `review.csv` into the variable called `review` using the `read.csv` function. Using the function `proc.time()`, find out how long it takes R to load this csv file. Simply run

`startingtime <- proc.time()

before the command runs and then

`stoppingtime <- proc.time()

after the `read.csv` command runs, and then take the difference of the two times, to find out how long it took to load the data.

Notice that read.csv has the parameter `head=TRUE` by default, which is good, since the csv file has the variable names stored on line 1, as a header, that lets R know the intended names of the variables found on all of the rest of the lines of the file.

b. Now load the equivalent `review.Rda` file into R using the `load` function. As above, use `proc.time()` to time how long this takes.

c. Which format is faster to read into R? Rda or csv?

d. Make sure that the data frames from the Rda and csv files of the `review.Rda` versus `review.csv` are the same dimensions.

Solution:

a. The time it takes to read the review.csv file is:

[source,r]
----
startingtime <- proc.time()
myreviewDF <- read.csv("review.csv")
stoppingtime <- proc.time()
stoppingtime - startingtime
   user  system elapsed 
 99.676   1.508 101.359
----

b. The time it takes to read the review.Rda file is:

[source,r]
----
startingtime <- proc.time()
load("review.Rda")
stoppingtime <- proc.time()
stoppingtime - startingtime
   user  system elapsed 
 16.462   0.215  17.278
----

c. The Rda file should be a lot faster to read.

d. We can check that the number of rows of the two resulting data frames are the same.

[source,r]
----
dim(review_frame)
[1] 1569264       8
dim(myreviewDF)
[1] 1569264      11
----

The number of columns are not exactly the same, because as you will recall, the review_frame has a data frame embedded within one column. Moreover, the data frame from the csv file has an extra column (called X) at the start, which just includes the row numbers of the reviews. This is not always the case, but it happens to be the case for this particular data frame.



Question 2.

a. Use the strptime function to convert the `date` factor from a factor to a `POSIXt` data type. This will allow you to add and subtract dates easily.

b. Find the time (in the format `%Y-%m-%d`) of the first review (chronologically). Find the time of the last review (chronologically). Now take a difference. This allows us to see the length of the time period in which reviews were collected.

a. Here we convert the dates into POSIXt data format.

`mytimes <- strptime(review_frame$date, "%Y-%m-%d")`

2b. The time of the first review chronologically is

[source,r]
----
min(mytimes)
[1] "2004-10-12 EDT"
----

The time of the last review chronologically is:

[source,r]
----
max(mytimes)
[1] "2015-01-08 EST"
----

The length of time during which reviews were collected is:

[source,r]
----
max(mytimes) - min(mytimes)
Time difference of 3740.042 days
----

This could also be performed by computing:

[source,r]
----
diff(range(mytimes))
Time difference of 3740.042 days
----

In the command above, the range shows the minimum and the maximum values of the vector.


Question 3.

a. Use strsplit (with `-` as the split parameter) to break the strings in the dates of the reviews into their component years, months, and dates. Then use unlist to combine the results into a vector that has all of the years, months, and dates.

b. From the vector above, extract the years of each review. (Hint: You can use the seq command, with by=3, as an index to your vector; this will allow you to extract every third element of your vector.) Check to make sure that the number of years in the vector that you created is the same as the number of reviews in the data set.

Solution:

a. We split the strings and then unlist the results:

`v <- unlist(strsplit(review_frame$date,"-"))`

b. Here are the data from the years:

`myyears <- v[seq(1,length(v),by=3)]`

The length of this vector agrees with the number of rows in the data frame, i.e., is the same as the number of reviews in the data set:

[source,r]
----
length(myyears)
[1] 1569264
dim(review_frame)[1]
[1] 1569264
----



Question 4.

a. Use tapply to find the average number of stars per year. Hint: Use the vector of years that you created in question 3b above.

b. Similarly to when you identified the total votes in Project 1, create a new column in the review_frame data frame that contains the mean of all three votes. Hint: If you use the mapply function, it is necessary to take a sum first, and then divide by 3. It is not necessary to use the mapply. It is probably just easier to take a mean directly.

c. Use tapply with the business data set to see how many reviews have been made of open businesses and how many have been made of closed businesses. (Use the review_count column to get the number of reviews of each business.)

Solution:

a. The number of reviews per year is found by taking the length of each of the parts of the review_frame$stars, according to the analogous value of myyears

[source,r]
----
tapply(review_frame$stars, myyears, length)
  2004   2005   2006   2007   2008   2009   2010   2011   2012   2013 
    13    680   4239  17724  45117  72948 137764 209429 244106 336273 
  2014   2015 
486306  14665
----

b. One possible method, using the mapply, is to do this:

`mynewcolumn <- mapply(sum, review_frame$votes$funny, review_frame$votes$useful, review_frame$votes$cool)/3`

An alternative method, we could use this approach:

`mynewcolumn <- (review_frame$votes$funny + review_frame$votes$useful + review_frame$votes$cool)/3`

Finally, we can append mynewcolumn to the data frame (which I am calling averagevotes) by writing:

`review_frame$averagevotes <- mynewcolumn`

c. We first load the business_frame

`load("business.Rda")`

We sum the number of review_count’s, according to the value (TRUE or FALSE) of whether the business is open or closed.

[source,r]
----
tapply(business_frame$review_count, business_frame$open, sum)
 FALSE    TRUE 
159561 1570264
----


Question 5.

a. Use tapply to get the number of businesses per state.

b. Use tapply to get the average number of reviews within each state.

c. How many businesses in each state have karaoke?

d. With regard to alcohol service, how many businesses are listed as having a full_bar? beer_and_wine? none? For how many businesses is it unknown whether alcohol is served? Use the table command to answer all four of these questions at once. The table command has a parameter that allows the NA elements to show up (check the documentation for the table command).

Solution:

a. We can extract the number of businesses per state by taking the length of the vector of businesses in a state, i.e., by looking at the state vector and breaking it up, according to the state vector’s values itself.

[source,r]
----
tapply(business_frame$state, business_frame$state, length)

   AZ    BW    CA   EDH   ELN   FIF   HAM    IL   KHL    MA   MLN    MN 
25230   934     3  2971    10     4     1   627     1     1   123     1 
   NC   NTH    NV    NW    ON    OR    PA    QC    RP    SC   SCB    WA 
 4963     1 16485     1   351     1  3041  3921    13   189     3     1 
   WI   XGL 
 2307     1
----

This can also be done, equivalently, in the following way:

[source,r]
----
table(business_frame$state)

   AZ    BW    CA   EDH   ELN   FIF   HAM    IL   KHL    MA   MLN    MN 
25230   934     3  2971    10     4     1   627     1     1   123     1 
   NC   NTH    NV    NW    ON    OR    PA    QC    RP    SC   SCB    WA 
 4963     1 16485     1   351     1  3041  3921    13   189     3     1 
   WI   XGL 
 2307     1
----

b. Now we take a mean of the review_count, splitting up the data according to the states in which the businesses are found.

[source,r]
----
tapply(business_frame$review_count, business_frame$state, mean)
       AZ        BW        CA       EDH       ELN       FIF       HAM 
25.238962  9.368308 14.333333 11.560417  5.100000  9.000000  3.000000 
       IL       KHL        MA       MLN        MN        NC       NTH 
20.987241  8.000000  4.000000  7.593496  4.000000 20.651823 17.000000 
       NV        NW        ON        OR        PA        QC        RP 
45.672066  5.000000  8.826211  4.000000 23.810917 13.917113  5.769231 
       SC       SCB        WA        WI       XGL 
12.798942  5.666667  9.000000 20.669267  3.000000
----

c. We just sum the TRUE’s (which become 1’s) and the FALSE’s (which become 0’s) in the karaoke vector, split according to the states, and we ignore all of the NA’s too.

[source,r]
----
tapply(business_frame$attributes$Music$karaoke, business_frame$state, sum, na.rm=T)
 AZ  BW  CA EDH ELN FIF HAM  IL KHL  MA MLN  MN  NC NTH  NV  NW  ON  OR 
 35   0   0   5   0   0   0   1   0   0   0   0   6   0  30   0   1   0 
 PA  QC  RP  SC SCB  WA  WI XGL 
 10   8   0   0   0   0   2   0
----

d. We can use useNA = "always” to see the NA values for the alcohol service:

[source,r]
----
table(business_frame$attributes$Alcohol,useNA="always")

beer_and_wine      full_bar          none          <NA> 
         2983          9069          8405         40727
----



Question 6.

a. Create a function that takes a factor with categorical variables and spits out a labeled pie chart (it is up to you whether or not to include the NA values). Call the function: givemepie. You can use the function "pie” within your function.

b. Use your function on the alcohol factor.

Solution:

a. There are several possible ways to accomplish this. Here is one such function:

[source,r]
----
givemepie <- function (x) {
  pie(table(x,useNA="always"))
}
----

b. Here is the resulting pie chart.

`givemepie(business_frame$attributes$Alcohol)`


Question 7.

a. What fraction of businesses have latitude between 32 and 40? 40 and 48? 48 and 57? Use only 1 line of code (Hint: Use the tapply function with specified values of cuts and breaks.)

b. What fraction of businesses have longitude between -120 and -80? -80 and -40? -40 and 0? 0 and 40? Use only 1 line of code.

c. In one line of code show what percent of businesses lies within the intersection of each of (a) and (b)’s breaks. You should end up with a 3x4 matrix of percentages.

Solution:

a. We can cut the latitude according to the specified values.

[source,r]
----
tapply(business_frame$latitude, cut(business_frame$latitude, breaks=c(32,40,48,57)), length)/61184
   (32,40]    (40,48]    (48,57] 
0.76608264 0.16751111 0.06640625
----

Since we are just using the "length” function, it does not actually matter what we put into the first coordinate, as long as it has the right length. Any of these would do, since they all have the same length:

[source,r]
----
length(business_frame$latitude)
[1] 61184
length(business_frame$longitude)
[1] 61184
length(seq(1,61184))
[1] 61184
----

Many other possibilities exist. So we get the same answer as above, for instance, if we substitute a dummy vector into the first coordinate. This only works because we are just taking length of whatever pieces we get.

[source,r]
----
tapply(seq(1,61184), cut(business_frame$latitude, breaks=c(32,40,48,57)), length)/61184
   (32,40]    (40,48]    (48,57] 
0.76608264 0.16751111 0.06640625
----

For a humorous way of seeing this, we could even write the following, and we would get the same answer:

[source,r]
----
tapply(rep("pizza",times=61184), cut(business_frame$latitude, breaks=c(32,40,48,57)), length)/61184
   (32,40]    (40,48]    (48,57] 
0.76608264 0.16751111 0.06640625
----

b. We handle the longitude in a similar way:

[source,r]
----
tapply(business_frame$longitude, cut(business_frame$longitude, breaks=c(-120,-80,-40,0,40)), length)/61184
(-120,-80]  (-80,-40]    (-40,0]     (0,40] 
0.83472803 0.09886572 0.05091200 0.01549425
----

but we could also have used any of the approaches similar to the above, e.g.,

[source,r]
----
tapply(seq(1,61184), cut(business_frame$longitude, breaks=c(-120,-80,-40,0,40)), length)/61184
(-120,-80]  (-80,-40]    (-40,0]     (0,40] 
0.83472803 0.09886572 0.05091200 0.01549425
----

c. Now we apply the cuts for both a and b simultaneously. You could put this all onto one line, but it might look a little long.

[source,r]
----
tapply(seq(1,61184), list( 
    cut(business_frame$latitude, breaks=c(32,40,48,57)),
    cut(business_frame$longitude, breaks=c(-120,-80,-40,0,40))
), length)/61184
        (-120,-80]  (-80,-40]  (-40,0]     (0,40]
(32,40]  0.7660826         NA       NA         NA
(40,48]  0.0686454 0.09886572       NA         NA
(48,57]         NA         NA 0.050912 0.01549425
----

If you really want to do it in a slight more readable way,here is one possible method:

[source,r]
----
myfirstcut <- cut(business_frame$latitude, breaks=c(32,40,48,57))
mysecondcut <- cut(business_frame$longitude, breaks=c(-120,-80,-40,0,40))
tapply(seq(1,61184), list(myfirstcut, mysecondcut), length)/61184
        (-120,-80]  (-80,-40]  (-40,0]     (0,40]
(32,40]  0.7660826         NA       NA         NA
(40,48]  0.0686454 0.09886572       NA         NA
(48,57]         NA         NA 0.050912 0.01549425
----


== Project 3

This project is all about the `Airline on-time performance`, from the American Statistical Association's http://stat-computing.org/dataexpo/2009/[2009 Data Expo]. There is also some href="http://stat-computing.org/dataexpo/2009/supplemental-data.html[supplemental data] provided by the ASA as well.

You can see href="http://stat-computing.org/dataexpo/2009/the-data.html[the data on the ASA site]. In particular, there is a listing of all of the parameters, which might be helpful for you to print.

I already downloaded it for you, to make things a little easier for you. Since the data itself is so large, I saved it into a common data directory:

`/data/public/dataexpo2009/`

Notes: If you want to read ALL of the data into R at once, you can do it, but it takes quite awhile (it might take more than 15 minutes to initially load the data).

You can import just a year or two of the data at a time, to start working with the data. You are not expected to import all of the data while you are solving the questions. You can wait until you have solved the questions, and then come back and try to get the answers with all of the data. So, for instance, you might want to start with just a few specific years only:

`bigDF <- rbind( read.csv("/data/public/dataexpo2009/2006.csv”), read.csv("/data/public/dataexpo2009/2007.csv”), read.csv("/data/public/dataexpo2009/2008.csv”) )`

and once you are sure that everything works, before you get ready to submit your data, you can load all of the years, by typing:

bigDF <- rbind( read.csv("/data/public/dataexpo2009/allyears.csv”) )`

There are over 3.5 billion pieces of data in the files altogether, if you load all of the years from 1987 through 2008.

Question 1.

a. What percentage of data is missing (`NA`) from `DepTime`? How about from `ArrTime`?

b. Focus on `DepTime`, `CRSDepTime`, `ArrTime`, and `CRSArrTime`. These are times in the `hhmm` format. Use the `strptime` function to convert the time `1359` to `POSIXlt` using strptime. What is the resulting output?

c. Now use the `strptime` function to convert the time `1360` to `POSIXlt` using `strptime`. What happens? Why?

d. Consider times that cannot exist (as in 1c) as erroneous data (it makes no sense!). Are there any erroneous times in `DepTime`, `CRSDepTime`, `ArrTime`, and `CRSArrTime`? If so, how many such times, in each category?

Solution:

`bigDF <- read.csv("/data/public/dataexpo2009/allyears.csv")`

a. The percentage of data that is missing from DepTime is

[source,r]
----
sum(is.na(bigDF$DepTime))/length(bigDF$DepTime)
[1] 0.0186355
----

or, expressed as a percentage (i.e., multiplied by 100)

[source,r]
----
100*sum(is.na(bigDF$DepTime))/length(bigDF$DepTime)
[1] 1.86355
----

The percentage of data that is missing from ArrTime is

[source,r]
----
sum(is.na(bigDF$ArrTime))/length(bigDF$ArrTime)
[1] 0.02092102
----

or, expressed as a percentage (i.e., multiplied by 100)

[source,r]
----
100*sum(is.na(bigDF$ArrTime))/length(bigDF$ArrTime)
[1] 2.092102
----

b. The result of converting "1359” to POSIXlt is:

[source,r]
----
strptime("1359","%H%M")
[1] "2015-09-22 13:59:00 EDT"
----

c. The result of converting "1360” to POSIXlt is an NA:

[source,r]
----
strptime("1360","%H%M")
[1] NA
----

1d. The vectors, each converted to POSIXlt format, are

[source,r]
----
myconvertedDepTime <- strptime(sprintf("%04d",bigDF$DepTime),"%H%M")
myconvertedCRSDepTime <- strptime(sprintf("%04d",bigDF$CRSDepTime),"%H%M")
myconvertedArrTime <- strptime(sprintf("%04d",bigDF$ArrTime),"%H%M")
myconvertedCRSArrTime <- strptime(sprintf("%04d",bigDF$CRSArrTime),"%H%M")
----

There are millions of erroneous data in the `DepTime` and `ArrTime` vectors. It all depends on what you treat as erroneous. For instance, here are the numbers of the `NA` values in the vectors:

[source,r]
----
sum(is.na(myconvertedDepTime))
[1] 2305590
sum(is.na(myconvertedCRSDepTime))
[1] 0
sum(is.na(myconvertedArrTime))
[1] 2604126
sum(is.na(myconvertedCRSArrTime))
[1] 1
----



Question 2.

a. Everyone hates late departure times. Of the late departures (DepDelay), what percentage of flights depart 0-5 minutes late? 5-10 minutes late? 10-15? 15-20? 20-25? Etc.?

b. Make boxplots that show, for each of the 7 days of the week, the degree to which departure times are delayed.

`If you want to only plot a random selection of the points, that is OK too. The reason is that it will probably take your R session forever to render the plot with all of the millions of dots for the millions of flights. If you choose to only plot a random selection of plots, please do not just plot the points at the start of the vector, since that would just correspond to the 1987 data. Instead, for instance, take every 1000th point. I.e., if the points that you wanted to plot are stored in vector v, then instead of plotting all of v, you could plot

`v[seq(1,length(v),by=1000)]`

This will save you a lot of time when you render your plot in R, and it will still give you a very good picture of what is going on, i.e., it will still give you a good understanding of the behavior of your data. In this case, you would need to be sure to take every 1000th point of your data, and also every 1000th day too, so that your data and the days of the week are in agreement.]

Solution:

a. First we make a vector of the delayed flights, i.e., those flights with delay 0 or greater. We would not have to make a separate vector for this, but it makes the solution a little easier to read.

`v <- bigDF$DepTime[bigDF$DepTime >= 0]`

The number of flights that are delayed, split according to 5 minute intervals, is:

`mydelays <- tapply( v, cut(v, breaks=seq(from=0,to=max(v,na.rm=T),by=5), include.lowest=T), length )`

The first several groups of delays are:

[source,r]
----
head(mydelays)
  [0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] 
  45162   35692   33958   31458   32361   38726
----

We probably do not want to look at all of the delays, because many of these regions are NA’s. Here are the ones that have at least one delay.

[source,r]
----
mydelays[!is.na(mydelays)]
      [0,5]      (5,10]     (10,15]     (15,20]     (20,25]     (25,30] 
      45162       35692       33958       31458       32361       38726 
    (30,35]     (35,40]     (40,45]     (45,50]     (50,55]     (55,60] 
      35513       38825       37430       36444       33906       23849 
    (90,95]    (95,100]   (100,105]   (105,110]   (110,115]   (115,120] 
          1       11162       23083       25344       25439       21214 
  (120,125]   (125,130]   (130,135]   (135,140]   (140,145]   (145,150] 
      20346       23274       18178       19550       19708       17385 
  (150,155]   (155,160]   (195,200]   (200,205]   (205,210]   (210,215] 
      15432       10344        3920        8219        7336        6017 
  (215,220]   (220,225]   (225,230]   (230,235]   (235,240]   (240,245] 
       5602        5015        4152        3171        3062        3229 
  (245,250]   (250,255]   (255,260]   (295,300]   (300,305]   (305,310] 
       3077        2637        1849         879        1933        1610 
  (310,315]   (315,320]   (320,325]   (325,330]   (330,335]   (335,340] 
       1646        1443        1985        1657        1469         639 
  (340,345]   (345,350]   (350,355]   (355,360]   (395,400]   (400,405] 
        679         606         743         746         366         593 
  (405,410]   (410,415]   (415,420]   (420,425]   (425,430]   (430,435] 
        804         645         670         576         925        1178 
  (435,440]   (440,445]   (445,450]   (450,455]   (455,460]   (495,500] 
       1134        1094        2495        7213       10520        5780 
  (500,505]   (505,510]   (510,515]   (515,520]   (520,525]   (525,530] 
      11111       11388       14432       19313       34435       69493 
  (530,535]   (535,540]   (540,545]   (545,550]   (550,555]   (555,560] 
      46594       60383       77003      105728      281017      460778 
  (595,600]   (600,605]   (605,610]   (610,615]   (615,620]   (620,625] 
     263508      350448      330492      396719      402073      488800 
  (625,630]   (630,635]   (635,640]   (640,645]   (645,650]   (650,655] 
     862358      538897      575575      668085      604264      707732 
  (655,660]   (695,700]   (700,705]   (705,710]   (710,715]   (715,720] 
     908703      679028      811447      661072      628139      535039 
  (720,725]   (725,730]   (730,735]   (735,740]   (740,745]   (745,750] 
     541686      758218      533787      559252      616433      553844 
  (750,755]   (755,760]   (795,800]   (800,805]   (805,810]   (810,815] 
     616234      609425      390119      665123      629096      656795 
  (815,820]   (820,825]   (825,830]   (830,835]   (835,840]   (840,845] 
     616957      645464      844236      655225      661620      689035 
  (845,850]   (850,855]   (855,860]   (895,900]   (900,905]   (905,910] 
     654662      716797      616401      329965      681731      646929 
  (910,915]   (915,920]   (920,925]   (925,930]   (930,935]   (935,940] 
     646899      623621      642518      785225      608135      583982 
  (940,945]   (945,950]   (950,955]   (955,960]  (995,1000] (1000,1005] 
     578918      542487      581879      496359      275793      613779 
(1005,1010] (1010,1015] (1015,1020] (1020,1025] (1025,1030] (1030,1035] 
     608524      608299      563555      576352      689018      545739 
(1035,1040] (1040,1045] (1045,1050] (1050,1055] (1055,1060] (1095,1100] 
     529503      549436      546609      578595      485600      289665 
(1100,1105] (1105,1110] (1110,1115] (1115,1120] (1120,1125] (1125,1130] 
     611771      607694      594353      574279      597214      722039 
(1130,1135] (1135,1140] (1140,1145] (1145,1150] (1150,1155] (1155,1160] 
     578649      585213      606791      592520      637269      531730 
(1195,1200] (1200,1205] (1205,1210] (1210,1215] (1215,1220] (1220,1225] 
     302615      651244      629277      640046      613385      617720 
(1225,1230] (1230,1235] (1235,1240] (1240,1245] (1245,1250] (1250,1255] 
     738858      607799      597697      636986      610006      632403 
(1255,1260] (1295,1300] (1300,1305] (1305,1310] (1310,1315] (1315,1320] 
     528199      283868      642784      638104      646244      636382 
(1320,1325] (1325,1330] (1330,1335] (1335,1340] (1340,1345] (1345,1350] 
     667613      828319      675873      643530      644663      580877 
(1350,1355] (1355,1360] (1395,1400] (1400,1405] (1405,1410] (1410,1415] 
     581048      459950      245878      529679      529087      554148 
(1415,1420] (1420,1425] (1425,1430] (1430,1435] (1435,1440] (1440,1445] 
     545921      555650      684341      587677      580118      614886 
(1445,1450] (1450,1455] (1455,1460] (1495,1500] (1500,1505] (1505,1510] 
     591119      591758      490188      273661      595868      590097 
(1510,1515] (1515,1520] (1520,1525] (1525,1530] (1530,1535] (1535,1540] 
     623228      592186      595176      711995      589718      577419 
(1540,1545] (1545,1550] (1550,1555] (1555,1560] (1595,1600] (1600,1605] 
     590961      560364      589540      504619      289877      620243 
(1605,1610] (1610,1615] (1615,1620] (1620,1625] (1625,1630] (1630,1635] 
     593041      598956      557670      530119      659929      545425 
(1635,1640] (1640,1645] (1645,1650] (1650,1655] (1655,1660] (1695,1700] 
     537169      585914      604387      645083      539875      327602 
(1700,1705] (1705,1710] (1710,1715] (1715,1720] (1720,1725] (1725,1730] 
     672042      664300      679292      667667      660792      782753 
(1730,1735] (1735,1740] (1740,1745] (1745,1750] (1750,1755] (1755,1760] 
     643873      623424      644666      618115      640933      504270 
(1795,1800] (1800,1805] (1805,1810] (1810,1815] (1815,1820] (1820,1825] 
     273923      605355      571921      566007      527379      526037 
(1825,1830] (1830,1835] (1835,1840] (1840,1845] (1845,1850] (1850,1855] 
     653881      577178      615526      656588      665645      718720 
(1855,1860] (1895,1900] (1900,1905] (1905,1910] (1910,1915] (1915,1920] 
     545721      271150      645130      616266      586852      544865 
(1920,1925] (1925,1930] (1930,1935] (1935,1940] (1940,1945] (1945,1950] 
     532702      622040      516769      497851      489482      473694 
(1950,1955] (1955,1960] (1995,2000] (2000,2005] (2005,2010] (2010,2015] 
     510245      391008      218815      489689      489559      504960 
(2015,2020] (2020,2025] (2025,2030] (2030,2035] (2035,2040] (2040,2045] 
     482916      483360      571979      476211      475557      487680 
(2045,2050] (2050,2055] (2055,2060] (2095,2100] (2100,2105] (2105,2110] 
     458289      450375      322611      181716      394658      383534 
(2110,2115] (2115,2120] (2120,2125] (2125,2130] (2130,2135] (2135,2140] 
     381735      345875      328127      370371      314078      309522 
(2140,2145] (2145,2150] (2150,2155] (2155,2160] (2195,2200] (2200,2205] 
     315878      298176      301226      225888      111068      251059 
(2205,2210] (2210,2215] (2215,2220] (2220,2225] (2225,2230] (2230,2235] 
     237944      224988      205610      194076      201475      153090 
(2235,2240] (2240,2245] (2245,2250] (2250,2255] (2255,2260] (2295,2300] 
     139609      141124      135640      127837       97582       44029 
(2300,2305] (2305,2310] (2310,2315] (2315,2320] (2320,2325] (2325,2330] 
     102983       98095       92429       76974       75423       85586 
(2330,2335] (2335,2340] (2340,2345] (2345,2350] (2350,2355] (2355,2360] 
      68294       70465       79380       75791       76787       52708 
(2395,2400] (2400,2405] (2405,2410] (2410,2415] (2415,2420] (2420,2425] 
      12228         397         389         304         294         205 
(2425,2430] (2430,2435] (2435,2440] (2440,2445] (2445,2450] (2450,2455] 
        239         191         168         143         140         114 
(2455,2460] (2495,2500] (2500,2505] (2505,2510] (2510,2515] (2515,2520] 
         47          55          81          91          80          61 
(2520,2525] (2525,2530] (2530,2535] (2535,2540] (2540,2545] (2545,2550] 
         44          51          42          47          35          33 
(2550,2555] (2555,2560] (2565,2570] (2595,2600] (2600,2605] (2605,2610] 
         34          13           1          17           8          15 
(2610,2615] (2615,2620] (2620,2625] (2625,2630] (2630,2635] (2635,2640] 
         15          10          12          24          10           8 
(2640,2645] (2645,2650] (2650,2655] (2655,2660] (2665,2670] (2695,2700] 
          5           6           2           1           1           2 
(2700,2705] (2705,2710] (2715,2720] (2720,2725] (2725,2730] (2730,2735] 
          1           5           1           3           1           1 
(2745,2750] (2750,2755] (2800,2805] (2925,2930] 
          1           2           1           1
----


Now we convert to percents, i.e., we divide by the total number of flights altogether.

[source,r]
----
mydelays[!is.na(mydelays)]/length(bigDF$DepTime)
       [0,5]       (5,10]      (10,15]      (15,20]      (20,25] 
3.655807e-04 2.889222e-04 2.748857e-04 2.546485e-04 2.619582e-04 
     (25,30]      (30,35]      (35,40]      (40,45]      (45,50] 
3.134821e-04 2.874733e-04 3.142835e-04 3.029911e-04 2.950096e-04 
     (50,55]      (55,60]      (90,95]     (95,100]    (100,105] 
2.744648e-04 1.930546e-04 8.094874e-09 9.035498e-05 1.868540e-04 
   (105,110]    (110,115]    (115,120]    (120,125]    (125,130] 
2.051565e-04 2.059255e-04 1.717247e-04 1.646983e-04 1.884001e-04 
   (130,135]    (135,140]    (140,145]    (145,150]    (150,155] 
1.471486e-04 1.582548e-04 1.595338e-04 1.407294e-04 1.249201e-04 
   (155,160]    (195,200]    (200,205]    (205,210]    (210,215] 
8.373338e-05 3.173191e-05 6.653177e-05 5.938400e-05 4.870686e-05 
   (215,220]    (220,225]    (225,230]    (230,235]    (235,240] 
4.534748e-05 4.059579e-05 3.360992e-05 2.566885e-05 2.478650e-05 
   (240,245]    (245,250]    (250,255]    (255,260]    (295,300] 
2.613835e-05 2.490793e-05 2.134618e-05 1.496742e-05 7.115394e-06 
   (300,305]    (305,310]    (310,315]    (315,320]    (320,325] 
1.564739e-05 1.303275e-05 1.332416e-05 1.168090e-05 1.606832e-05 
   (325,330]    (330,335]    (335,340]    (340,345]    (345,350] 
1.341321e-05 1.189137e-05 5.172624e-06 5.496419e-06 4.905494e-06 
   (350,355]    (355,360]    (395,400]    (400,405]    (405,410] 
6.014491e-06 6.038776e-06 2.962724e-06 4.800260e-06 6.508279e-06 
   (410,415]    (415,420]    (420,425]    (425,430]    (430,435] 
5.221194e-06 5.423566e-06 4.662647e-06 7.487758e-06 9.535761e-06 
   (435,440]    (440,445]    (445,450]    (450,455]    (455,460] 
9.179587e-06 8.855792e-06 2.019671e-05 5.838833e-05 8.515807e-05 
   (495,500]    (500,505]    (505,510]    (510,515]    (515,520] 
4.678837e-05 8.994214e-05 9.218442e-05 1.168252e-04 1.563363e-04 
   (520,525]    (525,530]    (530,535]    (535,540]    (540,545] 
2.787470e-04 5.625371e-04 3.771726e-04 4.887928e-04 6.233296e-04 
   (545,550]    (550,555]    (555,560]    (595,600]    (600,605] 
8.558548e-04 2.274797e-03 3.729940e-03 2.133064e-03 2.836832e-03 
   (605,610]    (610,615]    (615,620]    (620,625]    (625,630] 
2.675291e-03 3.211390e-03 3.254730e-03 3.956774e-03 6.980679e-03 
   (630,635]    (635,640]    (640,645]    (645,650]    (650,655] 
4.362303e-03 4.659207e-03 5.408064e-03 4.891441e-03 5.729001e-03 
   (655,660]    (695,700]    (700,705]    (705,710]    (710,715] 
7.355836e-03 5.496646e-03 6.568561e-03 5.351294e-03 5.084706e-03 
   (715,720]    (720,725]    (725,730]    (730,735]    (735,740] 
4.331073e-03 4.384880e-03 6.137679e-03 4.320938e-03 4.527074e-03 
   (740,745]    (745,750]    (750,755]    (755,760]    (795,800] 
4.989947e-03 4.483297e-03 4.988337e-03 4.933219e-03 3.157964e-03 
   (800,805]    (805,810]    (810,815]    (815,820]    (820,825] 
5.384087e-03 5.092453e-03 5.316673e-03 4.994189e-03 5.224950e-03 
   (825,830]    (830,835]    (835,840]    (840,845]    (845,850] 
6.833984e-03 5.303964e-03 5.355730e-03 5.577651e-03 5.299406e-03 
   (850,855]    (855,860]    (895,900]    (900,905]    (905,910] 
5.802381e-03 4.989688e-03 2.671025e-03 5.518526e-03 5.236809e-03 
   (910,915]    (915,920]    (920,925]    (925,930]    (930,935] 
5.236566e-03 5.048133e-03 5.201102e-03 6.356297e-03 4.922776e-03 
   (935,940]    (940,945]    (945,950]    (950,955]    (955,960] 
4.727261e-03 4.686268e-03 4.391364e-03 4.710237e-03 4.017964e-03 
  (995,1000]  (1000,1005]  (1005,1010]  (1010,1015]  (1015,1020] 
2.232510e-03 4.968464e-03 4.925925e-03 4.924104e-03 4.561907e-03 
 (1020,1025]  (1025,1030]  (1030,1035]  (1035,1040]  (1040,1045] 
4.665497e-03 5.577514e-03 4.417688e-03 4.286260e-03 4.447615e-03 
 (1045,1050]  (1050,1055]  (1055,1060]  (1095,1100]  (1100,1105] 
4.424731e-03 4.683654e-03 3.930871e-03 2.344802e-03 4.952209e-03 
 (1105,1110]  (1110,1115]  (1115,1120]  (1120,1125]  (1125,1130] 
4.919206e-03 4.811213e-03 4.648716e-03 4.834372e-03 5.844815e-03 
 (1130,1135]  (1135,1140]  (1140,1145]  (1145,1150]  (1150,1155] 
4.684091e-03 4.737225e-03 4.911897e-03 4.796375e-03 5.158612e-03 
 (1155,1160]  (1195,1200]  (1200,1205]  (1205,1210]  (1210,1215] 
4.304287e-03 2.449630e-03 5.271738e-03 5.093918e-03 5.181092e-03 
 (1215,1220]  (1220,1225]  (1225,1230]  (1230,1235]  (1235,1240] 
4.965274e-03 5.000366e-03 5.980962e-03 4.920056e-03 4.838282e-03 
 (1240,1245]  (1245,1250]  (1250,1255]  (1255,1260]  (1295,1300] 
5.156321e-03 4.937922e-03 5.119223e-03 4.275704e-03 2.297876e-03 
 (1300,1305]  (1305,1310]  (1310,1315]  (1315,1320]  (1320,1325] 
5.203255e-03 5.165371e-03 5.231264e-03 5.151432e-03 5.404243e-03 
 (1325,1330]  (1330,1335]  (1335,1340]  (1340,1345]  (1345,1350] 
6.705138e-03 5.471107e-03 5.209294e-03 5.218466e-03 4.702126e-03 
 (1350,1355]  (1355,1360]  (1395,1400]  (1400,1405]  (1405,1410] 
4.703510e-03 3.723237e-03 1.990351e-03 4.287685e-03 4.282893e-03 
 (1410,1415]  (1415,1420]  (1420,1425]  (1425,1430]  (1430,1435] 
4.485758e-03 4.419162e-03 4.497917e-03 5.539654e-03 4.757171e-03 
 (1435,1440]  (1440,1445]  (1445,1450]  (1450,1455]  (1455,1460] 
4.695982e-03 4.977425e-03 4.785034e-03 4.790206e-03 3.968010e-03 
 (1495,1500]  (1500,1505]  (1505,1510]  (1510,1515]  (1515,1520] 
2.215251e-03 4.823476e-03 4.776761e-03 5.044952e-03 4.793671e-03 
 (1520,1525]  (1525,1530]  (1530,1535]  (1535,1540]  (1540,1545] 
4.817875e-03 5.763510e-03 4.773693e-03 4.674134e-03 4.783755e-03 
 (1545,1550]  (1550,1555]  (1555,1560]  (1595,1600]  (1600,1605] 
4.536076e-03 4.772252e-03 4.084827e-03 2.346518e-03 5.020789e-03 
 (1605,1610]  (1610,1615]  (1615,1620]  (1620,1625]  (1625,1630] 
4.800592e-03 4.848473e-03 4.514268e-03 4.291246e-03 5.342042e-03 
 (1630,1635]  (1635,1640]  (1640,1645]  (1645,1650]  (1650,1655] 
4.415147e-03 4.348315e-03 4.742900e-03 4.892437e-03 5.221866e-03 
 (1655,1660]  (1695,1700]  (1700,1705]  (1705,1710]  (1710,1715] 
4.370220e-03 2.651897e-03 5.440095e-03 5.377425e-03 5.498783e-03 
 (1715,1720]  (1720,1725]  (1725,1730]  (1730,1735]  (1735,1740] 
5.404680e-03 5.349028e-03 6.336287e-03 5.212071e-03 5.046539e-03 
 (1740,1745]  (1745,1750]  (1750,1755]  (1755,1760]  (1795,1800] 
5.218490e-03 5.003563e-03 5.188272e-03 4.082002e-03 2.217372e-03 
 (1800,1805]  (1805,1810]  (1810,1815]  (1815,1820]  (1820,1825] 
4.900272e-03 4.629628e-03 4.581755e-03 4.269067e-03 4.258203e-03 
 (1825,1830]  (1830,1835]  (1835,1840]  (1840,1845]  (1845,1850] 
5.293084e-03 4.672183e-03 4.982605e-03 5.314997e-03 5.388312e-03 
 (1850,1855]  (1855,1860]  (1895,1900]  (1900,1905]  (1905,1910] 
5.817948e-03 4.417543e-03 2.194925e-03 5.222246e-03 4.988596e-03 
 (1910,1915]  (1915,1920]  (1920,1925]  (1925,1930]  (1930,1935] 
4.750493e-03 4.410613e-03 4.312156e-03 5.035335e-03 4.183180e-03 
 (1935,1940]  (1940,1945]  (1945,1950]  (1950,1955]  (1955,1960] 
4.030041e-03 3.962295e-03 3.834493e-03 4.130369e-03 3.165160e-03 
 (1995,2000]  (2000,2005]  (2005,2010]  (2010,2015]  (2015,2020] 
1.771280e-03 3.963971e-03 3.962918e-03 4.087588e-03 3.909144e-03 
 (2020,2025]  (2025,2030]  (2030,2035]  (2035,2040]  (2040,2045] 
3.912738e-03 4.630098e-03 3.854868e-03 3.849574e-03 3.947708e-03 
 (2045,2050]  (2050,2055]  (2055,2060]  (2095,2100]  (2100,2105] 
3.709792e-03 3.645729e-03 2.611495e-03 1.470968e-03 3.194707e-03 
 (2105,2110]  (2110,2115]  (2115,2120]  (2120,2125]  (2125,2130] 
3.104659e-03 3.090097e-03 2.799815e-03 2.656147e-03 2.998107e-03 
 (2130,2135]  (2135,2140]  (2140,2145]  (2145,2150]  (2150,2155] 
2.542422e-03 2.505542e-03 2.556993e-03 2.413697e-03 2.438386e-03 
 (2155,2160]  (2195,2200]  (2200,2205]  (2205,2210]  (2210,2215] 
1.828535e-03 8.990815e-04 2.032291e-03 1.926127e-03 1.821249e-03 
 (2215,2220]  (2220,2225]  (2225,2230]  (2230,2235]  (2235,2240] 
1.664387e-03 1.571021e-03 1.630915e-03 1.239244e-03 1.130117e-03 
 (2240,2245]  (2245,2250]  (2250,2255]  (2255,2260]  (2295,2300] 
1.142381e-03 1.097989e-03 1.034824e-03 7.899140e-04 3.564092e-04 
 (2300,2305]  (2305,2310]  (2310,2315]  (2315,2320]  (2320,2325] 
8.336344e-04 7.940667e-04 7.482011e-04 6.230948e-04 6.105397e-04 
 (2325,2330]  (2330,2335]  (2335,2340]  (2340,2345]  (2345,2350] 
6.928079e-04 5.528313e-04 5.704053e-04 6.425711e-04 6.135186e-04 
 (2350,2355]  (2355,2360]  (2395,2400]  (2400,2405]  (2405,2410] 
6.215811e-04 4.266646e-04 9.898412e-05 3.213665e-06 3.148906e-06 
 (2410,2415]  (2415,2420]  (2420,2425]  (2425,2430]  (2430,2435] 
2.460842e-06 2.379893e-06 1.659449e-06 1.934675e-06 1.546121e-06 
 (2435,2440]  (2440,2445]  (2445,2450]  (2450,2455]  (2455,2460] 
1.359939e-06 1.157567e-06 1.133282e-06 9.228156e-07 3.804591e-07 
 (2495,2500]  (2500,2505]  (2505,2510]  (2510,2515]  (2515,2520] 
4.452181e-07 6.556848e-07 7.366335e-07 6.475899e-07 4.937873e-07 
 (2520,2525]  (2525,2530]  (2530,2535]  (2535,2540]  (2540,2545] 
3.561745e-07 4.128386e-07 3.399847e-07 3.804591e-07 2.833206e-07 
 (2545,2550]  (2550,2555]  (2555,2560]  (2565,2570]  (2595,2600] 
2.671308e-07 2.752257e-07 1.052334e-07 8.094874e-09 1.376129e-07 
 (2600,2605]  (2605,2610]  (2610,2615]  (2615,2620]  (2620,2625] 
6.475899e-08 1.214231e-07 1.214231e-07 8.094874e-08 9.713849e-08 
 (2625,2630]  (2630,2635]  (2635,2640]  (2640,2645]  (2645,2650] 
1.942770e-07 8.094874e-08 6.475899e-08 4.047437e-08 4.856924e-08 
 (2650,2655]  (2655,2660]  (2665,2670]  (2695,2700]  (2700,2705] 
1.618975e-08 8.094874e-09 8.094874e-09 1.618975e-08 8.094874e-09 
 (2705,2710]  (2715,2720]  (2720,2725]  (2725,2730]  (2730,2735] 
4.047437e-08 8.094874e-09 2.428462e-08 8.094874e-09 8.094874e-09 
 (2745,2750]  (2750,2755]  (2800,2805]  (2925,2930] 
8.094874e-09 1.618975e-08 8.094874e-09 8.094874e-09
----


From this point of view, the delays do not look so bad.

b. The delay, plotted in boxplots, according to the day of the week, is given as follows. (As I suggested, I am only plotting every 10000th value, so that my plot renders in a reasonable time in R.)

[source,r]
----
boxplot(bigDF$DepDelay[seq(1,length(bigDF$DepDelay),by=1000)]
        ~ bigDF$DayOfWeek[seq(1,length(bigDF$DayOfWeek),by=1000)],
        xlab="Day of Week", ylab="Delay Time (in Minutes)",
        names=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
----




Question 3.

a. Give a chart with 12 columns (corresponding to the months) and 22 rows (corresponding to the years), which computes how many flights have DepDelay > 0 in each of the months and years.

b. Restrict attention to only the flights with delays. You can find whether a flight is delayed by checking whether the DepDelay is positive. What are the 5 carriers who are most responsible for these delays?

Solution:

a. We first extract the departure delays that are bigger than 0, along with the corresponding months and years.

[source,r]
----
mydelays <- bigDF$DepDelay[bigDF$DepDelay > 0]
mymonths <- bigDF$Month[bigDF$DepDelay > 0]
myyears <- bigDF$Year[bigDF$DepDelay > 0]
----

Then we make the required table my splitting the delays up, according to the years and months, and taking the length of each group.

[source,r]
----
tapply(mydelays, list(myyears,mymonths), length)
          1      2      3      4      5      6      7      8      9     10
1987     NA     NA     NA     NA     NA     NA     NA     NA     NA 175568
1988 198610 177939 187141 159216 164107 165596 174844 175591 138322 162211
1989 178161 181324 204720 157890 170654 201395 187426 203535 154504 173312
1990 192521 184949 205043 174695 174125 188768 186310 210335 160184 182707
1991 203500 160719 185433 178541 177146 178767 184046 191689 146535 174082
1992 178973 168341 193367 163937 174097 207596 217399 220465 171697 171109
1993 202742 185820 214420 185850 167812 189905 187657 193809 162196 181722
1994 215791 184811 195139 181870 170538 201658 222409 207187 170182 190085
1995 239816 199741 225147 205346 207197 229587 219451 226650 170373 208243
1996 242239 217413 226970 195384 202024 227273 222170 228420 177621 204564
1997 230529 189105 212869 189348 178435 208492 205474 205627 141879 172938
1998 183744 169230 202308 183799 190386 223474 203528 195213 141190 163813
1999 203215 153464 184895 181376 176281 209395 217703 195486 149983 175296
2000 175940 172394 192111 187660 195957 238751 229929 229822 164704 189149
2001 203813 191111 217148 187364 175053 212220 210760 213701 127047 166568
2002 150187 130155 176699 142233 143028 171039 174338 156760  99776 128181
2003 151238 158369 152156 125699 136551 163497 183491 178979 113916 131409
2004 198818 183658 183273 170114 191604 238074 237670 215667 147508 193951
2005 229809 184920 226883 169221 178327 236724 268988 240410 165541 186778
2006 197789 198371 235207 212412 218097 263900 281457 254405 209985 248878
2007 255777 259288 276261 249097 241699 307986 307864 298530 195615 231129
2008 247948 252765 271969 220864 220614 271014 253632 231349 147061 162531
         11     12
1987 177218 218858
1988 175123 189137
1989 176805 213745
1990 173768 218597
1991 167768 203388
1992 185240 224848
1993 188021 208364
1994 213819 240057
1995 218360 252739
1996 188032 253297
1997 176547 216099
1998 148623 210366
1999 164648 189462
2000 207426 245640
2001 152375 177983
2002 117199 165138
2003 157157 206743
2004 197560 254786
2005 193399 256861
2006 230224 274930
2007 217557 304011
2008 157278 263949
----

b. We first extract the carriers for the flights with positive DepDelays.

`mycarriers <- bigDF$UniqueCarrier[bigDF$DepDelay > 0]`

Then we find the 5 carriers with the most delays, using table:

[source,r]
----
sort(table(mycarriers),decreasing=TRUE)[1:5]
mycarriers
     DL      US      WN      AA      UA 
8064705 6771312 6264617 6064229 6005036
----



Question 4.

a. The airports.csv file contains data on each of the airports. Load airports.csv into a data frame called airports.

b. Add a factor to the airports data frame called "freq”, which gives the total number of flights both into and out of the respective airport.

c. Identify the 5 most popular departure-to-arrival paths in the USA.

d. Find the very most popular departure-to-arrival path in each year.

Solution:

a. We read in the data to a data frame called airports.

`airports <- read.csv("http://stat-computing.org/dataexpo/2009/airports.csv")`

b. First we find the number of flights into each airport, and then we use the IATA data from the airports as an index into this table, to select the airports that we want. I am using airport$iata as a vector, to ensure that they stay in the right order.

`airports$freq <- table(bigDF$Dest)[as.character(airports$iata)] + table(bigDF$Origin)[as.character(airports$iata)]`

If you want to check and see that the results look promising, for instance, you could check the lines for Los Angeles and Chicago:

[source,r]
----
airports[airports$iata %in% c("LAX","ORD") , ]
     iata                      airport        city state country      lat
2040  LAX    Los Angeles International Los Angeles    CA     USA 33.94254
2532  ORD Chicago O'Hare International     Chicago    IL     USA 41.97960
           long     freq
2040 -118.40807  8175942
2532  -87.90446 13235477
----

c. We paste together the departure to arrival paths.

`v <- paste(bigDF$Origin, "to", bigDF$Dest, sep="")`

Then we tabulate the results, sort them, and select the most popular 5 of them.

[source,r]
----
sort(table(v),decreasing=TRUE)[1:5]
v
SFOtoLAX LAXtoSFO LAXtoLAS LAStoLAX PHXtoLAX 
  338472   336938   292125   286328   279716
----

d. We again use the vector of departure to arrival paths above. Then we split the paths according to the year, and within each year, we tabulate the results and take the largest value within the year. Then we get the name of this largest value

[source,r]
----
tapply( v, bigDF$Year, function(x) names(sort(table(x),decreasing=TRUE)[1]) )
      1987       1988       1989       1990       1991       1992 
"SFOtoLAX" "LAXtoSFO" "LAXtoSFO" "SFOtoLAX" "SFOtoLAX" "LAXtoSFO" 
      1993       1994       1995       1996       1997       1998 
"LAXtoSFO" "SFOtoLAX" "LAXtoLAS" "LAXtoLAS" "LAXtoSFO" "LAXtoLAS" 
      1999       2000       2001       2002       2003       2004 
"LAXtoSFO" "LAXtoLAS" "LAXtoLAS" "LAXtoLAS" "SANtoLAX" "SANtoLAX" 
      2005       2006       2007       2008 
"SANtoLAX" "SANtoLAX" "OGGtoHNL" "SFOtoLAX"
----


Question 5.

a. The file plane-data.csv contains data on the planes. Load plane-data.csv into a data frame called planes.

b. Rank the 10 manufacturers, according to the total number of miles flown. It will be necessary to use the TailNum information from the plane-data file (which has tailnum and manufacturer) and from the large dataexpo data (which has TailNum and Distance).

c. Consider all of the planes that flew over 10000 miles in 2008. How many such planes are there? How old is the oldest such plane?

d. There are 5 airplane types in the plane-data ("Co-Owner”, "Corporation”, "Foreign Corporation”, "Individual”, "Partnership”, and also one unknown "”). Show the total breakdown of miles, according to these types of plane.

Solution:

a. We read in the data to a data frame called planes.

`planes <- read.csv("http://stat-computing.org/dataexpo/2009/plane-data.csv")`

b. First we find the number of miles for each TailNum, and then we use the manufacturer data from the planes data frame as an index into this table, to select the TailNums that we want.

`mymiles <- tapply(bigDF$Distance, bigDF$TailNum, sum, na.rm=TRUE)`

Then we break these miles into groups, according to the manufacturer, and we sum the miles in each group (i.e., within each manufacturer). Finally we extract the top 10 of these manufacturers, according to the most miles flown altogether.

[source,r]
----
mymanufacturersmiles <- tapply(as.numeric(mymiles[as.character(planes$tailnum)]),planes$manufacturer,sum, na.rm=TRUE)
sort(mymanufacturersmiles,decreasing=TRUE)[1:10]
                       BOEING              AIRBUS INDUSTRIE 
                  17601082510                    4773100539 
            MCDONNELL DOUGLAS                               
                   3627696769                    3523114053 
               BOMBARDIER INC                       EMBRAER 
                   2333926590                    2237565666 
MCDONNELL DOUGLAS AIRCRAFT CO                        AIRBUS 
                   1418210205                    1388680768 
                     CANADAIR                       DOUGLAS 
                    353418163                     333387468
----

c. The planes that flew over 10000 miles in 2008 have these TailNum’s:

[source,r]
----
mymiles2008 <- tapply(bigDF$Distance[bigDF$Year == 2008],
                      as.character(bigDF$TailNum)[bigDF$Year == 2008], sum, na.rm=TRUE)
mylong2008miles <- mymiles2008[mymiles2008 > 10000]
----

The number of such planes is:

[source,r]
----
length(mylong2008miles)
[1] 5333
----

In fact, this is almost all of the planes that flew in 2008, because the total number of planes from 2008 is:

[source,r]
----
length(mymiles2008)
[1] 5374
----

Now we make a vector of the plane dates, with the tailnum’s as the names of the vector, and we extract the elements of this vector that appeared in mylong2008miles

[source,r]
----
mydates <- as.character(planes$issue_date)
names(mydates) <- as.character(planes$tailnum)
----

The oldest such plane was issued on this date:

[source,r]
----
min(strptime(mydates[names(mylong2008miles)], "%m/%d/%Y"),na.rm=TRUE)
[1] "1976-01-09 EST"
----

d. We again use the number of miles for each TailNum, from 5b above, stored in mymiles. Then we break these miles into groups, according to the "type”, and we sum the miles in each group (i.e., within each type"). Finally we sort these”types", according to the most miles flown altogether.

[source,r]
----
mytypesmiles <- tapply(as.numeric(mymiles[as.character(planes$tailnum)]),planes$type,sum, na.rm=TRUE)
sort(mytypesmiles,decreasing=TRUE)
        Corporation                              Individual 
        34483365326          3523114053           280267907 
Foreign Corporation            Co-Owner         Partnership 
           73042583            55112463            15614504
----


Question 6.

a. Use the airports.csv file to determine how many airports are listed for each state.

b. Using the iata codes from the airports.csv file, and restricting attention to the airports from Indiana, which 5 airports in Indiana had the most arriving flights?

c. Using the iata codes from the airports.csv file, and restricting attention to the airports from the Midwest (which we will call "IL”, "IN”, "MI”, "OH”, "WI”), identify the 5 most popular departure-to-arrival paths within the Midwest (i.e., which both depart and also arrive in the Midwest).

Solution:

a. The number of airports per state is:

[source,r]
----
table(airports$state)

 AK  AL  AR  AS  AZ  CA  CO  CQ  CT  DC  DE  FL  GA  GU  HI  IA  ID  IL 
263  73  74   3  59 205  49   4  15   1   5 100  97   1  16  78  37  88 
 IN  KS  KY  LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM 
 65  78  50  55  30  18  34  94  89  74  72  71  72  52  73  14  35  51 
 NV  NY  OH  OK  OR  PA  PR  RI  SC  SD  TN  TX  UT  VA  VI  VT  WA  WI 
 32  97 100 102  57  71  11   6  52  57  70 209  35  47   5  13  65  84 
 WV  WY 
 24  32
----

b. First we get the iata codes from Indiana airports.

`indiana <- airports$iata[airports$state == "IN"]`

Then we go into the bigDF, and look at the flights for which the airport code is in Indiana. There are actually only 4 such airports! These are also listed on Wikipedia’s list

http://en.wikipedia.org/wiki/List_of_airports_in_Indiana

namely, they are IND, FWA, SBN, EVV:

[source,r]
----
sort(table(bigDF$Dest)[as.character(indiana)],decreasing=TRUE)[1:5]

   IND    FWA    SBN    EVV   <NA> 
821734  78590  65110  48571     NA
----

c. The airports in the Midwest are:

`midwest <- airports$iata[airports$state %in% c("IL", "IN", "MI", "OH", "WI")]`

Then we use the departure-to-arrival paths from vector v, created back in question 4c. We limit attention to those paths with Origin and Destination in the midwest. Then we make a table of the relevant paths, sort the table, and extract the top 5 paths.

[source,r]
----
sort(table(v[(bigDF$Origin %in% midwest) &amp; (bigDF$Dest %in% midwest)]),decreasing=TRUE)[1:5]

ORDtoDTW DTWtoORD CLEtoORD ORDtoCLE DTWtoMDW 
  159849   156932   109296   109211   103333
----


Question 7.

Use mapply to print sentences for corresponding to question 4c, e.g., the sentences might say something like:

"The number 1 departure-to-arrival path in the USA is `ORD` to `IND` with 000000 flights altogether."

(but of course use the actual values for the origin, destination, and number of flights, and do this for all 5 results in 4c, by using the `mapply` function with the `paste` command.)

Solution:

We proceed as in 4c, but we save the results:

`myresults <- sort(table(v),decreasing=TRUE)[1:5]`
Then we use mapply to print the results:

[source,r]
----
mapply(paste, "The number ", 1:5, " departure-to-arrival path in the USA is ", names(myresults),
             " with ", myresults, " flights altogether.", sep="", USE.NAMES=FALSE)
[1] "The number 1 departure-to-arrival path in the USA is SFOtoLAX with 338472 flights altogether."
[2] "The number 2 departure-to-arrival path in the USA is LAXtoSFO with 336938 flights altogether."
[3] "The number 3 departure-to-arrival path in the USA is LAXtoLAS with 292125 flights altogether."
[4] "The number 4 departure-to-arrival path in the USA is LAStoLAX with 286328 flights altogether."
[5] "The number 5 departure-to-arrival path in the USA is PHXtoLAX with 279716 flights altogether."
----



Question 8.

a. One way that we might try to predict the hub airport for each of the airlines is to find the airport where that airline departs most often, i.e., the airport that is most often used as the origin for that airline. Print a table that shows, for each airline, this top airport origin.

b. Solve question 8a again, using the destination airports instead of origin airports this time.

c. Now consider each airport, and find which airline departs from that airport most often.

d. Solve question 8c again, this time finding which airline arrives to that airport most often.

Solution:

a. We group the origins according to the airlines. Then we tabulate these cities, sort and take the maximum, and find the name of the city.

[source,r]
----
tapply(bigDF$Origin, bigDF$UniqueCarrier, function(x) names(sort(table(x),decreasing=TRUE)[1]))
    9E     AA     AQ     AS     B6     CO     DH     DL     EA     EV 
 "DTW"  "DFW"  "HNL"  "SEA"  "JFK"  "IAH"  "IAD"  "ATL"  "ATL"  "ATL" 
    F9     FL     HA     HP ML (1)     MQ     NW     OH     OO PA (1) 
 "DEN"  "ATL"  "HNL"  "PHX"  "MDW"  "DFW"  "DTW"  "CVG"  "SLC"  "MIA" 
    PI     PS     TW     TZ     UA     US     WN     XE     YV 
 "CLT"  "LAX"  "STL"  "MDW"  "ORD"  "CLT"  "PHX"  "IAH"  "PHX"
----

b. Same thing, but with destinations.

[source,r]
----
tapply(bigDF$Dest, bigDF$UniqueCarrier, function(x) names(sort(table(x),decreasing=TRUE)[1]))
    9E     AA     AQ     AS     B6     CO     DH     DL     EA     EV 
 "DTW"  "DFW"  "HNL"  "SEA"  "JFK"  "IAH"  "IAD"  "ATL"  "ATL"  "ATL" 
    F9     FL     HA     HP ML (1)     MQ     NW     OH     OO PA (1) 
 "DEN"  "ATL"  "HNL"  "PHX"  "MDW"  "DFW"  "DTW"  "CVG"  "SLC"  "MIA" 
    PI     PS     TW     TZ     UA     US     WN     XE     YV 
 "CLT"  "LAX"  "STL"  "MDW"  "ORD"  "CLT"  "PHX"  "IAH"  "PHX"
----

c. Same thing as 8a, but interchanging the role of the origin and the airline.

[source,r]
----
tapply(bigDF$UniqueCarrier, bigDF$Origin, function(x) names(sort(table(x),decreasing=TRUE)[1]))
 ABE  ABI  ABQ  ABY  ACK  ACT  ACV  ACY  ADK  ADQ  AEX  AGS  AKN  ALB  ALO 
"US" "MQ" "WN" "EV" "XE" "MQ" "OO" "US" "AS" "AS" "EV" "DL" "AS" "US" "9E" 
 AMA  ANC  ANI  APF  ASE  ATL  ATW  AUS  AVL  AVP  AZO  BDL  BET  BFF  BFI 
"WN" "AS" "AS" "EV" "OO" "DL" "OO" "WN" "US" "US" "NW" "US" "AS" "OO" "CO" 
 BFL  BGM  BGR  BHM  BIL  BIS  BJI  BLI  BMI  BNA  BOI  BOS  BPT  BQK  BQN 
"OO" "US" "MQ" "WN" "NW" "NW" "9E" "US" "MQ" "WN" "WN" "US" "XE" "EV" "B6" 
 BRO  BRW  BTM  BTR  BTV  BUF  BUR  BWI  BZN  CAE  CAK  CCR  CDC  CDV  CEC 
"XE" "AS" "OO" "DL" "US" "US" "WN" "WN" "DL" "DL" "FL" "US" "OO" "AS" "OO" 
 CHA  CHO  CHS  CIC  CID  CKB  CLD  CLE  CLL  CLT  CMH  CMI  CMX  COD  COS 
"DL" "EV" "DL" "OO" "TW" "OH" "OO" "CO" "MQ" "US" "US" "MQ" "9E" "OO" "UA" 
 CPR  CRP  CRW  CSG  CVG  CWA  CYS  DAB  DAL  DAY  DBQ  DCA  DEN  DET  DFW 
"OO" "WN" "US" "EV" "DL" "OO" "OO" "DL" "WN" "US" "MQ" "US" "UA" "WN" "AA" 
 DHN  DLG  DLH  DRO  DSM  DTW  DUT  EAU  EFD  EGE  EKO  ELM  ELP  ERI  EUG 
"EV" "AS" "NW" "YV" "UA" "NW" "AS" "NW" "XE" "AA" "OO" "US" "WN" "US" "UA" 
 EVV  EWN  EWR  EYW  FAI  FAR  FAT  FAY  FCA  FLG  FLL  FLO  FMN  FNT  FOE 
"MQ" "EV" "CO" "EV" "AS" "NW" "OO" "US" "DL" "HP" "DL" "EV" "OO" "NW" "UA" 
 FSD  FSM  FWA  GCC  GCN  GEG  GFK  GGG  GJT  GLH  GNV  GPT  GRB  GRK  GRR 
"NW" "MQ" "MQ" "YV" "HP" "WN" "NW" "MQ" "OO" "9E" "EV" "EV" "NW" "MQ" "NW" 
 GSO  GSP  GST  GTF  GTR  GUC  GUM  HDN  HHH  HKY  HLN  HNL  HOU  HPN  HRL 
"US" "DL" "AS" "NW" "EV" "YV" "CO" "AA" "EV" "EV" "DL" "HA" "WN" "UA" "WN" 
 HSV  HTS  HVN  IAD  IAH  ICT  IDA  ILE  ILG  ILM  IND  INL  IPL  ISO  ISP 
"DL" "US" "UA" "UA" "CO" "UA" "OO" "MQ" "EV" "US" "US" "9E" "OO" "PI" "WN" 
 ITH  ITO  IYK  JAC  JAN  JAX  JFK  JNU  KOA  KSM  KTN  LAN  LAS  LAW  LAX 
"US" "HA" "OO" "DL" "DL" "US" "B6" "AS" "HA" "AS" "AS" "NW" "WN" "MQ" "UA" 
 LBB  LCH  LEX  LFT  LGA  LGB  LIH  LIT  LMT  LNK  LNY  LRD  LSE  LWB  LWS 
"WN" "XE" "DL" "XE" "US" "B6" "HA" "WN" "OO" "UA" "HA" "MQ" "NW" "EV" "OO" 
 LYH  MAF  MAZ  MBS  MCI  MCN  MCO  MDT  MDW  MEI  MEM  MFE  MFR  MGM  MHT 
"EV" "WN" "MQ" "NW" "WN" "EV" "DL" "US" "WN" "EV" "NW" "CO" "OO" "DL" "WN" 
 MIA  MIB  MKC  MKE  MKG  MKK  MLB  MLI  MLU  MOB  MOD  MOT  MQT  MRY  MSN 
"AA" "NW" "OO" "NW" "OO" "HA" "DL" "TW" "DL" "DL" "OO" "NW" "MQ" "OO" "NW" 
 MSO  MSP  MSY  MTH  MTJ  MYR  OAJ  OAK  OGD  OGG  OKC  OMA  OME  ONT  ORD 
"OO" "NW" "WN" "EV" "OO" "US" "US" "WN" "OO" "HA" "WN" "UA" "AS" "WN" "UA" 
 ORF  ORH  OTH  OTZ  OXR  PBI  PDX  PFN  PHF  PHL  PHX  PIA  PIE  PIH  PIR 
"US" "US" "OO" "AS" "OO" "DL" "AS" "EV" "FL" "US" "HP" "MQ" "TZ" "OO" "9E" 
 PIT  PLN  PMD  PNS  PSC  PSE  PSG  PSP  PUB  PVD  PVU  PWM  RAP  RDD  RDM 
"US" "9E" "HP" "DL" "DL" "B6" "AS" "OO" "HP" "US" "EV" "DL" "NW" "OO" "OO" 
 RDR  RDU  RFD  RHI  RIC  RKS  RNO  ROA  ROC  ROP  ROR  ROW  RST  RSW  SAN 
"NW" "AA" "OO" "9E" "US" "YV" "WN" "US" "US" "CO" "CO" "MQ" "NW" "DL" "WN" 
 SAT  SAV  SBA  SBN  SBP  SCC  SCE  SCK  SDF  SEA  SFO  SGF  SGU  SHV  SIT 
"WN" "DL" "OO" "NW" "OO" "AS" "OH" "US" "WN" "AS" "UA" "MQ" "OO" "DL" "AS" 
 SJC  SJT  SJU  SLC  SLE  SMF  SMX  SNA  SOP  SPI  SPN  SPS  SRQ  STL  STT 
"WN" "MQ" "AA" "DL" "OO" "WN" "OO" "AA" "EV" "OO" "CO" "MQ" "DL" "TW" "AA" 
 STX  SUN  SUX  SWF  SYR  TEX  TLH  TOL  TPA  TRI  TTN  TUL  TUP  TUS  TVC 
"AA" "OO" "UA" "AA" "US" "YV" "DL" "US" "US" "US" "OH" "WN" "EV" "AA" "NW" 
 TVL  TWF  TXK  TYR  TYS  UCA  VCT  VIS  VLD  VPS  WRG  WYS  XNA  YAK  YAP 
"AA" "OO" "MQ" "MQ" "DL" "US" "OO" "OO" "EV" "NW" "AS" "OO" "MQ" "AS" "CO" 
 YKM  YUM 
"US" "OO"
----

d. Same thing as 8b, but interchanging the role of the destination and the airline.

[source,r]
----
tapply(bigDF$UniqueCarrier, bigDF$Dest, function(x) names(sort(table(x),decreasing=TRUE)[1]))
 ABE  ABI  ABQ  ABY  ACK  ACT  ACV  ACY  ADK  ADQ  AEX  AGS  AKN  ALB  ALO 
"US" "MQ" "WN" "EV" "XE" "MQ" "OO" "US" "AS" "AS" "EV" "DL" "AS" "US" "9E" 
 AMA  ANC  ANI  APF  ASE  ATL  ATW  AUS  AVL  AVP  AZO  BDL  BET  BFF  BFI 
"WN" "AS" "AS" "EV" "OO" "DL" "OO" "WN" "US" "US" "NW" "US" "AS" "OO" "AS" 
 BFL  BGM  BGR  BHM  BIL  BIS  BJI  BLI  BMI  BNA  BOI  BOS  BPT  BQK  BQN 
"OO" "US" "MQ" "WN" "NW" "NW" "9E" "US" "MQ" "WN" "WN" "US" "XE" "EV" "B6" 
 BRO  BRW  BTM  BTR  BTV  BUF  BUR  BWI  BZN  CAE  CAK  CBM  CCR  CDC  CDV 
"XE" "AS" "OO" "DL" "US" "US" "WN" "WN" "DL" "DL" "FL" "EV" "US" "OO" "AS" 
 CEC  CHA  CHO  CHS  CIC  CID  CKB  CLD  CLE  CLL  CLT  CMH  CMI  CMX  COD 
"OO" "DL" "EV" "DL" "OO" "TW" "OH" "OO" "CO" "MQ" "US" "US" "MQ" "9E" "OO" 
 COS  CPR  CRP  CRW  CSG  CVG  CWA  CYS  DAB  DAL  DAY  DBQ  DCA  DEN  DET 
"UA" "OO" "WN" "US" "EV" "DL" "OO" "OO" "DL" "WN" "US" "MQ" "US" "UA" "WN" 
 DFW  DHN  DLG  DLH  DRO  DSM  DTW  DUT  EAU  EFD  EGE  EKO  ELM  ELP  ERI 
"AA" "EV" "AS" "NW" "YV" "UA" "NW" "AS" "NW" "XE" "AA" "OO" "US" "WN" "US" 
 EUG  EVV  EWN  EWR  EYW  FAI  FAR  FAT  FAY  FCA  FLG  FLL  FLO  FMN  FNT 
"UA" "MQ" "EV" "CO" "EV" "AS" "NW" "OO" "US" "DL" "HP" "DL" "EV" "OO" "NW" 
 FOE  FSD  FSM  FWA  GCC  GCN  GEG  GFK  GGG  GJT  GLH  GNV  GPT  GRB  GRK 
"UA" "NW" "MQ" "MQ" "YV" "HP" "WN" "NW" "MQ" "OO" "9E" "EV" "EV" "NW" "MQ" 
 GRR  GSO  GSP  GST  GTF  GTR  GUC  GUM  HDN  HHH  HKY  HLN  HNL  HOU  HPN 
"NW" "US" "DL" "AS" "NW" "EV" "YV" "CO" "AA" "EV" "EV" "DL" "HA" "WN" "UA" 
 HRL  HSV  HTS  HVN  IAD  IAH  ICT  IDA  ILE  ILG  ILM  IND  INL  IPL  ISO 
"WN" "DL" "US" "UA" "UA" "CO" "UA" "OO" "MQ" "EV" "US" "US" "9E" "OO" "PI" 
 ISP  ITH  ITO  IYK  JAC  JAN  JAX  JFK  JNU  KOA  KSM  KTN  LAN  LAR  LAS 
"WN" "US" "HA" "OO" "DL" "DL" "US" "B6" "AS" "HA" "AS" "AS" "NW" "OO" "WN" 
 LAW  LAX  LBB  LBF  LCH  LEX  LFT  LGA  LGB  LIH  LIT  LMT  LNK  LNY  LRD 
"MQ" "UA" "WN" "OO" "XE" "DL" "XE" "US" "B6" "HA" "WN" "OO" "UA" "HA" "MQ" 
 LSE  LWB  LWS  LYH  MAF  MAZ  MBS  MCI  MCN  MCO  MDT  MDW  MEI  MEM  MFE 
"NW" "EV" "OO" "EV" "WN" "MQ" "NW" "WN" "EV" "DL" "US" "WN" "EV" "NW" "CO" 
 MFR  MGM  MHT  MIA  MIB  MKC  MKE  MKG  MKK  MLB  MLI  MLU  MOB  MOD  MOT 
"OO" "DL" "WN" "AA" "NW" "AA" "NW" "OO" "HA" "DL" "TW" "DL" "DL" "OO" "NW" 
 MQT  MRY  MSN  MSO  MSP  MSY  MTH  MTJ  MYR  OAJ  OAK  OGD  OGG  OKC  OMA 
"MQ" "OO" "NW" "OO" "NW" "WN" "EV" "OO" "US" "US" "WN" "OO" "HA" "WN" "UA" 
 OME  ONT  ORD  ORF  ORH  OTH  OTZ  OXR  PBI  PDX  PFN  PHF  PHL  PHX  PIA 
"AS" "WN" "UA" "US" "US" "OO" "AS" "OO" "DL" "AS" "EV" "FL" "US" "HP" "MQ" 
 PIE  PIH  PIR  PIT  PLN  PMD  PNS  PSC  PSE  PSG  PSP  PUB  PVD  PVU  PWM 
"TZ" "OO" "9E" "US" "9E" "HP" "DL" "DL" "B6" "AS" "OO" "HP" "US" "OO" "DL" 
 RAP  RCA  RDD  RDM  RDR  RDU  RFD  RHI  RIC  RKS  RNO  ROA  ROC  ROP  ROR 
"NW" "OO" "OO" "OO" "NW" "AA" "OO" "9E" "US" "YV" "WN" "US" "US" "CO" "CO" 
 ROW  RST  RSW  SAN  SAT  SAV  SBA  SBN  SBP  SCC  SCE  SCK  SDF  SEA  SFO 
"MQ" "NW" "DL" "WN" "WN" "DL" "OO" "NW" "OO" "AS" "OH" "US" "WN" "AS" "UA" 
 SGF  SGU  SHV  SIT  SJC  SJT  SJU  SKA  SLC  SLE  SMF  SMX  SNA  SOP  SPI 
"MQ" "OO" "DL" "AS" "WN" "MQ" "AA" "OO" "DL" "OO" "WN" "OO" "AA" "EV" "OO" 
 SPN  SPS  SRQ  STL  STT  STX  SUN  SUX  SWF  SYR  TEX  TLH  TOL  TPA  TRI 
"CO" "MQ" "DL" "TW" "AA" "AA" "OO" "UA" "AA" "US" "YV" "DL" "US" "US" "US" 
 TTN  TUL  TUP  TUS  TVC  TVL  TWF  TXK  TYR  TYS  UCA  VCT  VIS  VLD  VPS 
"OH" "WN" "EV" "AA" "NW" "AA" "OO" "MQ" "MQ" "DL" "US" "OO" "OO" "EV" "NW" 
 WRG  WYS  XNA  YAK  YAP  YKM  YUM 
"AS" "OO" "MQ" "AS" "CO" "US" "OO"
----


Question 9.

a. If we classify flights by their distance (e.g., 0 to 500 miles; 500 to 1000 miles; 1000 to 1500 miles; etc.), which classification of flights have the longest delays, on average? This will give us some information about whether shorter or longer flights have a longer average delay.

b. If we classify flights by their departure time (e.g., before 6 AM; 6 AM to 12 noon; 12 noon to 6 PM; 6 PM to 12 midnight), which classification of flights have the longest delays, on average? This will give us some information about whether it is preferable to depart earlier or later in the day.

Solution:

a. We look at the departure delays, split into groups according to how long the flight will be, and we take an average within each group. We discover that the flights that are less than 500 miles are delayed only 7.42 minutes, on average.

[source,r]
----
tapply(bigDF$DepDelay, cut(bigDF$Distance, breaks=seq(from=0,to=max(bigDF$Distance,na.rm=T),by=500)), mean, na.rm=TRUE)
        (0,500]     (500,1e+03] (1e+03,1.5e+03] (1.5e+03,2e+03] 
       7.422427        8.621311        8.945828        9.405297 
(2e+03,2.5e+03] (2.5e+03,3e+03] (3e+03,3.5e+03] (3.5e+03,4e+03] 
       9.423052        8.891673       11.272760        8.145777 
(4e+03,4.5e+03] 
       9.727186
----

b. Similar idea, but we now split according to the scheduled departure time. We see that the flights that depart before 6 AM are delayed only 4.21 minutes, on average.

[source,r]
----
tapply(bigDF$DepDelay, cut(bigDF$CRSDepTime, breaks=seq(from=0,to=2400,by=600), include.lowest=TRUE), mean, na.rm=TRUE)
          [0,600]     (600,1.2e+03] (1.2e+03,1.8e+03] (1.8e+03,2.4e+03] 
         4.213356          4.437826          9.635142         12.798243
----


Question 10.

a. Write a function that takes two airports as inputs and finds the number of flights from the first airport to the second airport (you can call it numflightsfunc).

b. Try your function from 10a on a pair of airpots, e.g., flights from IND to ORD.

c. Write a "most popular destination function” (you can call it mostpopfunc) that takes a group of airports as the input and finds which of them is the most popular destination, i.e., which airport has the most arrivals.

d. Try your function from 10c on 3 popular airports, e.g., JFK, ORD, and LAX, to see which of these 3 airports is the most popular destination.

Solution:

a. To build the function, we find the number of flights with Origin city that matches parameter 1 (origincity), and that simultaneously have Destination city that matches parameter 2 (destcity). We use sum to count the number of such flights.

[source,r]
----
numflightsfunc <- function(origincity, destcity) {
  return(sum((bigDF$Origin == origincity) &amp; (bigDF$Dest == destcity), na.rm=TRUE))
}
----

b. Now we test this function with IND as Origin and ORD as destination.

[source,r]
----
numflightsfunc("IND", "ORD")
[1] 80498
----

c. We extract the destinations that are in the given list of cities. Then we tabulate the results, sort them, and print the largest.

[source,r]
----
mostpopfunc <- function(cities) {
  return( sort(table(bigDF$Dest[bigDF$Dest %in% cities]),decreasing=TRUE)[1] )
}
----

d. Of these three cities, ORD is the most popular destination.

[source,r]
----
mostpopfunc( c("JFK", "ORD", "LAX") )
    ORD 
6638035
----



== Project 4

Question 1.

Check out the website http://www.ibm.com/manyeyes[Many Eyes] (sponsored by IBM). Find 3 (or more) separate plots on Many Eyes (please give links to each of these plots) that violate the concepts of effective data visualization that are discussed in the handouts from class (e.g., in Cleveland’s book and Robbins’s book, and in the paper "How to display data badly”). Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved. Imagine, for instance, that you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data. (Your discussion of these 3 plots should be about 1/3 of a page per plot, i.e., about 1 page altogether; more than 1 page altogether is certainly allowed.) Each student should write about at least 1 plot.

Question 2.

Revisit the website http://www.ibm.com/manyeyes[Many Eyes]. Find 3 (or more) separate plots (again, with links to the plots) on Many Eyes that do an overall good job of effective data visualization. Justify the reasons why you think that the plots are effective. (Again, please write at least 1/3 of a page for each plot, i.e., one page total, justifying the reasons that you think each plot is effective.) Each student should write about at least 1 plot.

Question 3.

Check out the website http://www.informationisbeautiful.net[Information Is Beautiful]. Find 3 (or more) separate plots on Information Is Beautiful (please give links to each of these plots) that violate the concepts of effective data visualization. Write a paragraph about each plot, with a critique of what aspects of the plotting could be improved. Imagine you were going to correspond with the people who designed the plot, and give them guidance about how to make a more effective depiction of the data. Your constructive criticism should be at least 1/3 of a page per plot, i.e., at least 1 page altogether.

Question 4.

The http://www.gapminder.org/world[Wealth and Health of Nations] is a fun depiction of data. On the other hand, as with many depictions of data, it violates some of the techniques of effective data display. Please write an explanation of which techniques of effective data display are violated. If you imagine you are writing a constructive criticism to the authors of this animation, please make suggestions for how the depiction of data (for the health and wealth, over the years displayed) could have been done more effectively. Please make sure your explanation is at least 1 page long.

Question 5.

Describe (at least!) 3 very significant ways that the poster winner `Congestion in the sky` (http://stat-computing.org/dataexpo/2009/posters/[from the Data Expo 2009 poster competition results]) could be significantly improved, using the concepts of effective data visualization. Write a constructive criticism (of at least 1 page) that gives suggestions for improvement on each aspect that you criticize.

Question 6.

For the other posters (do not use the winner, `Congestion in the sky`, since it was discussed already in question 5), find a total of at least 3 significant ways that some of the other posters can be improved. You can analyze several different posters, that is OK. Your constructive critique should be at least 1 page.

Question 7.

Which of the posters in the Data Expo 2009 do you think should be the winner? Why? (It is OK if you choose the poster that actually won, or any of the other posters.) Thoroughly justify your answer, using the techniques of effective data visualization, to justify your answer, with an explanation that is at least 1 page long.

Questions 8, 9, 10

Imagine that you are going to enter the Data Expo 2009. Rather than having to organize your information into a poster, prepare 3 pages of analysis, exploring some aspects of the airline data set that are interesting to you, and which you think might be of broad interest to potential readers too. Your discussion and plots should be at least 3 pages long.


== Project 5

Question 1.

a. Load the airplane data from 2008. Make a new data frame that contains only the 15th, 16th, and 19th columns, i.e., the ArrDelay, DepDelay, and the Distance, and that only contains every 1000th row of the original data frame, i.e., it contains the 1st row, 1001st row, 2001st row, etc. (You can either index the columns by the numbers 15, 16, 19, or by the names of the columns; it is worthwhile to make sure that you know how to do this both ways.)

b. Read the help documentation for the "pairs” function (which generates scatterplot matrices) and take a look at the examples at the end of the "pairs” documentation.

c. Use the pairs function to build a scatterplot of the data frame that you built in 1a.

d. Which two of the three variables (ArrDelay, DepDelay, and Distance) do you think are most correlated? Why?

Solution:

Loading the 2008 airline data

`bigDF <- read.csv("/data/public/dataexpo2009/2008.csv")`

a.  We build a data frame from row 1, row 1001, row 2001, etc.

`smallDF <- bigDF[seq(1,dim(bigDF)[1],by=1000), ]`

1b.  The documentation for the pairs data is given by:

[source,r]
----
{r, eval=FALSE}
?pairs
----

c.  The pairs functions can be used to build a scatterplot:

`pairs(smallDF[ ,c(15,16,19)])`

1d.  The ArrDelay and DepDelay seem to be the most corrleated.



Question 2

a. Using Google and the help utility in RStudio, install the package called `ggmap`

b. Using Google and the help utility in RStudio, load ggmap into the R environment.

c. Create a map containing all of Europe.

d. Create a map containing the United States (excluding Hawaii and Alaska).

e. Map the points of each business from the business_frame (in the business.Rda from the Yelp Dataset Challenge) on the USA map.

f. Map only the Illinois businesses from business_frame on the USA map.

g. Repeat 2e, but this time make the points for each business be equal to the size of the square root of the number of review counts for that business.

Solution:

a.  We first install the ggmap package.

[source,r]
----
{r, eval=FALSE}
install.packages(ggmap)
----

b.  Now we load ggmap into the R environment.

`library(ggmap)`

c.  Next we create a map of Europe

[source,r]
----
europe <- get_map(location = "Europe", zoom = 4)
ggmap(europe)
----

d.  and a map of the USA

[source,r]
----
usa <- get_map(location = "United States", zoom = 4)
ggmap(usa)
----

e.  Next we load the data from the business.Rda data set, which we assume is already in the current directory.

`load("business.Rda")`

and we map the locations of the businesses on the USA map

[source,r]
----
allpoints <- ggmap(usa) + geom_point(aes(x = longitude, y = latitude),
                                     data = business_frame, alpha = .5)
allpoints
----

f.  Now we do the same thing, but for the Illinois businesses:

[source,r]
----
illpoints <- ggmap(usa) + geom_point(aes(x = longitude[state == "IL"],
            y = latitude[state == "IL"]),
            data = business_frame[which(business_frame$state == "IL"), ],
            alpha = .5)
illpoints
----

g.  Finally, we repeat 2e but we make the size of the points equal to the square roots of the number of review counts for each business.

[source,r]
----
allpointsscaled <- ggmap(usa) + geom_point(aes(x = longitude, y = latitude,
                                size = sqrt(review_count)),
                                data = business_frame, alpha = .5)
allpointsscaled
----


Question 3

a. Use ggmap to plot out the locations of the airports in the United States.

b. Add 5 lines to the USA map. Each line corresponds to one of the 5 most popular departure-to-arrival paths in the USA, as studied in Question 4 on Problem Set 3.

Solution:

### Question 3

ab.  First we load the airports data

`airports <- read.csv("airports.csv")`

Then we get the latitudes and longitudes of the origins and destinations

[source,r]
----
latitudes <- airports$lat
names(latitudes) <- airports$iata
originlat <- latitudes[c("SFO","LAX","LAX","LAS","PHX")]
destlat <- latitudes[c("LAX","SFO","LAS","LAX","LAX")]

longitudes <- airports$long
names(longitudes) <- airports$iata
originlong <- longitudes[c("SFO","LAX","LAX","LAS","PHX")]
destlong <- longitudes[c("LAX","SFO","LAS","LAX","LAX")]

popular = data.frame(originlat,destlat,originlong,destlong)
----

Finally, we make a USA map with these lines added (note that some of the lines are duplicated)

[source,r]
----
usa <- get_map(location = "United States", zoom = 4)

usa1 <- ggmap(usa, extent = "device") +
      geom_point(aes(x = long, y = lat), data = airports, alpha = .5) +
      geom_segment(aes(x = originlong, xend = destlong,
      y = originlat, yend = destlat), data = popular, colour = "black")

usa1
----


Question 4

a. The england_outcome data contains a lot of cool information about the outcomes of the crimes in the city of London. It shows the outcome of the crime, and the longitude and latitude. Whenever there is longitude and latitude, you should know that you can easily use ggmap to plot. For this question, however, please create a colored bar graph of the counts of the 20 different outcomes of the crimes denoted by the factor "V1”. What is the most common outcome (in non numeric form. i.e. 2 is "court case unable to proceed”).

b. Do the same thing (using ggplot) to similarly plot the crime_data crime types. What is the most common crime?

c. Stack the different types of crimes (like in b), and then put them side by side based on "Month”. Make an observation as time goes on.

d. Do the same for (a) like you did for (b) in (c). Make an observation.

e. As time goes on, what appears to change more, the outcomes of the crimes or the crimes?

Solution:

Again we assume that the england_crime.Rda and england_outcomes.Rda files are found in the working directory.

[source,r]
----
load("england_crime.Rda")
load("england_outcomes.Rda")
----

a.  The bar graph is:

`ggplot(england_outcomes, aes(V1, fill = V1)) + geom_bar()`

So the most common outcome is 

`(england_outcomes$Outcome.type[england_outcomes$V1 == "9"])[1]`

which corresponds to: Investigation complete; no suspect identified.

b.  The bar plot of the crime_data crime types is:

`ggplot(england_crime, aes(V1, fill = V1)) + geom_bar() `

The most common crime is

`(england_crime$Crime.type[england_crime$V1 == "7"])[1]`

which corresponds to:  Other theft.

c.  If we stack the different types of crimes, and put them side by side based on Month, we get

`ggplot(england_crime, aes(Month, fill = V1)) + geom_bar(position = "stack")`

d.  Making a similar kind of plot for the outcomes from part a yields:

`ggplot(england_outcomes, aes(Month, fill = V1)) + geom_bar(position = "stack")`

e.  The crimes themselves seem to change more.


Question 5

a. Use ggmap to get a map of London. Show the map.

b. Plot the crimes as points on the map you made in (a). Use zoom = 12.

c. Add color to (b).

d. Repeat (c) but limit to "Violent Crimes” and "Violent and Sexual offenses”.

Solution:

### Question 5

a.  The city of London is mapped as follows

[source,r]
----
london <- get_map("City of London", zoom = 12)
london <- ggmap(london)
london
----

b.  Now we add the crimes to the map

[source,r]
----
london <- get_map("City of London", zoom = 12)
map1 <- ggmap(london) + geom_point(aes(x = Longitude, y = Latitude),
                                   data = england_crime, alpha = .5)
map1
----

c.  Now we add color to the map

[source,r]
----
london <- get_map("City of London", zoom = 12)
map2 <- ggmap(london) + geom_point(aes(x = Longitude, y = Latitude,
                        color = V1), data = england_crime, alpha = .5)
map2
----

d.  We repeat the previous map, but this time with just violent crimes and violent and sexual offenses.

[source,r]
----
london <- get_map("City of London", zoom = 12)
subset <- as.data.frame(rbind(england_crime[which(england_crime$V1 == 4),],
                        england_crime[which(england_crime$V1 == 13),]))
map3 <- ggmap(london) + geom_point(aes(x = Longitude, y = Latitude, color = V1),
                                   data = subset, alpha = .2)
map3
----


Question 6

a. Plot a density map of the United States (zoom = 4) of airports.

b. Plot a density map of the United States with a color gradient where low is green and high is red.

c. On top of the map in (b), add points to the map that represent the airports. Size those points based on the "total” factor. The "total” factor is simply the frequency of inbound and outbound flights.

Solution:


### Question 6

a.  This is a density map of the US airports

[source,r]
----
usa <- get_map(location = "United States", zoom = 4)
usa1 <- ggmap(usa) + geom_density2d(aes(x = long, y = lat), size = .3, data = airports)
usa1
----

b.  Now we make a density map with color gradient where low is green and high is red

[source,r]
----
usa <- get_map(location = "United States", zoom = 4)
usa1 <- ggmap(usa) + stat_density2d(aes(x = long, y = lat, fill = ..level.., alpha = ..level..),
      size = .01, data = airports, bins = 15, geom = "polygon") +
      scale_fill_gradient(low = "green", high = "red")
usa1
----

c.  We add points to the map for airports, where the total is the frequency of inbound and outbound flights.

Here we just use the 2008 flight data, since these are the flights loaded earlier.  If you prefer to use all of the flights from all of the years, that is OK too.

[source,r]
----
airports$freq <- table(bigDF$Dest)[as.character(airports$iata)]
                 + table(bigDF$Origin)[as.character(airports$iata)]

usa <- get_map(location = "United States", zoom = 4)
usa1 <- ggmap(usa) + stat_density2d(aes(x = long, y = lat, fill = ..level.., alpha = ..level..),
      size = .01, data = airports, bins = 15, geom = "polygon") +
      scale_fill_gradient(low = "green", high = "red") + 
      geom_point(aes(x = long, y = lat, size = freq),
                    data = airports, alpha = .5)
usa1
----



Question 7

Generate the first 20 Lucas numbers and store them in a vector. You can either use recursion or an explicit formula. If you are able to do both, which way is faster? How much faster?

https://en.wikipedia.org/wiki/Lucas_number

Solution:

To use an explicit formula for the Lucas numbers, we can write:

[source,r]
----
startingtime <- proc.time()
sapply(0:19, function(n) {
    ((1+sqrt(5))/2)^n + ((1-sqrt(5))/2)^n
} )
stoppingtime <- proc.time()
stoppingtime - startingtime
----
```
To use a recursive formula for the Lucas numbers, we can write:

[source,r]
----
startingtime <- proc.time()
L <- function(n) {
  returnval <- 0
  if (n == 0)
    returnval <- 2
  if (n == 1)
    returnval <- 1
  if (n > 1)
    returnval <- L(n-1) + L(n-2)
  returnval
}
sapply(0:19, L)
stoppingtime <- proc.time()
stoppingtime - startingtime
----

The explicit formula is much faster.



Question 8

a. Create a data frame called random_vars where:

the first column contains 10000 Bernoulli random variables, each with p=1/3.

the second column contains 10000 Binomial random variables, each with n=5 and p=1/3.

the third column contains 10000 Geometric random variables, each with expected value 3.

the fourth column contains 10000 Negative Binomial random variables, each of which is a sum of 5 Geometric random variables, and each of those Geometric random variables has expected value 3.

the fifth column contains 10000 Poisson random variables, each with expected value 3.

the sixth column contains 10000 Hypergeometric random variables, each with parameters N=20, M=5, and n=3 (using the notation from STAT/MA 41600).

the seventh column contains 10000 continuous Uniform random variables, each with min 5 and max 10.

the eighth column contains 10000 discrete Uniform random variables, each with min 5 and max 10.

the ninth column contains 10000 Exponential random variables, each with expected value 3.

the tenth column contains 10000 Gamma random variables, each with lambda = 3 and r = 5 (using the notation from STAT/MA 41600).

the eleventh column contains 10000 Beta random variables, each with alpha = 3 and beta = 8 (using the notation from STAT/MA 41600).

the twelveth vector contains 10000 Normal random variables with mean = 3 and variance = 5

b. Find the mean and variance of each column. (Do this efficiently, i.e., do not write 12 separate lines of code.)

Solution:

### Question 8

a.  The required data frame is

[source,r]
----
myDF <- data.frame(
  mybern   = rbinom(10000,1,1/3),
  mybinom  = rbinom(10000,5,1/3),
  mygeom   = rgeom(10000,1/3) + 1,    # note that we need to add 1 to the geoms
  mynegbin = rnbinom(10000,5,1/3) + 5,    # note that we need to add 5 to each negbinom
  mypois   = rpois(10000,3),
  myhyper  = rhyper(10000,5,15,3),    # note that we use our M, N-M, n
  mycunif  = runif(10000,5,10),
  mydunif  = floor(runif(10000,5,11)),    # note that we round down so the max is 11
  myexp    = rexp(10000,1/3),
  mygamma  = rgamma(10000,5,3),
  mybeta   = rbeta(10000,3,8),
  mynorm   = rnorm(10000,3,sqrt(5))
)
----

b.  The mean and variance of each column can be computed, for instance, as follows:

[source,r]
----
sapply(1:12, function(x) mean(myDF[[x]]))
sapply(1:12, function(x) var(myDF[[x]]))
----



== Project 6

Question 1.

a.  In the DataFest 2015 file for the visitors to Edmunds.com, found in:

`/data/public/datafest2015/visitor.csv`

identify the zip codes that had 800 or more visitors.
It is OK to ignore the blank and undefined zip codes in your answer.

b.  There should be 4 such zip codes in part a. For each of these 4 zip codes, identify the city corresponding to that zip code. (Hint: The cities are listed in the field called `dma_name`.)

c.  If we focus only on the cities directly (ignoring the zip codes, and only using the `dma_name` field), what are the 20 most popular cities?

Solution:

a.  In the DataFest 2015 file for the visitors to Edmunds.com, found in:

`/data/public/datafest2015/visitors.csv`

identify the zip codes that had 800 or more visitors.
It is OK to ignore the blank and undefined zip codes in your answer.

[source,bash]
----
awk -F, 'BEGIN{ } { print $197 } END{ }' visitor.csv | sort | uniq -c | sort -n

  count   zip
    822 90011
    892 95051
    893 94043
   1192 90404
----

b.  There should be 4 such zip codes in part a.  For each of these 4 zip codes, identify the city corresponding to that zip code.  (Hint: The cities are listed in the field called dma_name.)

One line solution:

[source,bash]
----
awk -F, 'BEGIN{ } { if ($197 ~ /90011|95051|94043|90404/) print $197, $198 } END{ }' visitor.csv | sort | uniq -c | sort -n
   822 90011 LOS ANGELES
   892 95051 SAN FRANCISCO-OAK-SAN JOSE
   893 94043 SAN FRANCISCO-OAK-SAN JOSE
  1192 90404 LOS ANGELES
----

Four line solution:

[source,bash]
----
awk -F, 'BEGIN{ } { if ($197 == "90011") print $198 } END{ }' visitor.csv | sort | uniq -c | sort -n
   822 LOS ANGELES
awk -F, 'BEGIN{ } { if ($197 == "95051") print $198 } END{ }' visitor.csv | sort | uniq -c | sort -n
   892 SAN FRANCISCO-OAK-SAN JOSE
awk -F, 'BEGIN{ } { if ($197 == "94043") print $198 } END{ }' visitor.csv | sort | uniq -c | sort -n
   893 SAN FRANCISCO-OAK-SAN JOSE
awk -F, 'BEGIN{ } { if ($197 == "90404") print $198 } END{ }' visitor.csv | sort | uniq -c | sort -n
  1192 LOS ANGELES
----

c.  If we focus only on the cities directly (ignoring the zip codes, and only using the dma_name field), what are the 20 most popular cities?

[source,bash]
----
awk -F, 'BEGIN{ } { print $198 } END{ }' visitor.csv | sort | uniq -c | sort -n | tail -n21

  6648 MINNEAPOLIS-ST. PAUL
  8211 ORLANDO-DAYTONA BCH-MELBRN
  8271 SACRAMNTO-STKTON-MODESTO
  8651 DENVER
  8875 SAN DIEGO
  9501 TAMPA-ST. PETE (SARASOTA)
  9608 PHOENIX (PRESCOTT)
 11279 SEATTLE-TACOMA
 11797 MIAMI-FT. LAUDERDALE
 11916 BALTIMORE
 15808 DALLAS-FT. WORTH
 15975 ATLANTA
 19498 HOUSTON
 20691 BOSTON (MANCHESTER)
 24117 PHILADELPHIA
 25949 WASHINGTON. DC (HAGRSTWN)
 26919 CHICAGO
 36902 SAN FRANCISCO-OAK-SAN JOSE
 57513 NEW YORK
 60296 LOS ANGELES
144505
----


Question 2

If we study the `first_device_model` and `last_device_model` fields, we can discover the first and last devices that the visitors used during their visit to the site.  (Sometimes visitors change platforms while they are visiting, e.g., if they continue a search that they began on an earlier device.)

a.  Categorize the types of `first_device_model` used, according to how many times each device was used.  Sort your resulting list according to the number of each kind of device.

b.  Same question, for `last_device_model` used.

c.  Now consider only the people who switched devices.  Ignoring blank entries, and ignoring "other" entries, what was the most common device switch?

Solution:

If we study the first_device_model and last_device_model fields, we can discover the first and last devices that the visitors used during their visit to the site.  (Sometimes visitors change platforms while they are visiting, e.g., if they continue a search that they began on an earlier device.)

a.  Categorize the types of first_device_model used, according to how many times each device was used.  Sort your resulting list according to the number of each kind of device.

[source,bash]
----
awk -F, 'BEGIN{ } { print $188 } END{ }' visitor.csv | sort | uniq -c | sort -n
     1 first_device_model
    27 Blackberry
    67 Windows Phone
  1362 Kindle Fire
  3372 Windows Tablet
  4591 iPhone
 12385 Android
 15744 other
 79809 iPad
 87504
106753 Mac
413317 Windows PC
----

b.  Same question, for last_device_model used.

[source,bash]
----
awk -F, 'BEGIN{ } { print $189 } END{ }' visitor.csv | sort | uniq -c | sort -n
     1 last_device_model
    30 Blackberry
    75 Windows Phone
  1180 Kindle Fire
  3258 Windows Tablet
  4872 iPhone
 13401 Android
 18049 other
 37312
 89879 iPad
118416 Mac
438459 Windows PC
----

c.  Now consider only the people who switched devices.  Ignoring blank entries, and ignoring "other" entries, what was the most common device switch?

[source,bash]
----
awk -F, 'BEGIN{ } { if ($188 != $189) print $188, $189 } END{ }' visitor.csv | sort | uniq -c | sort -n | tail
   510 Windows Tablet Windows PC
----


Question 3

Now consider the `shopping.csv` file.

a.  What are the ten most popular makes of cars that people shopped for?

b.  If we consider both the make and the model of the car, what are the ten most popular make-and-model pairs of cars?

c.  What are all the models that Toyota sells?

d.  For this question (only), consider instead the leads.csv file, and identify the top 10 makes of cars for which there is a lead.  Please note that the cars do not have a uniform capitalization, so it is necessary for you to standardize the capitalization before you make your tally.

Solution:

Now consider the shopping.csv file.

a.  What are the ten most popular makes of cars that people shopped for?

[source,bash]
----
awk -F, 'BEGIN{ } { print $3 } END{ }' shopping.csv | sort | uniq -c | sort -n | tail
 76919 Mercedes-Benz
 90482 Chevrolet
100163 Mazda
101158 BMW
102170 Hyundai
116566 Subaru
126890 Nissan
157486 Ford
275181 Honda
289938 Toyota
----

b.  If we consider both the make and the model of the car, what are the ten most popular make-and-model pairs of cars?

[source,bash]
----
awk -F, 'BEGIN{ } { print $3, $4 } END{ }' shopping.csv | sort | uniq -c | sort -n | tail
 31988 Subaru Outback
 33166 Mazda CX-5
 33378 Toyota Camry
 34287 Mazda Mazda3
 34600 Subaru Forester
 35260 Toyota RAV4
 42118 Honda Civic
 44760 Toyota Highlander
 68623 Honda Accord
 68832 Honda CR-V
----

c.  What are all the models that Toyota sells?

[source,bash]
----
awk -F, 'BEGIN{ } { if ($3 == "Toyota") print $4 } END{ }' shopping.csv | sort | uniq -c | sort -n
     1 Celica
     8 Camry Solara
   547 Matrix
  1221 RAV4 EV
  1387 Land Cruiser
  2718 Yaris
  3481 Highlander Hybrid
  3644 Sequoia
  3654 FJ Cruiser
  4075 Prius Plug-in
  5661 Avalon Hybrid
  6019 Venza
  7092 Prius v
  8510 Prius c
  9552 Tundra
 10368 Camry Hybrid
 11698 Avalon
 12469 4Runner
 18896 Prius
 19331 Tacoma
 19541 Sienna
 26667 Corolla
 33378 Camry
 35260 RAV4
 44760 Highlander
----

d.  For this question (only), consider instead the leads.csv file, and identify the top 10 makes of cars for which there is a lead.  Please note that the cars do not have a uniform capitalization, so it is necessary for you to standardize the capitalization before you make your tally.

[source,bash]
----
awk -F, 'BEGIN{ } { print tolower($5) } END{ }' leads.csv | sort | uniq -c | sort -n | tail
 82654 jeep
 91612 bmw
 95021 chevrolet
 98358 mazda
105005 hyundai
115203 subaru
141438 nissan
169958 ford
395879 toyota
406738 honda
----


Question 4

a.  If we classify the click dates in the shopping file, how many shopping entries were made per year?

b.  Now consider only the year and the month but not the day.  How many shopping entries were made per each year-and-month pair?

c.  During which year-and-month pair were the most shopping entries made?

Solution:

a.  If we classify the click dates in the shopping file, how many shopping entries were made per year?

[source,bash]
----
awk -F, 'BEGIN{ } { print $2 } END{ }' shopping.csv | awk -F- 'BEGIN{ } { print $1 } END{ }' | sort | uniq -c
  30328 2013
1792586 2014
 287246 2015
      1 click_date
----

b.  Now consider only the year and the month but not the day.  How many shopping entries were made per each year-and-month pair?

[source,bash]
----
awk -F, 'BEGIN{ } { print $2 } END{ }' shopping.csv | awk -F- 'BEGIN{ } { print $1, $2 } END{ }' | sort | uniq -c
 30328 2013 12
122958 2014 01
116435 2014 02
134709 2014 03
117080 2014 04
127792 2014 05
147894 2014 06
172329 2014 07
192786 2014 08
172518 2014 09
171845 2014 10
145645 2014 11
170595 2014 12
153998 2015 01
127347 2015 02
  5901 2015 03
     1 click_date
----

c.  During which year-and-month pair were the most shopping entries made?

[source,bash]
----
awk -F, 'BEGIN{ } { print $2 } END{ }' shopping.csv | awk -F- 'BEGIN{ } { print $1, $2 } END{ }' | sort | uniq -c | sort -n | tail -n1
192786 2014 08
----


Question 5

a.  Back to the visitor file, if you look at the first_referring_url, what are the top 10 URL's that people used when first getting a reference to the site?  (It is necessary to only use the first part of an address, e.g., to only use "www.google.com" for example, and to trim the rest of the URL off.)

b.  When people are actually making the purchase, which model year do they tend to buy?  Use the `transactions.csv` to rank the years according to how many cars were bought in that model year.

c.  Same question, but this time, limit yourself to the 381 cars bought in Indiana (abbreviation IN)

d.  For which colors were there 800 or more cars sold with that color?

e.  How many car colors have the word "blue" in the title?

Solution:

a.  Back to the visitor file, if you look at the first_referring_url, what are the top 10 URL's that people used when first getting a reference to the site?  (It is necessary to only use the first part of an address, e.g., to only use "www.google.com" for example, and to trim the rest of the URL off.)

[source,bash]
----
awk -F, 'BEGIN{ } { print $190 } END{ }' visitor.csv | awk -F/ 'BEGIN{ } { print $3 } END{ }' | sort | uniq -c | sort -n | tail -n11
  2855 www.swagbucks.com
  5300 search.aol.com
  5616 www.newcar-leases.com
  7342 c.autoaffiliatenetwork.com
  7377 search.yahoo.com
  8303 www.googleadservices.com
  9769 www.edmunds.com
 24433 r.search.yahoo.com
 36131 www.bing.com
250074
314811 www.google.com
----

b.  When people are actually making the purchase, which model year do they tend to buy?  Use the transactions.csv to rank the years according to how many cars were bought in that model year.

[source,bash]
----
awk -F, 'BEGIN{ } { print $9 } END{ }' transactions.csv | sort | uniq -c | sort -n      1 1980
     1 1991
     1 1993
     1 model_year_bought
     3 1983
     3 1995
     6 1996
    10 1994
    10 1997
    11 1990
    11 2016
    18 1998
    20 1999
    31 2000
    62 2001
    89 2002
   125 2003
   180 2004
   263 2005
   289 2006
   488 2009
   527 2007
   611 2008
  1056 2010
  3071 2011
  3901 2012
  6900 2013
 32854 2015
 57290 2014
----

c.  Same question, but this time, limit yourself to the 381 cars bought in Indiana (abbreviation IN)

[source,bash]
----
awk -F, 'BEGIN{ } { if ($8 == "IN") print $9 } END{ }' transactions.csv | sort | uniq -c | sort -n
     1 2003
     1 2005
     2 2004
     2 2007
     4 2008
     7 2009
     7 2010
    13 2012
    15 2011
    33 2013
    96 2015
   200 2014
----

d.  For which colors were there 800 or more cars sold with that color?

[source,bash]
----
awk -F, 'BEGIN{ } { print $15 } END{ }' transactions.csv | sort | uniq -c | sort -n | tail
   840 ICE SILVER METALLIC
  1180 MODERN STEEL METALLIC
  1628 CRYSTAL BLACK PEARL
  3132 BLUE
  3190 RED
  4901
  5574 SILVER
  7297 GRAY
  7958 WHITE
  9620 BLACK
----

e.  How many car colors have the word "blue" in the title?

[source,bash]
----
awk -F, 'BEGIN{ } { print $15 } END{ }' transactions.csv | sort | uniq -c | sort -n | grep "BLUE" | wc
   316    1138    7741
----


Question 6

Consider the file yow.lines, which is distributed with `emacs 21.4`. It can be downloaded from the llc server or you can access it directly from `/proj/www/2015/29000/projects/yow.lines` if you prefer.

a. Consider the number of fields on each line of the file.  What is the maximum number of fields on a line?

b. Print the lines that have at least 15 fields.

c. Do any lines contain the word pizza?  Print all such lines, regardless of how the word pizza is capitalized.

Solution:

Consider the file `yow.lines`, which is distributed with `emacs 21.4`. It can be downloaded from the llc server or you can access it directly from `/proj/www/2015/29000/projects/yow.lines` if you prefer.

a. Consider the number of fields on each line of the file. What is the maximum number of fields on a line?

`awk 'BEGIN{ } { print NF } END { }' /proj/www/2015/29000/projects/yow.lines | sort -n | tail -n1`

b. Print the lines that have at least 15 fields.

`awk 'BEGIN{ } { if (NF >= 15) print $0 } END { }' /proj/www/2015/29000/projects/yow.lines | sort -n`

c. Do any lines contain the word pizza?  Print all such lines, regardless of how the word pizza is capitalized.

`awk 'BEGIN{ } { if (tolower($0) ~ /pizza/) print $0 } END { }' /proj/www/2015/29000/projects/yow.lines`


Question 7

Consider the file `/usr/share/dict/words`

a.  How many words contain 2 or more consecutive vowels?

b.  How many words contain the pattern `n't` or the pattern `'ve` ?
(Hint:  Use '\'' for the single quote in your pattern.)

[source,bash]
----
'\''
----

c.  Print all of the words that contain 5 or more consecutive vowels.

d.  Print the following words from the file `/usr/share/dict/words`
The 1st word, the 10001st word, the 20001st word, the 30001st word, etc.  I.e., print every 10000th word, starting with the first word.  There should be 48 words in the resulting list.

Solution:

Consider the file `/usr/share/dict/words`

a.  How many words contain 2 or more consecutive vowels?

[source,bash]
----
awk 'BEGIN{ } { if (tolower($0) ~ /[aeiou][aeiou]/) print $0 } END{ }' /usr/share/dict/words | wc
179408  179408 2000804
----

b.  How many words contain the pattern n't or the pattern 've ?
(Hint:  Use '\'' for the single quote in your pattern.)

[source,bash]
----
awk 'BEGIN{ } { if ($0 ~ /n'\''t|'\''ve/) print $0 } END{ }' /usr/share/dict/words | wc
42      42     295
----

c.  Print all of the words that contain 5 or more consecutive vowels.

[source,bash]
----
awk -W posix 'BEGIN{ } { if (tolower($0) ~ /[aeiou]{5,}/) print $0 } END{ }' /usr/share/dict/words
AAAAAA
Aeaea
Aeaean
AIEEE
cadiueio
Chaouia
cooeeing
euouae
Guauaenok
miaoued
miaouing
Pauiie
queueing
----

d.  Print the following words from the file `/usr/share/dict/words`

The 1st word, the 10001st word, the 20001st word, the 30001st word, etc.  I.e., print every 10000th word, starting with the first word.  There should be 48 words in the resulting list.

`awk 'BEGIN{ } { if (NR % 10000 == 1) print $0 } END{ }' /usr/share/dict/words | wc`



Question 8

a.  Print the names of all of the files and directories in the `/etc` directory that were modified in the current month (i.e., in October 2015).

b.  Make a list of all the file names in the three directories:

[source,bash]
----
    /usr/local/bin
    /bin
    /usr/bin 
----

Then sort the list, remove any duplicate file names, and store the results in a file called `myprograms.txt`.

Solution:

a.  Print the names of all of the files and directories in the /etc directory that were modified in the current month (i.e., in October 2015).

(We use the colon in the 8th field because we want the year to be the current year.)

`ls -la /etc | awk 'BEGIN{ } { if (($6 == "Oct") && ($8 ~ /\:/)) print $0 } END{ }'`

b.  Make a list of all the file names in the three directories:

[source,bash]
----
    /usr/local/bin
    /bin
    /usr/bin 
----

Then sort the list, remove any duplicate file names, and store the results in a file called myprograms.txt.

`ls -la /usr/local/bin /bin /usr/bin | awk 'BEGIN{ } { print $9 } END{ }' | sort | uniq >myprograms.txt`


== Project 7

Question 1.

a.  In R, use the `system` function with the parameter `intern=TRUE` to solve question 1a from project 3.  Inside the system function, you can use any method from bash that you like.  The goal is to be able to solve this question relatively quickly, without having to import the complete file `allyears.csv` into R.

b.  In R, use the `pipe` function, wrapped inside the `read.csv` function, to solve question 1a in a different way, without using the `system` function.

c.  Use `system.time` to see which of these two methods is faster.  By the way, both methods should be MUCH faster than importing the entire `allyears.csv` file, as we naively did back in project 3.

Solution:

a. For the `DepTime`, there are 2302136 values that are `NA` values

`system("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | grep NA | wc")`

and there are 123534970 `DepTime` values altogether.

`system("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | wc")`

so the fraction of NA values is `2302136 / 123534970 = 0.0186355`

b. Now we use the pipe instead of the system functions.

[source,r]
----
dim(read.csv(pipe("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | grep NA"),header=F))[1]
dim(read.csv(pipe("cut -d, -f5 /data/public/dataexpo2009/allyears.csv"),header=F))[1]
----

We get the same results as in part 1a.

c. The times required for the 2 system calls in 1a are:

[source,r]
----
system.time(system("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | grep NA | wc"))
   user  system elapsed 
415.703   4.350 417.814 

system.time(system("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | wc"))
   user  system elapsed 
442.151   4.025 428.428
----

These times may vary, of course, depending on the load of system jobs at the time the calls are made.

The times required for the 2 system calls in 1b are:

[source,r]
----
system.time(dim(read.csv(pipe("cut -d, -f5 /data/public/dataexpo2009/allyears.csv | grep NA"),header=F))[1])
   user  system elapsed
422.942   4.194 423.715

system.time(dim(read.csv(pipe("cut -d, -f5 /data/public/dataexpo2009/allyears.csv"),header=F))[1])
   user  system elapsed 
484.114  34.339 463.846
----

The solutions in 1a, using the system function,
are just a little bit faster than the solutions in 1b, using the pipe function.

Similar answers to 1a, 1b, 1c can be found by just changing `-f5` to `-f7`,
if we want to analyze the `ArrTime` instead of the `DepTime`


Question 2.

See what is the quickest method that you can use to solve question 4c from project 3, using your knowledge of bash and/or awk tools, as well as the system or pipe functions in R.

Solution:

We extract fields 17 and 18 using the cut function, and then sort the output, and pipe it to the uniq function, printing the number of unique lines, along with the associated counts, and then we sort those counted lines into order, and we conclude by taking the greatest 5 such counts of origins and destinations

`system("cut -d, -f17,18 /data/public/dataexpo2009/allyears.csv | sort | uniq -c | sort -n | tail -n5")`

As a result, we get:

[source,bash]
----
279716 PHX,LAX
286328 LAS,LAX
292125 LAX,LAS
336938 LAX,SFO
338472 SFO,LAX
----


Question 3.

Solve questions 8a and 8c from project 3 again, using your knowledge of bash and/or awk tools, as well as the system or pipe functions in R.

Solution:

a.

[source,r]
----
originDF <- read.csv(pipe("cut -d, -f9,17 /data/public/dataexpo2009/1987.csv"),header=T)
v <- originDF$Origin
names(v) <- originDF$UniqueCarrier
tapply(v, names(v), function(x) {names(sort(table(x),decreasing=T)[1])} )
----

b.

[source,r]
----
w <- originDF$UniqueCarrier
names(w) <- originDF$Origin
tapply(w, names(w), function(x) {names(sort(table(x),decreasing=T)[1])} )
----


Question 4.

Use `awk` (and the `system` or `pipe` function in R) to solve question 1 from project 5 again.  How much faster is your solution, using these tools, as compared to the method you used from project 5?

Solution:

Everything can be done the same as before, but using:

[source,r]
----
system.time(smallDF <- read.csv(pipe("awk -F, 'BEGIN{ } { if(NR % 1000 == 1) {print $15, \",\", $16, \",\", $19} } END{ }' /data/public/dataexpo2009/2008.csv"),header=T))
dim(smallDF)
----

(This solution uses line 1 for the headers, as opposed to the solution from project 5, in which line 1 starts with the first line of the actual data.)

The time taken for this command is:

[source,bash]
----
user  system elapsed 
1.589   0.207   1.887
----

This is much, much faster than the code from Project 5.

The solution from Project 5 took the following time (and notice that we previously read into R all of the 2008 data).

[source,r]
----
system.time(bigDF <- read.csv("/data/public/dataexpo2009/2008.csv"))
  user  system elapsed 
90.696 116.722 228.198
system.time(smallDF <- bigDF[seq(1,dim(bigDF)[1],by=1000), ])
  user  system elapsed 
 0.048   0.002   0.050
dim(smallDF)
----


Question 5.

a.  Use `awk` to find the lengths of the lines in the `yow.lines` file, and then use R to make a plot of the distribution of the lengths.

b.  Is it faster to (a) use awk to find the lengths of the lines, and then import these lengths in R (instead of the whole lines themselves), or (b) is it faster use R to import all of the lines and find the lengths within R?

c.  Find the distribution of the words in the `/usr/share/dict/words` file, according to the starting character.  The letters should be treated as case insensitive.

d.  Use R to plot the distribution from part c.  Plot the letters in decreasing order, according to how many words start with those letters.

Solution:

a.

[source,r]
----
yowDF <- read.csv(pipe("awk -F=\"\\n\" 'BEGIN{ } { print length($1) } END{ }' yow.lines"),header=F)
plot(table(yowDF[[1]]))
table(yowDF[[1]])
----

b.
The solution in 5a used time:

[source,r]
----
system.time(yowDF <- read.csv(pipe("awk -F=\"\\n\" 'BEGIN{ } { print length($1) } END{ }' yow.lines"),header=F))
  user  system elapsed 
 0.004   0.047   0.050

system.time(filename <- "yow.lines")
system.time(con <- file(filename,open="r"))
system.time(v <- readLines(con))
system.time(table(sapply(v, nchar)))
----

Only the readLines takes any significant time, but it takes much longer in R than using awk:

[source,bash]
----
 user  system elapsed 
0.064   0.003   0.066
----

c.

[source,r]
----
wordsDF <- read.csv(pipe("awk 'BEGIN{FS=\"\" } { print tolower($1) } END{ }' /usr/share/dict/words"),header=F)
sort(table(wordsDF[[1]]),decreasing=T)
----

d.

[source,r]
----
v <- sort(table(wordsDF[[1]]),decreasing=T)
dotchart(v)
----



Question 6.

Working with the DataFest 2015 `visitor.csv` file, use question 1c from project 6 to make a dotchart in R of the twenty cities with the most entries, showing the number of entries per city.  Please put the data in the dotchart into numerical order, according to the number of entries for the city.

[source,r]
----
citiesDF <- read.csv(pipe("awk -F, 'BEGIN{ } { print $198 } END{ }' /data/public/datafest2015/visitor.csv | sort | uniq -c | sort -n | tail -n21"),header=F)
mylist <- strsplit(as.character(citiesDF[[1]][1:20]), "\\s+")
mycounts <- as.numeric(sapply(mylist, function(x) x[[2]]))
names(mycounts) <- as.character(citiesDF[[1]][1:20])
class(mycounts)
dotchart(mycounts)
----

Question 7.

For parts a, b, c, use `bash` or `awk` tools.

a.  The file `babynames.txt` has 134 years of data, with all of the baby names from 1880 to 2013.  Extract a list of all of the names (regardless of gender).

b.  Remove the duplicates from the list in part a.

c.  Count the number of (unique) names that remain, according to the length of the name.

d.  Finally, import the resulting distribution of lengths to R, and make a plot of the distribution of the number of names, according to the length of the name.

e.  Redo parts 7a through 7d using only R functions, without resorting to bash or awk.

f.  Which method was faster?  The method that blended bash/awk/R tools, or the method that used only tools from R?

Solution:

a.

`read.csv(pipe("awk 'BEGIN{ } { print $3; print $5 } END{ }' babynames.txt"),header=F)`

b.

`read.csv(pipe("awk 'BEGIN{ } { print $3; print $5 } END{ }' babynames.txt | sort | uniq"),header=F)`

c.

`lengthDF <- read.csv(pipe("awk 'BEGIN{ } { print $3; print $5 } END{ }' babynames.txt | sort | uniq | awk 'BEGIN{ } { print length($0) } END{ }' | sort -n | uniq -c | awk 'BEGIN{ } { print $1, \",\", $2 } END{ }'"),header=F)`

d.

`plot( lengthDF[[2]], lengthDF[[1]])`

e.

[source,r]
----
allnames <- read.delim("babynames.txt",header=F,sep=" ")
class(allnames)
plot(table(nchar(unique(c(as.character(allnames[[3]]), as.character(allnames[[5]]))))))
----

f. It is faster to use a blend of tools.


Question 8.

Make a list (in increasing order) of all of the integers from 1 to 1000000 whose prime factors are only 2's and/or 3's.  Hint: It might help to think cleverly and use an inner product, but you can do this in any way that you like.  Time your solution.  What is the fastest way that you can solve the problem?  Compare with your peers to see what kinds of solutions that they found, and how fast the solution worked.  [Hint: there are 142 such numbers, starting with 1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, ..., and ending with 995328.]

Solution:

[source,r]
----
?log
v <- (2^(0:log(1000000,base=2))) %o% (3^(0:log(1000000,base=3)))
sort(v[v<=1000000])
----


== Project 8

Question 1.

a.  How many pitchers have ever hit more than 20 home runs in one season?

b.  How many pitchers have ever hit a home run in their lifetime?

Solution:

a. all the years and players who hit more than 20 home runs in one season are generated by:

[source,sql]
----
SELECT b.yearID, m.nameFirst, m.nameLast, b.HR FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN Pitching p ON (p.yearID = b.yearID AND p.playerID = b.playerID) WHERE b.HR > 20;
----

There are 17 such players

[source,sql]
----
SELECT m.nameFirst, m.nameLast FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN Pitching p ON (p.yearID = b.yearID AND p.playerID = b.playerID) WHERE b.HR > 20 GROUP BY m.playerID;
----

b. There are 1599 such pitchers

[source,sql]
----
SELECT m.nameFirst, m.nameLast FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN Pitching p ON (p.yearID = b.yearID AND p.playerID = b.playerID) WHERE b.HR > 0 GROUP BY m.playerID;
----


Question 2.

a.  Which team has committed the most errors altogether?

b.  Which team has hit the most home runs altogether?

c.  Re-do questions 2a and 2b, limiting our attention to 2010 - present.

Solution:

a. The Cincinnati Reds have committed the most errors altogether: 22837 of them.

`SELECT t.name, SUM(t.E) FROM Teams t GROUP BY t.name ORDER BY SUM(t.E);`

b. The New York Yankees have hit the most home runs altogether: 14814 of them.

`SELECT t.name, SUM(t.HR) FROM Teams t GROUP BY t.name ORDER BY SUM(t.HR);`

c. When restricting attention to errors since 2010, the Chicago Cubs have the most, with 593 errors altogether.

`SELECT t.name, SUM(t.E) FROM Teams t WHERE t.yearID >= 2010 GROUP BY t.name ORDER BY SUM(t.E);`

When restricting attention to HR since 2010, the Toronto Blue Jays have the most, with 1003 HR altogether.

`SELECT t.name, SUM(t.HR) FROM Teams t WHERE t.yearID >= 2010 GROUP BY t.name ORDER BY SUM(t.HR);`



Question 3.

a.  Which batter has been hit by the most pitches overall?  How many times?

b.  Which pitcher has hit the most batters overall?  How many batters did he hit?

c.  What are the 10 overall wildest pitchers, according to the number of wild pitches they have thrown altogether?

Solution:

a. Hughie Jennings has been hit by the most pitches: 287 of them altogether.

`SELECT m.nameFirst, m.nameLast, SUM(b.HBP) FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID GROUP BY m.playerID ORDER BY SUM(b.HBP);`

b. Walter Johnson has hit the most batters overall: 203 of them altogether.

`SELECT m.nameFirst, m.nameLast, SUM(p.HBP) FROM Pitching p INNER JOIN Master m ON p.playerID = m.playerID GROUP BY m.playerID ORDER BY SUM(p.HBP);`

c. The wildest 10 pitchers are the last 10 in the query here; notice that there is a two-way tie for the 10th place.

`SELECT m.nameFirst, m.nameLast, SUM(p.WP) FROM Pitching p INNER JOIN Master m ON p.playerID = m.playerID GROUP BY m.playerID ORDER BY SUM(p.WP);`


Question 4.

a.  What is the most number of wins that a team (or teams) ever had in one year?  Which team(s) and year(s)?

b.  During such year(s) and team(s), who was the team's leader in home runs, during that year?

Solution:

a. The Seattle Mariners won 116 games in 2001, and the Chicago Cubs won 116 games in 1906.

`SELECT t.name, t.W, t.yearID FROM Teams t ORDER BY t.W;`

b. Bret Boone was the team leader in 2001 for the Seattle Mariners

`SELECT m.nameFirst, m.nameLast, b.HR FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN Teams t ON (t.yearID = b.yearID AND t.teamID = b.teamID) WHERE t.name = 'Seattle Mariners' AND t.yearID = '2001' ORDER BY b.HR;`

Frank Schulte was the team leader in 1906 for the Chicago Cubs

`SELECT m.nameFirst, m.nameLast, b.HR FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN Teams t ON (t.yearID = b.yearID AND t.teamID = b.teamID) WHERE t.name = 'Chicago Cubs' AND t.yearID = '1906' ORDER BY b.HR;`


Question 5.

Sum the number of home runs hit by players who studied in each university while they were in college. (E.g., for Purdue, consider how many home runs that all of the Purdue alums have ever hit, altogether.) Which universities are the ten best, in terms of yielding the players with the most home runs altogether?

Solution:

The ten best universities, in terms of their alumni hitting home runs, are the top ten in this list; note that there are several schools tied for 9th to 17th place.

`SELECT m.nameFirst, m.nameLast, SUM(b.HR), s.name_full FROM Batting b INNER JOIN Master m ON b.playerID = m.playerID INNER JOIN CollegePlaying c ON (c.yearID = b.yearID AND c.playerid = b.playerID) INNER JOIN Schools s ON s.schoolID = c.schoolID GROUP BY s.name_full ORDER BY SUM(b.HR);`


Question 6.

a.  Which manager's teams have had the most home runs overall?

b.  Which manager's teams have had the most stolen bases overall?

Solution:

a. Tony LaRussa's teams have had the most home runs overall.

`SELECT mng.playerID, m.nameFirst, m.nameLast, SUM(b.HR) FROM Batting b INNER JOIN Managers mng ON (b.yearID = mng.yearID AND b.teamID = mng.teamID) INNER JOIN Master m ON mng.playerID = m.playerID GROUP BY mng.playerID ORDER BY SUM(b.HR);`

b. John McGraw's teams have had the most stolen bases overall.

`SELECT mng.playerID, m.nameFirst, m.nameLast, SUM(b.SB) FROM Batting b INNER JOIN Managers mng ON (b.yearID = mng.yearID AND b.teamID = mng.teamID) INNER JOIN Master m ON mng.playerID = m.playerID GROUP BY mng.playerID ORDER BY SUM(b.SB);`


Question 7.

a.  How many players have played at least one game in each of all three outfield positions?

b.  Considering all positions (not just the outfield), which player has played in the greatest number of different positions altogether at least one time?

Solution:

a. There are 1486 players who have played at least one game in all three outfield positions.

`SELECT f.playerID, COUNT(DISTINCT(f.Pos)) FROM Fielding f WHERE (f.Pos = 'LF' OR f.Pos = 'CF' OR f.Pos = 'RF') GROUP BY f.playerID HAVING (COUNT(DISTINCT(f.Pos)) = 3) ORDER BY f.playerID;`

b. There are 7 players who have played in exactly 11 positions at least one time.


Question 8.

What percent of batters are lefties?  Switchhitters?

Solution:

There are 4814 left handed batters

`SELECT m.playerID, m.bats FROM Master m WHERE m.bats = 'L' GROUP BY m.playerID;`

and 11435 right handed batters

`SELECT m.playerID, m.bats FROM Master m WHERE m.bats = 'R' GROUP BY m.playerID;`

and 1150 switchhitters (i.e., bat with both hands)

`SELECT m.playerID, m.bats FROM Master m WHERE m.bats = 'B' GROUP BY m.playerID;`

so the percent of lefties is `4814/(4814 + 11435 + 1150) = 0.28`
and the percent of switchhitters is `1150/(4814 + 11435 + 1150) = 0.07`


Question 9.

a. Which team has the highest (i.e., worst) average number of errors per year?

b. Which team has the highest (i.e., best) average number of home runs per year?

Solution:

a. The Baltimore Monumentals had the worst average number of errors per year.

`SELECT t.name, SUM(t.E) / COUNT(DISTINCT(t.yearID)) FROM Teams t GROUP BY t.name ORDER BY SUM(t.E) / COUNT(DISTINCT(t.yearID));`

b. The Colorado Rockies had the best average number of home runs per year.

`SELECT t.name, SUM(t.HR) / COUNT(DISTINCT(t.yearID)) FROM Teams t GROUP BY t.name ORDER BY SUM(t.HR) / COUNT(DISTINCT(t.yearID));`


Question 10.

How many bases have been stolen in each year?  Plot the data.

Solution:

The bases stolen per year are (here given in increasing order according to the number stolen per year)

`SELECT b.yearID, SUM(b.SB) FROM Batting b GROUP BY b.yearID ORDER BY SUM(b.SB);`


Question 11.

Make a dotchart of the number of home runs hit by Derek Jeter each year.

Solution:

The number of home runs that Derek Jeter hit each year is (given in increasing order according to the number of home runs per year)

`SELECT b.yearID, b.HR FROM Batting b INNER JOIN Master m ON m.playerID = b.playerID WHERE m.nameLast = 'Jeter' AND m.nameFirst = 'Derek' GROUP BY b.yearID ORDER BY b.HR;`



Question 12.

Make a plot with years on the x axis, home runs on the y axis, and one data point for each player on the Yankees. Jitter the data so that overlaps among the players can be seen.

Solution:

The Yankees home run data is given by:

`SELECT b.yearID, b.HR FROM Batting b INNER JOIN Teams t ON (b.teamID = t.teamID AND b.yearID = t.yearID) WHERE t.name = 'New York Yankees' ORDER BY b.HR;`



Question 13.

Discuss the extent to which a player's number of hits is correlated with his number of home runs.

Solution:

The data for home runs and hits is given by:

`SELECT sum(b.HR), sum(b.H) FROM Batting b GROUP BY b.playerID ORDER BY sum(b.HR);`

Question 14.

Plot how many players were born in each state.

Solution:

The number of players born in each state is:

`SELECT m.birthState, COUNT(DISTINCT(m.playerID)) FROM Master m GROUP BY m.birthState ORDER BY COUNT(DISTINCT(m.playerID));`

or if we want to limit it to US States and DC, we can do that, for instance, by

`SELECT m.birthState, COUNT(DISTINCT(m.playerID)) FROM Master m WHERE m.birthState IN ('AK','AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','HI','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NV','NH','NJ','NM','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY') GROUP BY m.birthState ORDER BY COUNT(DISTINCT(m.playerID));`


These kinds of questions can be a lot of fun.  Maybe some of you have some more questions/trivia to suggest?  All suggestions are welcome, and I could even post more questions here, if you have additional suggestions.  It is hard to know how quickly you will move through this material.


== Project 9

Project 9 is about scraping data from the web in XML format,
and then parsing the data using XML tools.

The goal is to scrape the Hot 100 chart from Billboard.
This chart is posted every Saturday.  So the first chart is here:

`http://www.billboard.com/charts/hot-100/1958-08-09`

and the most current chart is here:

`http://www.billboard.com/charts/hot-100/2015-11-14`

So there are 2989 such charts during the history of the Hot 100.

You can do questions 1 and 2 at the same time, if your group decides to split up its efforts.  I.e., questions 1 and 2 can be solved independently from questions 3 and 4.

Question 1.

Make a list in R of all of the Saturdays from `1958-08-09` to `2015-11-14`.
Dr. Ward did this in about 6 lines of code, using commands in R such as:
`rep`, `sprintf`, `sapply`, `unlist`, and `paste`.
You might have other solutions.  Please try to resist the urge to use a for loop.  Notice that functions like sapply can be used instead, e.g., `sapply(1:12, function(x) ...... )` can be used, if you put the thing that you want to do for each month, where the ...... goes.

Solution:

[source,r]
----
months <- c(31,28,31,30,31,30,31,31,30,31,30,31)
names(months) <- 1:12

myYY <- rep(1958:2015, each=365)
myMM <- unlist(sapply(1:12, function(x) sprintf("%02d", rep( x,times=months[x])) ))
myDD <- unlist(sapply(1:12, function(x) sprintf("%02d",1:months[x]) ))
myYYMMDD <- paste(myYY,myMM,myDD,sep="-")
myYYMMDD <- c(myYYMMDD, sapply(seq(1960,2012,by=4), function(x) paste(x, "-02-29", sep="")))
v <- sort(myYYMMDD)
v <- v[215:21137]
v <- v[(1:length(v))%%7==0]
----


Question 2.

Scrape the data for all of the charts into files on your computer.

Solution:

[source,r]
----
mycommands <- sapply(v, function(x) paste("wget www.billboard.com/charts/hot-100/", x, sep="") )
sapply(mycommands, system)
----

Question 3.

Parse the XML data from one of the Saturdays.  More specifically, parse the title and artist for each of the 100 songs.  Hint:  You might want to try some early years and some later years, to make sure that your code works consistently.  For instance, the title of a song does not appear as an html link.  In contrast, the artist usually appears as an html link, but not always.  (There is more variability in the earlier years of the chart about that.)

Solution:

[source,r]
----
install.packages("XML")
library(XML)

mydoc <- htmlParse("1958-08-09")
mysongs <- xpathSApply(mydoc, "//*/article/*/div[@class='row-title']/h2", xmlValue)
mysongs
mysongs <- sub("^\\s+", "", mysongs)
mysongs <- sub("\\s+$", "", mysongs)
mysongs

myartists <- xpathSApply(mydoc, "//*/article/*/div[@class='row-title']/h3", xmlValue)
myartists
myartists <- sub("^\\s+", "", myartists)
myartists <- sub("\\s+$", "", myartists)
myartists
----


Question 4.

Once you have your code working for 3, wrap the code into a function that can be called with just one input, namely, the Saturday you want to parse.  For instance, you might call:  `myfunction( "1958-08-09" )`  to parse the data from the `1958-08-09` file.

Solution:

[source,r]
----
songfunction <- function(x) {
sub("\\s+$", "", sub("^\\s+", "",
xpathSApply(htmlParse(x), "//*/article/*/div[@class='row-title']/h2", xmlValue))) }

artistfunction <- function(x) {
sub("\\s+$", "", sub("^\\s+", "",
xpathSApply(htmlParse(x), "//*/article/*/div[@class='row-title']/h3", xmlValue))) }
----


Question 5.

Now use the results from question 2 (where you scraped all of the files from the web) along with the results from question 4 (where you wrote a parser), with the goal of parsing all 2989 charts.  Hint:  You might want to just try this for a few of the charts at a time, until you have this working well.

Solution:

[source,r]
----
mysonglist <- sapply(v, songfunction)
myartistlist <- sapply(v, artistfunction)
----

Note:  Some of the songs and artists are missing from the Billboard charts. There is nothing we can do about that! We can check to see which are missing!

[source,r]
----
class(mysonglist)
length(mysonglist)

class(myartistlist)
length(myartistlist)
----

These are the weeks with missing data:

[source,r]
----
sapply(myartistlist, length)[sapply(myartistlist, length) != 100]
sapply(mysonglist, length)[sapply(mysonglist, length) != 100]
----

These are the lengths for all of the weeks:

[source,r]
----
mylengths <- sapply(myartistlist, length)

myweeks <- rep(names(mylengths), times=mylengths)
length(myweeks)
alltheartists <- unlist(myartistlist)
length(alltheartists)
allthesongs <- unlist(mysonglist)
length(allthesongs)
----

Since some of the songs are missing some songs, it is helpful to get the song positions. Here is how to do that for one week (which happens to be missing a song):

[source,r]
----
mydoc <- htmlParse("1961-09-16")
mypositions <- xpathSApply(mydoc, "//*/span[@class='this-week']", xmlValue)
as.integer(mypositions)
----

Here is a function for finding the position in a given week:

[source,r]
----
positionfunction <- function(x) {
  as.integer(xpathSApply(htmlParse(x), "//*/span[@class='this-week']", xmlValue)) }
----

Here are all of the song positions across all of the weeks:

[source,r]
----
mypositionlist <- sapply(v, positionfunction)
allthepositions <- unlist(mypositionlist)
----

Now we build a data frame with all of this data:

[source,r]
----
myBB <- data.frame(alltheartists, allthesongs, myweeks, allthepositions)
names(myBB) <- c("artist", "song", "week", "position")
length(myBB$artist)
length(myBB$song)
length(myBB$week)
length(myBB$position)
----

Now we use the myBB data frame to answer the questions.



Now answer some interesting questions about the data in the charts, for instance:

Question 6.

a.  What song(s) stayed in the Hot 100 for the most weeks overall?

b.  What song(s) stayed at number 1 in the Hot 100 for the most weeks overall?

c.  What song(s) stayed in the Top 10 for the most weeks overall?

Solution:

a. We might initially try to just look at the song titles,

`head(sort(table(myBB$song),decreasing=T))`

but some common song titles were sung by more than one person. So it is better to take the artist name into account too.

`head(sort(table(  paste(myBB$song, "by", myBB$artist)  ),decreasing=T))`

b. Now we do something similar, but we restrict attention to songs at position #1.

`head(sort(table(  paste(myBB$song, "by", myBB$artist)[myBB$position == 1]  ),decreasing=T), n=8)`

c. Now we do something similar, but we restrict attention to songs at position <= 10

`head(sort(table(  paste(myBB$song, "by", myBB$artist)[myBB$position <= 10]  ),decreasing=T))`

All the answers from question 6 agree with those in Wikipedia:

https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_chart_achievements_and_milestones#Most_total_weeks_on_the_Hot_100


Question 7.

a.  What artist(s) had the most songs in the Hot 100?

b.  What artist(s) had the most number 1 songs in the Hot 100?

c.  What artist(s) spent the most weeks in the Hot 100?

Solution:

a. The results for the artist will depend on exact matches of the artists. For instance, if an artist name is listed differently, or with another artist together, then it will not show up.  So our results here are slightly different than the Wikipedia page mentioned above.

`head(sort(tapply(myBB$song, myBB$artist, function(x) length(unique(x)) ),decreasing=T))`

b. Here are the most #1 songs. The same kinds of differences with Wikipedia apply here. For instance, Wikipedia shows 20 songs at #1 for the Beatles, but we only have 19 here.  The one we are missing is "Get Back" because it is listed as having artist "The Beatles With Billy Preston"

`head(sort(tapply(myBB$song[myBB$position == 1], myBB$artist[myBB$position == 1], function(x) length(unique(x)) ),decreasing=T))`

c. For this question, it depends if we allow an artist to appear two or more times in the same week, e.g., with different songs. If we allow an artist to count every week that they appear, with possible repetitions for multiple songs, then this is easy:

`head(sort(table(myBB$artist),decreasing=T))`

If we prefer to only allow an artist to be counted at most 1 time each week, then we can work a little harder. This has, for instance, a big impact on Taylor Swift, who has been in the chart a lot lately, and has multiple songs in the chart at once, but has not been on the charts for as long as other artists. So she will not be represented as strongly, with this method. Notice that each count will be less, with this method, than the previous method, because we are simply just counting each week at most once per artist.

`head(sort(tapply(myBB$week, myBB$artist, function(x) length(unique(x)) ),decreasing=T))`


Question 8.

What song(s) have been at number 1 in the Hot 100, with 2 or more covers by different artists?

Solution:

We look at the number 1 songs, grouped according to the title. We cannot be sure that these are actually the same songs, without doing more research.

`head(sort(tapply(myBB$artist[myBB$position==1], myBB$song[myBB$position==1], function(x) length(unique(x)) ),decreasing=T),n=26)`


Question 9.

What artist(s) have had a number 1 song for the longest number of consecutive years?

Solution:

When working with a vector of years, the challenging thing is to find the longest consecutive string of years.  First we get the years.

[source,r]
----
M <- matrix(unlist(strsplit(as.character(myBB$week), "-")),ncol=3,byrow=T)
allyears <- M[ ,1]
length(allyears)
----

As an example, here are the years in which the Beatles were on the charts.

`w <- as.numeric(unique(allyears[myBB$artist=="The Beatles"]))`

We can use the rle function to do this pretty easily.

`max(rle(diff(w))$lengths)`

Now we go apply this function to all of the years that an artist had a number 1 hit, grouping by the artist.  We also put "0" into each of the lists of consecutive years, so that we do not get any trivial values.

[source,r]
----
head(sort(tapply( as.numeric(allyears)[myBB$position==1], myBB$artist[myBB$position==1], 
    function(x) max(c(0,rle(diff(sort(unique(x))))$lengths)))   ,decreasing=T))
----


Question 10.

What artist(s) had the most number 1 singles during a calendar year?  How many singles in the same calendar year was that?

As always, you are welcome to suggest some questions/answers of your own too, if you find some interesting trends.

Solution:

The Beatles managed to get 6 songs at #1 in 1964, and also 5 songs at #1 in 1965.  Remarkable!

[source,r]
----
head(sort(tapply( myBB$song[myBB$position==1], paste(myBB$artist[myBB$position==1], "in", allyears[myBB$position==1]), 
        function(x) length(unique(x)) ),decreasing=T),n=14)
----



== Project 10

Project 10 is about summarizing what you have learned in the course.

Please find some data on the web that you are interested in (as a group).

Scrape it from the web in XML format, and then parse the data using XML tools, and finally design 6 questions about the data, and answer your questions.

Since we are focusing on large data, I would like you to (please) have at least 2 million pieces of data in the set that you scrape.  You are certainly welcome to have more than this.

For comparison, in Project 9, we had roughly 3000 weeks of data, with 1 webpage/chart per week, and roughly 200 pieces of data per week, so it was about 600,000 pieces of data.  So I would like your Project 10 to be a little bigger than this... but it will be a similar "order of magnitude".  In other words, you can handle this, I know it for sure!, because you have all been doing great on Project 9.  You can certainly handle 2 million pieces of data.  (For comparison, the airline data set had about 120 million pieces of data.)

I would also request (please) that, once you identify your website with your 2 million (or more) pieces of data, you run your project idea by me.  OK?

Once you have identified your website, and you run your project idea by me, I will ask you to scrape the data from the web, and parse it.  Then you should design 6 or more interesting questions about the data, and answer each of the 6 questions.

So you will give to me and Chen the following:

The code for scraping the data from the web, and the code for parsing the data, and the 6 questions you designed about the data, and the answers to the 6 questions.

I hope that sounds suitable, and I hope that it will be fun for your project 10 groups.

The due date for Project 10 is the end of the final week of classes, i.e., by the end of the day on Friday, December 11.  (We don't have a final exam, of course.)  I just want you to be done with this project before the final exams start, so that it doesn't get in the way of your exams.

If you have any questions, please let me know.  Enjoy!


