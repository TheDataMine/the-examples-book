= TDM 20200: Project 8 -- 2024

**Motivation:** Spark uses a distributed computing model to process data, which means that data is processed in parallel across a cluster of machines. PySpark is a Spark API that allows you to interact with Spark through the Python shell. PySpark provides a way to access Spark's framework using Python, combining the simplicity of Python with the power of Apache Spark.
**Context:** Understand components of Spark's ecosystem that PySpark can use

**Scope:** Python, Spark SQL , Spark Streaming, MLib, GraphX

.Learning Objectives
****
- Develop skills and techniques to use PySpark to read a dataset, perform transformations like filtering, mapping and execute actions like count, collect 
- Understand how to use PySpark SQL to run SQL queries
****

== Dataset(s)

The following questions will use the following dataset:

- /anvil/projects/tdm/data/whin/weather.parquet


== Readings and Resources
[NOTE]
====
- Make sure to read about, and use the template found xref:templates.adoc[here], and the important information about projects submissions xref:submissions.adoc[here].
- https://the-examples-book.com/starter-guides/data-engineering/containers/pyspark[PySpark]
- https://spark.apache.org/docs/latest/[Apache Spark]
- https://sparkbyexamples.com/[Spark Examples]
- https://www.analyticsvidhya.com/blog/2022/10/most-important-pyspark-functions-with-example/[PySpark Examples]
====
== Questions

=== Question 1 (2 points)

.. Run the following code to initiating a PySpark application."sp" is the name of our PySpark session, you may use a different name.It is the entry point to using Spark's functionality with DataFrame and SQL API
+
[source,python]
====
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
sp = SparkSession.builder.appName('TDM_S').config("spark.driver.memory", "10g").getOrCreate()
====
.. Read file "/anvil/projects/tdm/data/whin/weather.parquet" to a PySpark DataFrame "df" 
.. Show the first 5 

[TIP]
====
SparkSession.builder.appName("TDM_S")
  .config("spark.sql.debug.maxToStringFields", 10000)
  .getOrCreate()
====

=== Question 2 (2 points)
.. List the DateFrame's column names and data types
.. How many unique station_ids are there?

[TIP]
====
- "printSchema()" function is useful to explore a DataFrame's structure
-  You may use "select()" function to select the column and "distinct()" and "count()" function to get distinct values
====

=== Question 3 (2 points)
.. List the DateFrame's column names and data types
.. How many rows in the DataFrame?
.. How many unique station_ids are there?


=== Question 4 (2 points)
.. Create a Temporary View from the PySpark DataFrame 'df', name it "weather"
+
[TIP]
====
- "createOrReplaceTemView()" is useful
====
.. Run a SQL Query on the view to get the total record numbers for each station in the DataFrame
.. Run a SQL Query on the view to get the highest temperature in the DataFrame

=== Question 5 (2 points)

.. Explore the DataFrame,and run 2 SQL Queries to provide some information and explain the meaning.
 

Project 08 Assignment Checklist
====
* Jupyter Lab notebook with your code, comments and outputs for the assignment
    ** `firstname-lastname-project08.ipynb` 
 
* Submit files through Gradescope
====

[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====