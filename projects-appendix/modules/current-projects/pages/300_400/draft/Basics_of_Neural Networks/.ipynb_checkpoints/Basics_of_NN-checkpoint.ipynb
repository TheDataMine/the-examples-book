{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b377e8-bfa4-432b-bccb-340dcdb8e39e",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8faf5-baeb-43f4-87a9-ffffe7cc5147",
   "metadata": {},
   "source": [
    "## Our Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e823155-2ac4-4923-aabd-24118ec5c69b",
   "metadata": {},
   "source": [
    "- https://web.archive.org/web/20230730223730/https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy\n",
    "- https://www.statlearning.com/ (Chapter 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4facdad-e48d-4985-834a-d0c444f095c1",
   "metadata": {},
   "source": [
    "## What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d525b6c-fb1f-440b-a3ea-ab0430489729",
   "metadata": {},
   "source": [
    "A neural network takes an input vector of $p$ variables $X=(X_1, X_2, ..., X_p)$ and builds a nonlinear activation function $f(X)$ to predict the response $Y$. Neural networks are incredibly useful when the expected output is not a line. <a href=https://web.archive.org/web/20230730223730/https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy>Check out this resource from Google for a visual introduction of what this can look like.</a>\n",
    "\n",
    "Neural networks were originally inspired by animal brains/neurons, the idea being to create a machine that can learn by imitating neurons. While this has proven challenging at times given that contemporary neuroscience has revealed neurons to be far more complicated than expected, neural networks are a leading model architecture choice for machine learning today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f24b7e-6230-45a5-a0dc-c004e75495c3",
   "metadata": {},
   "source": [
    "### Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3adf1-f433-40aa-9b6a-481cb6382bf7",
   "metadata": {},
   "source": [
    "<a href=https://www.statlearning.com/>ISLR chapters 10.1+10.2</a> is our reference for this section, and it comes with much better visuals- check it out!\n",
    "\n",
    "We will define a single layer *feedforward* neural network model architecture. There are others, but we will not be exploring them here.\n",
    "\n",
    "Given the features $X$, with $p$ predictors, we would have $X_1, X_2, ..., X_p$ input features. These are our **input** layer.\n",
    "\n",
    "The input layer is fed into the **hidden layer(s)**; for now we will define a neural network with only one hidden layer, but we will soon see models with more than 1.\n",
    "\n",
    "As such we have:\n",
    "\n",
    "$f(X) =\\beta_0+\\sum_{k=1}^{K}\\beta_kh_k(X)$\n",
    "<br>$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_kg(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_j)$\n",
    "\n",
    "$\\beta_0$ is the bias term, sometimes called the intercept. If you've done regression before this is the same as an intercept bias term in a linear regression model.\n",
    "\n",
    "$K$ is the number of neurons in the hidden layer. So the term $ \\sum_{k=1}^{K} $ says \"the summation from 1 to the max number of neurons\". *The number of neurons per layer is up to you and a key **tuning** parameter*.\n",
    "\n",
    "$\\beta_k$ is the coefficient term for the given $k$ neuron.\n",
    "\n",
    "You will notice the difference between the two given equations above is the $h_k(X)$ portion. This is the activation function; you will also see it written as $g(z)$. We will look a little more into activation functions soon, but for now just notice that\n",
    "\n",
    "$A_k = h_k(X) = g(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j})=g(z)$\n",
    "\n",
    "The $\\beta$ and $w$ parameters need to be estimated; the manner by which they are estimated differs, but for feedforward architectures the standard technique is called **backpropagation**. We will not explore that in this assignment, but you will see it in other assignments.\n",
    "\n",
    "ADD TEH FINAL OUTPUT LAYER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eeb78d-1905-4621-934e-3fbf1f1f25a2",
   "metadata": {},
   "source": [
    "## Layers (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba6761-0185-4733-bb43-8a78f789795c",
   "metadata": {},
   "source": [
    "A typical *feedforward* neural network architecture has 3 different types of layers: **input**, **output**, and **hidden**. The input and output layers are only 1 layer deep, but the hidden layers can be any positive non zero integer (when you have 2 or more layers, you are said to be performing *deep learning*). \n",
    "\n",
    "Instead of writing all this out, let's just *make our own with code*! `keras` can help us do that. `keras` is an interface to the Google machine learning package, `Tensorflow`. It let's us use `Tensorflow` in a simple and efficient manner.\n",
    "\n",
    "We have a simple 1 layer neural network implemented below that needs the number of *neurons* added to it. Sometimes you will see neurons referred to as *nodes* or *units*; these are all analagous. `keras` gives us the ability to create a model *sequentially*, so we can add each layer one at a time, and that is what you see below. We aren't going to *train* the model; for now we are just building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f77470-f968-41bd-8480-b2553f2f2241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:33:27.687969: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 14:33:28.089800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 14:33:28.089847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 14:33:28.092671: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 14:33:28.293669: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-01 14:33:28.295363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 14:33:29.989010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02302757-a302-4798-90f0-9bf1b1f67258",
   "metadata": {},
   "source": [
    "**We have 16 input variables, so set the `input_neurons` to reflect that. Let's use 8 neurons for the hidden layer, and we want 1 output neuron. Edit the variables below to create our model architecture.** Make sure to use ints and not floats here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef563ca5-d172-4cf9-81de-b96ae7fded8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (267590109.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_neurons = ??\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "input_neurons = ??\n",
    "hidden_layer_1_neurons = ??\n",
    "output_neurons = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a92b27-1c29-4069-a31f-d57186d9b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_neurons != 16:\n",
    "    raise AssertionError(\"Input layer neurons is not correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d118801-c62b-41f2-9a50-e040accf4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hidden_layer_1_neurons != 8:\n",
    "    raise AssertionError(\"Hidden layer neurons is not correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056de70-436a-4edc-9277-d3ea563ff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_neurons != 1:\n",
    "    raise AssertionError(\"Output layer neurons is not correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52f1440-8c12-4364-bf1d-70c6155db33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(input_neurons,))) #input layer\n",
    "model.add(Dense(hidden_layer_1_neurons)) #hidden layer\n",
    "model.add(Dense(output_neurons)) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0254f4-8ab3-4bb0-9a8c-d59170c28e59",
   "metadata": {},
   "source": [
    "`keras` can summarize our current model architecture. You should see two `Dense` layers, one for our hidden layer and one for our output layer. The number of parameters is important, and we will see later how this is calculated. You should have 145 total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e647bfcc-9d58-4f4e-9481-87659eb33d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145 (580.00 Byte)\n",
      "Trainable params: 145 (580.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136f8fa-c7fc-4443-86e9-8d413c5c03c1",
   "metadata": {},
   "source": [
    "## Weights + Parameters (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca783478-f772-44c4-bf55-344449fa70bd",
   "metadata": {},
   "source": [
    "Recall the $\\beta$ and $w$ parameters above. These are coefficients that need to be estimated, much in the same way that $\\beta$ values have to be estimated for a regression equation.\n",
    "\n",
    "You will notice that for the $f(X)$ neural networks equation\\, the $b_k$ coefficients are parameters for each given neuron. But what about the $w$ parameters? These are the coefficients that each neuron to neuron connection between layers. \n",
    "\n",
    "The blessing and the curse of neural networks is that they have so many parameters. Even with our relatively simple example in the *layers* section above, with only 16 input variables, 8 neurons in the hidden layer, and 1 output neuron, that still amounts to 145 parameters that have to be estimated! But why 145?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d193cdb-6b33-4924-b2d6-ca19c747add1",
   "metadata": {},
   "source": [
    "### Calculating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de018b7-d9ad-48bb-8e05-46f2dffafb70",
   "metadata": {},
   "source": [
    "![alternative text](nnexmaple.png)\n",
    "\n",
    "All models have 1 weight per neuron to neuron connection. So in the image above, in between the input neurons and hidden layer 1 is $W_1$. In a feedforward architecture, each layer is connected sequentially, so the hidden input layer feeds into the output layer, with the $W_2$ weights representing those connections. The total number of parameters is weights ($W$) plus $\\beta$ values (recall there is an additional intercept value per each neuron at the next layer). \n",
    "\n",
    "For our model in the *layers* section, it would have 1 weight per neuron to neuron connection (so, with 16 inputs, and 8 neurons, that would be 16*8=128) plus 8 $\\beta$ coefficients per neuron, so $8+128=136$ parameters.\n",
    "\n",
    "For the next layer, it would have 1 weight per neuron to neuron connection, plus the additional $\\beta$ coefficient for the final output neuron, which would be $8*1 + 1=9$. If we total these together, that's $136+9=145$ parameters.\n",
    "\n",
    "Confirming the number of parameters during model building is important for neural networks. Although it can be tedious, manually calculating and confirming the number of parameters and then confirming with the `keras` `summary()` function to verify is a surefire way to ensure you are building the model you think you are building.\n",
    "\n",
    "Incredibly large networks today order on the millions or billions scale of parameters, so manual verification might not be realistic; but you should still attempt to confirm the parameters because it is a check to ensure you understand the model architecture in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46225c8b-1168-4418-bc63-6d34967fb55c",
   "metadata": {},
   "source": [
    "**Manually calculate the number of weights for the following neural network model with 1 hidden layer:**\n",
    "\n",
    "* 15 neuron input layer\n",
    "* 30 neuron hidden layer\n",
    "* 5 neuron output layer\n",
    "\n",
    "**Set the variable `total_params` equal to the total number of parameters.** You can manually calculate it by hand and/or use a software package, **so long as the answer is an int and is the total number of parameters for this model architecture**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51aa18a-5e0a-4172-980d-7e4192125793",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d94bba-61d1-4f7d-9cfd-b740366fe465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell should be confirming that total_params = (15*30+15)+(30*5+30)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e22731f-5b52-493f-9abf-5f4b33f60d84",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b31ba5-f1a9-4572-8214-78691473b0bb",
   "metadata": {},
   "source": [
    "Activation functions are critical to neural networks. They serve 2 primary purposes:\n",
    "\n",
    "1. They create nonlinearity (where otherwise you might have a simple linear relationship)\n",
    "2. They ensure the model can capture complex nonlinearities and interactions between the variables\n",
    "\n",
    "In short, activation functions are what ensure neural networks produce nonlinear output.\n",
    "\n",
    "Although there are many activation functions, two of the most common include the ReLU (Rectified Linear Unit) and sigmoid activation functions. <a href=https://keras.io/api/layers/activations/#available-activations>A machine learning package like `keras` will offer both of these and more</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ad2ca-06eb-40a8-830c-64e3de0d8de7",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35e3fe-c212-4fc3-b768-9f55ec613c90",
   "metadata": {},
   "source": [
    "The sigmoid function looks like a flattened \"S\" shape. Below is an example of a logistic function (which is also the same function used in logistic regression to convert a linear function into probabilities between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187e3c0-4863-47eb-b075-3a7d695cebe0",
   "metadata": {},
   "source": [
    "![alternative text](320px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d916d-ac28-4852-a653-e13ab1c81e81",
   "metadata": {},
   "source": [
    "The equation for this function is $g(z)=\\frac{e^{z}}{1+e^{z}}=\\frac{1}{1+e^{-z}}$. **Set the variable `sigmoid` equal to this function, using `z` as the variable.** Hint: use `exp(z)` for $e^{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6495ef4-daa8-43de-8011-b393e7d03351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7262cbc-ee96-4f98-b90b-2e1d90e9c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda z : #YOUR FUNCTION IN CODE GOES HERE#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da15e73a-2323-41e0-90f1-ca7cf06f0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310585786300049\n"
     ]
    }
   ],
   "source": [
    "#check for the value of sigmoid(1), sigmoid(.85), sigmoid(19), sigmoid(3.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da039c7-02a8-44c4-8e6e-92e6dc8dc29c",
   "metadata": {},
   "source": [
    "### ReLU Activation Function (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fe94f-e4a2-4cb3-aac7-b7501566b9d3",
   "metadata": {},
   "source": [
    "The ReLU activation function is defined as:\n",
    "\n",
    "$ g(z) = \\left\\{\\begin{array}{ll}0 & \\text{if} \\ z<0 \\\\z & \\text{otherwise} \\\\\\end{array} \\right. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbe10f-a222-4fd4-b6cb-932ad8da7bca",
   "metadata": {},
   "source": [
    "Today, ReLU is the preferred choice for neural networks, in part because it can be computed and stored more efficiently than the sigmoid activation function. **Do the same here that we did with the sigmoid function; create your own ReLU function below using `z` as the variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad0bde-b31a-40ef-80e3-5c7aef1cfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    if z < 0:\n",
    "        return ??\n",
    "    else:\n",
    "        return ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8af3e1-e9b2-4c98-9de8-ef51cef7cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that when ReLU(-.1), ReLU(-293), ReLU(2) and ReLU(100.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe112dd6-14a8-4343-bdd2-8f6093441196",
   "metadata": {},
   "source": [
    "## Deep Learning (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b562a9-3010-4181-b93e-d3138455b93e",
   "metadata": {},
   "source": [
    "Deep learning is a neural network model with 2 or more hidden layers. So a deep learning neural network architecture would have 1 input layer, more than 1 hidden layer, and an output layer. Some of the more complex deep learning models out there include dozens or hundreds of layers. The number of layers, and the number of neurons per layer, is yet another **tuning parameter** to consider when designing neural network architectures.\n",
    "\n",
    "You will notice that the input layer is fed into the first hidden layer, the first hidden layer feeds into the second, etc. You can in theory have as many layers as you like, but there are important considerations such as bias-variance tradeoff and computational resources that may limit the number of layers that is practical/ideal. We will explore these considerations in later homeworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e33d10-8db1-42b6-82c0-4617cfab7191",
   "metadata": {},
   "source": [
    "**For the final problem, finish the deep learning model by adding the given neurons to each specified layer:** \n",
    "\n",
    "* 34 input neurons\n",
    "* 17 hidden layer 1 neurons\n",
    "* 8 hidden layer 2 neurons\n",
    "* 4 output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49e02d-162b-4202-a394-55fd0fcb9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = ??\n",
    "hidden_layer_1_neurons = ??\n",
    "hidden_layer_2_neurons = ??\n",
    "output_neurons = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec4b9c-a91c-4423-9377-acfc408a17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for 34, 17, 8, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e0282-8322-4753-aa13-e51beaef5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model = Sequential()\n",
    "dl_model.add(Input(shape=(input_neurons,))) #input layer\n",
    "dl_model.add(Dense(hidden_layer_1_neurons)) #hidden layer\n",
    "dl_model.add(Dense(hidden_layer_2_neurons)) #hidden layer\n",
    "dl_model.add(Dense(output_neurons)) #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72c6c6-55fa-408b-87fb-51e63945b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary_python_env",
   "language": "python",
   "name": "primary_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
