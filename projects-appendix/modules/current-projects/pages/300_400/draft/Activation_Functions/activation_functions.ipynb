{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d593891-3c0e-4506-92e2-31e089454c3c",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3b8da-4931-4e6b-98d8-6073315ae18f",
   "metadata": {},
   "source": [
    "## Module Objectives\n",
    "- Explore Activation Functions\n",
    "- Implement Activation Functions Using `keras`\n",
    "- Deep Dive Into ReLU and Softmax Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d9384-0435-4690-80be-7bc2c2c3cdca",
   "metadata": {},
   "source": [
    "## Our Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d60941-1b64-4786-b920-b1634e0ca0c6",
   "metadata": {},
   "source": [
    "* https://keras.io/api/layers/activations/\n",
    "* https://www.statlearning.com/ (Chapter 10)\n",
    "* https://web.archive.org/web/20231204181718/https://en.wikipedia.org/wiki/Activation_function (has good reference table built for many different activation functions)\n",
    "* https://web.archive.org/web/20230828124751/https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78\n",
    "* https://web.archive.org/web/20231122184108/https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f49a96-8595-4898-bd8e-ad631b224e82",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5cdc5-c8a5-4875-acf5-20de2897d0a7",
   "metadata": {},
   "source": [
    "Recall the equation given for a (single hidden) neural network layer (see Basics of Neural Networks homework for a reminder of its interpretation): \n",
    "\n",
    "$f(X) =\\beta_0+\\sum_{k=1}^{K}\\beta_kh_k(X)$\n",
    "<br>$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_kg(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_j)$\n",
    "\n",
    "where $K$ is the total number of neurons (and $k$ is an individual neuron), $w$ is the weight, and $p$ is the predictor/inputs.\n",
    "\n",
    "Recall that the activation function is the portion\n",
    "\n",
    "$A_k = h_k(X) = g(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j})=g(z)$\n",
    "\n",
    "The commonly used shorthand is $g(z)$. Notice too that there is exactly one activation function per neuron in any layer.\n",
    "\n",
    "Activation functions are critical to neural networks. They serve 2 primary purposes:\n",
    "\n",
    "1. They create nonlinearity (where otherwise you might have a simple linear relationship)\n",
    "2. They ensure the model can capture complex nonlinearities and interactions between the variables\n",
    "\n",
    "In short, activation functions are what ensure neural networks produce nonlinear output. There are other properties that activation functions can bring, but they differ depending on which activation function gets used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cf3d2-cd06-458e-a05e-9cb8832b8f7d",
   "metadata": {},
   "source": [
    "## Activation Functions provided by `keras` (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da9b5-fc08-4d31-aa3b-7635ebcaa9a6",
   "metadata": {},
   "source": [
    "There are a few major \"families\" of activation functions that have the desirable properties for activation functions, including:\n",
    "\n",
    "* *-LU: Linear Unit (such as ReLU)\n",
    "* Sigmoid\n",
    "* Hyperbolic (such as tanh)\n",
    "* Softmax\n",
    "\n",
    "There are certainly others as activation functions are still heavily under research, however these are the most common varieties you will see in use today.\n",
    "\n",
    "`keras` carries all of these, with multiple different varieties. Below we list some of the most commonly used activation functions and their use case:\n",
    "\n",
    "* ReLU (Rectified Linear Unit): Most commonly used. Good general purpose activation function. Can be computed and stored more efficiently than sigmoid (see https://dl.acm.org/doi/pdf/10.1145/3065386, under section 4.1).\n",
    "* Sigmoid: Used to be the most commonly used, before ReLU. Is also used in logistic regression to convert a linear function into probabilities between 0 and 1.\n",
    "* Softmax: Also used for (multinomial) logistic regression. Used when there is categorical output and when you want a probability of the chance of each given category. \n",
    "\n",
    "In general, it's not a bad idea to stick to the Linear Unit family (ReLU, GELU, etc) of activation functions (ReLU in particular) unless you are doing categorical output, in which case a softmax family function (softmax, softplus, etc) might come in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddf2ac-be18-47cd-81f1-ee18e3318c51",
   "metadata": {},
   "source": [
    "## Implementing Activation Functions in `keras` (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d5bc7-eefe-48bb-96e8-5da756797f89",
   "metadata": {},
   "source": [
    "Adding activation functions in `keras` is a breeze. Below is a simple implementation of a single layer neural network **that will have ReLU in the hidden layer and sigmoid for the output layer**.\n",
    "\n",
    "**For the cell below, type in the `keras` syntax for both of the activation functions**. Verify it's correct with https://keras.io/api/layers/activations/ if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50764a-7bf2-42fa-881e-37de35f54ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6fd54-3dff-4c6a-8ec8-2861ec3fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_activation = #YOUR ANSWER GOES HERE\n",
    "output_layer_activation = #YOUR ANSWER GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bc0b1-108a-4889-a298-a760d415e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(82,))) #input layer\n",
    "model.add(Dense(41, activation=hidden_layer_1_activation)) #hidden layer 1\n",
    "model.add(Dense(10, activation=output_layer_activation)) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f2045-62bf-41a2-bec2-1633cf145864",
   "metadata": {},
   "source": [
    "## ReLU: Deep Dive (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d3c13-168f-4121-801f-4d644224b681",
   "metadata": {},
   "source": [
    "The ReLU activation function is the most commonly used because of its efficient computational properties (relative to the previous standard, sigmoid). \n",
    "\n",
    "The ReLU function is defined as\n",
    "\n",
    "$ g(z) = \\left\\{\\begin{array}{ll}0 & \\text{if} \\ z<0 \\\\z & \\text{otherwise} \\\\\\end{array} \\right. $\n",
    "\n",
    "Recall the single layer neural network equation for the hidden layer:\n",
    "\n",
    "$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_kg(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_j)$\n",
    "\n",
    "We pointed out in the introduction that \n",
    "\n",
    "$g(w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j})=g(z)$\n",
    "\n",
    "Recall that $w_k$ is the weight at the neuron $k$, and $p$ is the total number of predictors/inputs. $w_{k0}$ is the bias intercept at that given neuron; and the summation is all the weights at the current neuron for each predictor times each predictor. As long as $w_{k0}+\\sum_{j=1}^{p}w_{kj}X_{j}$ is positive, then the ReLU activation function is equivalent to its value; else, it's set to 0. Let's see what happens if it is set to 0:\n",
    "\n",
    "$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_k*0$\n",
    "\n",
    "$ f(X)=\\beta_0$\n",
    "\n",
    "Let's see what happens when $g(z)=1$:\n",
    "\n",
    "$ f(X)=\\beta_0+\\sum_{k=1}^{K}\\beta_k*1$\n",
    "\n",
    "$ f(X)=\\beta_0+\\beta_1+\\beta_2+...+\\beta_k$\n",
    "\n",
    "This situation, where the $z$ value is the same across all weight, neuron and predictor combinations, is highly unlikely. Let's see a simple demonstration that is slightly more likely, where we have 3 neurons ($K=3$) and the results of $g(z)$ are 1, 2 and 3 respectively:\n",
    "\n",
    "$ f(X)=\\beta_0+\\beta_1 + 2\\beta_2+3\\beta_3$\n",
    "\n",
    "But with 3 neurons, we would have 3 activation functions total in that layer, with the values:\n",
    "\n",
    "$A_1 = \\beta_1$\n",
    "<br> $A_2 = 2\\beta_2$\n",
    "<br> $A_3 = 3\\beta_3$\n",
    "\n",
    "From there, the results would feed into the next layer (in this case of a single layer network, into the output layer; for deep learning, into the next hidden layer) as the input.\n",
    "\n",
    "There are many other varieties of the Linear Unit family of functions, such as GELU (\"Gaussian Error Linear Units\") which is used for BERT, ChatGPT and the like (source: https://arxiv.org/pdf/1606.08415.pdf). ReLU is the \"tried and true\" gold standard and the other Linear Unit functions are often judged by their ability to improve ReLU.\n",
    "\n",
    "The `keras` implementation of ReLU comes with configuration options. **Read about the configuration options <a href=https://keras.io/api/layers/activations/#relu-function>here</a> and add the command to change the max value of the threshold to (int) 25**. Below we have implemented an example using a different option for reference. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cffd17ad-41d8-4143-8837-0b494c6522f0",
   "metadata": {},
   "source": [
    "## EXAMPLE\n",
    "\n",
    "relu_model = Sequential()\n",
    "relu_model.add(Input(shape=(129,))) #input layer\n",
    "relu_model.add(Dense(13, activation=tf.keras.layers.ReLU(threshold=15)))\n",
    "relu_model.add(Dense(1, activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be67284-7441-407e-96ef-fca3a01bebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_model = Sequential()\n",
    "relu_model.add(Input(shape=(129,))) #input layer\n",
    "relu_model.add(Dense(13, activation=#YOUR ANSWER GOES HERE))\n",
    "relu_model.add(Dense(1, activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687d74c-be61-4a67-8450-4fbd2bdad072",
   "metadata": {},
   "source": [
    "## Softmax: Deep Dive (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d409d-0f7d-4742-a4f4-840455b29230",
   "metadata": {},
   "source": [
    "The softmax function gets used when there is categorical output for the output layer. **Read about the softmax function from at least one of these resources then answer the question below:**\n",
    "\n",
    "* https://web.archive.org/web/20230828124751/https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78\n",
    "* https://web.archive.org/web/20231122184108/https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n",
    "* https://keras.io/api/layers/activations/#softmax-function\n",
    "\n",
    "**In the hidden cell below is a vector of probabilities that is the output of a softmax function. Based on what you know about the softmax function, set the answer variable `sum` equal to what all of the (output) probabilities will sum to.** Hint: You cannot know what these probabilities are, nor how many there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68fe8e-9f51-4fd9-97c6-2458e1651889",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = #YOUR ANSWER GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary_python_env",
   "language": "python",
   "name": "primary_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
