= 201 Project 01: Data Wrangling with GNU Command Line Utilities

== Project Objectives

- Learn basic command line operations for handling CSV files
- Prepare raw data for further analysis by resolving formatting inconsistencies, missing values 

== Reading and Resources

- https://www.gnu.org/software/coreutils/manual/coreutils.html[GNU Coreutils Documentation]
- https://www.gnu.org/software/gawk/manual/gawk.html[GNU Awk User's Guide]

== Dataset

- /anvil/projects/tdm/data/noaa/2010.csv

== Questions  

=== Question 1 (1 point) 

Read from the GNU Coreutils documentation on the `ls`, `head` and `tail` commands:

- https://www.gnu.org/software/coreutils/manual/coreutils.html[GNU Coreutils Documentation]
- https://www.gnu.org/software/coreutils/manual/coreutils.html[GNU Coreutils - head and tail]

[NOTE]
====
To get basic file information and view specific rows, the following commands are useful:
- `ls` to list file details, like
[source,bash]
ls -lh
- `head` to display the first few lines, like
[source,bash]
head -n 20 filename
- `tail` to display the last few lines, like
[source,bash]
tail -n 20 filename
====

.. Use the `ls` command to view file details; `head`,`tail` and pipe commands to display first 10 records of the 2010.csv file and row 33rd content.

 
== Question 2 (2 points)

Read from https://www.gnu.org/software/gawk/manual/gawk.html[GNU Awk User's Guide] to learn use `awk` command to do analysis

[NOTE]
====
- You may use `wc` to analyze file such as to count the total number of records

[source, bash]
wc -l filename

- You may use `awk` to manipulate data and generate data information like to get and print out the number of fields in the current record 
[source,bash]
awk -F, '{print NF}'
====
.. Please use `wc`, `awk` and pipeline to count how many records in 2010.csv that column 3 ELEMENT is "TAVG"  
 

== Question 3 (1 point) 

[NOTE]
====
- You may use `>` to save data to a file
====
.. Select a value for the `column 3` from `TMAX`, `TMIN`, or `TAVG`, use `awk` command to compare and save filtered data to a file named "2010_filtered.csv"  

 
== Question 4 (2 points)

[NOTE]
====
You may extract unique values by using combination of commands like `sort`,`uniq` etc
- `sort` can be used to sort lines of text files 
[source,bash]
sort filename
- `uniq` can be used to remove adjacent duplicated lines
[source,bash]
uniq filename
====

.. Use `awk`, `sort`, `uniq` and pipeline to to get get the unique stations which is column 1 in the file, and save the output to a file named 'stations_output.txt' 
 

== Question 5 (2 points) 

[NOTE]
====
- `awk` action block can also include selection or repetition structures, for example if we have a file named surplus.txt contains following data:

table, 20
printer,100
bike,10
printer,60
bike,30

We want to reduce each printer by 5, we can do

[source,bash]
awk -F, '{if ($1== 'printer') $2=$2-5;print}' surplus.txt

We can change the first column to upper case by doing following loop

[source,bash]
awk -F, '{for (i=1;i<=NF;i++) $i=toupper($i);print}' surplus.txt 

====

.. Please convert the temperature values in column 4 to regular decimal point values if column 3 is  `TMAX`, `TMIN`, or `TAVG`  

 

== Question 6 (2 points) 

.. Please use `awk` with `if` and `for` loop to count the total number of missing values in each column and print out the result like

.output
Column, 3: 23

[TIP]
====
- You may use an associate array to hold the empty value for each column like 

awk -F,'{for (i=1;i<=NF;i++) if($i=="") empty_count[i]++} ' filename
====
 

 
[WARNING]
====
_Please_ make sure to double check that your submission is complete, and contains all of your code and output before submitting. If you are on a spotty internet connection, it is recommended to download your submission after submitting it to make sure what you _think_ you submitted, was what you _actually_ submitted.

In addition, please review our xref:projects:current-projects:submissions.adoc[submission guidelines] before submitting your project.
====