= Data Analysis Workshop

== What is Webscraping?

Webscraping is the process of collecting data (information) from public websites that is then exported into a easier to read format. It can be done automatically or manually. Often we are looking for external data sources that can be relevant to our learnings.

== What are websites are made of?
We can think of it this way, in order to build a house we must first understand what materials are used in the construction.  In the same way to gather the relevant data from websites, we must first learn the way that websites are built. 

Websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. We are going to focus on the HTML. HTML in a simple explanation is the way that material is formatted/displayed over the internet. It allows creaters to create and structure sections, paragprahs and links with things like elements, tags, and attributes. 

* Tags: starting and ending parts of an HTML element. They will always begin and end with <...> whatever is written inside <and> is a tag. Tags are like keywords with a distinctive meaning. They also must be opened <a>...</a> and closed in order to function. 
            ex: <a> </a>
        
* Elements: is the content inbetween the tag
            ex: <a> THIS IS THE ELEMENT </a>
    
* Attributes: used to definte the characteristics of the HTML element in detail. They will only be found in the starting tag. 
            ex: <a align="right"> THIS IS THE ELEMENT </a>

When we are scraping we need to find the tags that have the relevant information between them. 

== Tools to webscrape
There are several structures used to webscrape, such as `requests` and `lxml` and `beautifulsoup4` but we will be focusing today on using `selenium`. This will let us create script to webscrape multiple pages to create our dataframe. Through `selenium` the script can interact, scrape and parse through the browser. 

In getting started we must choose a browswer and it's web driver
i.e. - Firefox: gekodriver
         - Chrome: ChromeDriver
         - Safari: SafariDriver
(we will be using Firefox and geckodriver)

Then we will need to download `selenium` to use this copy and paste this code into your notebook.

[source, python]
----
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
import uuid

firefox_options = Options()
firefox_options.add_argument("window-size=1920,1080")
# Headless mode means no GUI
firefox_options.add_argument("--headless")
firefox_options.add_argument("start-maximized")
firefox_options.add_argument("disable-infobars")
firefox_options.add_argument("--disable-extensions")
firefox_options.add_argument("--no-sandbox")
firefox_options.add_argument("--disable-dev-shm-usage")
firefox_options.add_argument('--disable-blink-features=AutomationControlled')

# Set the location of the executable Firefox program on Brown
firefox_options.binary_location = '/depot/datamine/bin/firefox/firefox'

profile = webdriver.FirefoxProfile()

profile.set_preference("dom.webdriver.enabled", False)
profile.set_preference('useAutomationExtension', False)
profile.update_preferences()

desired = DesiredCapabilities.FIREFOX

# Set the location of the executable geckodriver program on Brown
uu = uuid.uuid4()
driver = webdriver.Firefox(log_path=f"/tmp/{uu}", options=firefox_options, executable_path='/depot/datamine/bin/geckodriver', firefox_profile=profile, desired_capabilities=desired)
----





Next we will take a look at the website
 https://www.goodreads.com/list/show/18645.Best_Books_That_Grow_You) 
we want to scrape. Go to the 1st book titled "The Alchemist", highlight and right click you should see a drop down menu, go ahead and choose `Inspect`. 
image::book/modules/gallygoogle/Images/GoodReads1.png[A screenshot showing the webpage goodreads with the alchamist highlighted and the dropdown menu showing]
